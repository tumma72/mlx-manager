---
phase: 05-chat-multimodal-support
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - backend/mlx_manager/routers/chat.py
  - backend/mlx_manager/routers/__init__.py
  - backend/mlx_manager/main.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "POST /api/chat/completions streams SSE events"
    - "Thinking content extracted and typed as thinking events"
    - "Non-thinking content typed as response events"
    - "Connection errors return error event with user-friendly message"
    - "Endpoint requires authentication"
  artifacts:
    - path: "backend/mlx_manager/routers/chat.py"
      provides: "Chat streaming endpoint"
      exports: ["router"]
    - path: "backend/mlx_manager/main.py"
      provides: "App with chat router registered"
      contains: "chat_router"
  key_links:
    - from: "backend chat.py"
      to: "mlx-openai-server"
      via: "httpx proxy with SSE streaming"
      pattern: "httpx.*chat/completions"
    - from: "main.py"
      to: "chat.py router"
      via: "app.include_router"
      pattern: "include_router.*chat"
---

<objective>
Create backend chat streaming endpoint that proxies to mlx-openai-server with thinking tag parsing.

Purpose: Provide a backend endpoint that handles SSE streaming from mlx-openai-server, parses thinking tags, and emits typed events for the frontend to consume.

Output: POST /api/chat/completions endpoint with SSE streaming and thinking extraction.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-chat-multimodal-support/05-CONTEXT.md
@.planning/phases/05-chat-multimodal-support/05-RESEARCH.md
@.planning/phases/05-chat-multimodal-support/05-01-SUMMARY.md
@backend/mlx_manager/routers/__init__.py
@backend/mlx_manager/main.py
@backend/mlx_manager/routers/auth.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat streaming endpoint</name>
  <files>backend/mlx_manager/routers/chat.py</files>
  <action>
Create a new chat router that proxies to mlx-openai-server with SSE streaming and thinking tag parsing.

**Create backend/mlx_manager/routers/chat.py:**

```python
"""Chat completions router with streaming support."""

import json
import time
from typing import AsyncGenerator

import httpx
from fastapi import APIRouter, Depends, HTTPException, Body
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlmodel.ext.asyncio.session import AsyncSession

from ..database import get_db
from ..models import ServerProfile
from .auth import get_current_user

router = APIRouter(prefix="/chat", tags=["chat"])


class ChatRequest(BaseModel):
    """Chat completion request."""

    profile_id: int
    messages: list[dict]  # OpenAI-compatible message format


@router.post("/completions")
async def chat_completions(
    request: ChatRequest = Body(...),
    db: AsyncSession = Depends(get_db),
    _user=Depends(get_current_user),  # Auth required, user object unused
):
    """
    Stream chat completions from mlx-openai-server.

    Proxies to the MLX server running for the given profile.
    Parses thinking tags and emits typed chunks for frontend rendering.

    SSE Event Types:
    - thinking: Content inside <think>...</think> tags
    - thinking_done: Emitted when </think> is encountered, includes duration
    - response: Regular response content (outside thinking tags)
    - error: Error message (connection failure, server error)
    - done: Stream complete

    Note: Connection errors (httpx.ConnectError) indicate the server is not running.
    This is the appropriate check - we don't need to pre-verify server state since
    the connection attempt itself is the verification.
    """
    # Get profile
    profile = await db.get(ServerProfile, request.profile_id)
    if not profile:
        raise HTTPException(status_code=404, detail="Profile not found")

    # Build MLX server URL
    server_url = f"http://{profile.host}:{profile.port}/v1/chat/completions"

    # Check if profile has reasoning parser enabled
    has_reasoning = bool(profile.reasoning_parser)

    async def generate() -> AsyncGenerator[str, None]:
        thinking_start: float | None = None
        in_thinking = False

        async with httpx.AsyncClient(timeout=300.0) as client:
            try:
                async with client.stream(
                    "POST",
                    server_url,
                    json={
                        "model": profile.model_path,
                        "messages": request.messages,
                        "stream": True,
                    },
                ) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        yield f"data: {json.dumps({'type': 'error', 'content': error_text.decode()})}\n\n"
                        return

                    async for line in response.aiter_lines():
                        if not line.startswith("data: "):
                            continue

                        data_str = line[6:]  # Remove "data: " prefix
                        if data_str == "[DONE]":
                            break

                        try:
                            data = json.loads(data_str)
                            delta = data.get("choices", [{}])[0].get("delta", {})
                            content = delta.get("content", "")

                            if not content:
                                continue

                            # Parse thinking tags if reasoning enabled
                            if has_reasoning:
                                i = 0
                                while i < len(content):
                                    # Check for opening tag
                                    if content[i:i+7] == "<think>":
                                        in_thinking = True
                                        thinking_start = time.time()
                                        i += 7
                                        continue

                                    # Check for closing tag
                                    if content[i:i+8] == "</think>":
                                        in_thinking = False
                                        duration = time.time() - thinking_start if thinking_start else 0
                                        yield f"data: {json.dumps({'type': 'thinking_done', 'duration': round(duration, 1)})}\n\n"
                                        i += 8
                                        continue

                                    # Emit character
                                    char = content[i]
                                    if in_thinking:
                                        yield f"data: {json.dumps({'type': 'thinking', 'content': char})}\n\n"
                                    else:
                                        yield f"data: {json.dumps({'type': 'response', 'content': char})}\n\n"
                                    i += 1
                            else:
                                # No reasoning parser, emit all as response
                                yield f"data: {json.dumps({'type': 'response', 'content': content})}\n\n"

                        except json.JSONDecodeError:
                            continue

                    yield f"data: {json.dumps({'type': 'done'})}\n\n"

            except httpx.ConnectError:
                yield f"data: {json.dumps({'type': 'error', 'content': 'Failed to connect to MLX server. Is it running?'})}\n\n"
            except httpx.TimeoutException:
                yield f"data: {json.dumps({'type': 'error', 'content': 'Request timed out. The model may be processing a complex request.'})}\n\n"
            except Exception as e:
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )
```

Key design decisions:
- `_user` parameter: Auth is required but user object not needed. Underscore prefix indicates intentionally unused.
- Connection check: We don't pre-verify server status. The httpx.ConnectError from the actual connection attempt is the appropriate check. This avoids race conditions and extra network calls.
- Timeout: Added httpx.TimeoutException handling for long-running requests.
  </action>
  <verify>
1. Run `cd backend && ruff check .` - should pass
2. Run `cd backend && ruff format --check .` - should pass
3. Run `cd backend && mypy mlx_manager` - should pass
  </verify>
  <done>
- Chat streaming endpoint created at POST /api/chat/completions
- Thinking tag parsing with duration tracking
- SSE events with type: thinking | thinking_done | response | error | done
- Proper error handling for connection failures and timeouts
  </done>
</task>

<task type="auto">
  <name>Task 2: Register chat router</name>
  <files>
    backend/mlx_manager/routers/__init__.py
    backend/mlx_manager/main.py
  </files>
  <action>
**1. Update backend/mlx_manager/routers/__init__.py:**
Add the chat router to the imports and __all__ list.

Add import:
```python
from .chat import router as chat_router
```

Add to __all__: `"chat_router"`

**2. Update backend/mlx_manager/main.py:**
Register the chat router. Find where other routers are included and add:

```python
from .routers import chat_router
# ...
app.include_router(chat_router, prefix="/api")
```
  </action>
  <verify>
1. Run `cd backend && ruff check .` - should pass
2. Run `cd backend && ruff format --check .` - should pass
3. Run `cd backend && mypy mlx_manager` - should pass
4. Run `cd backend && pytest tests/ -v` - existing tests should pass
  </verify>
  <done>
- Chat router exported from routers package
- Chat router registered in main app at /api/chat prefix
- Endpoint available at POST /api/chat/completions
  </done>
</task>

</tasks>

<verification>
1. Backend quality: `cd backend && ruff check . && ruff format --check . && mypy mlx_manager && pytest -v`
2. Manual test (optional): Start a server, use curl to test SSE:
   ```bash
   curl -X POST http://localhost:8080/api/chat/completions \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer <token>" \
     -d '{"profile_id": 1, "messages": [{"role": "user", "content": "Hello"}]}'
   ```
</verification>

<success_criteria>
- POST /api/chat/completions streams SSE events
- Thinking content parsed and typed correctly
- Connection errors return user-friendly messages
- All backend quality gates pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-chat-multimodal-support/05-02-SUMMARY.md`
</output>
