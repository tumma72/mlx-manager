---
phase: 05-chat-multimodal-support
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - backend/mlx_manager/routers/chat.py
  - backend/mlx_manager/routers/__init__.py
  - backend/mlx_manager/main.py
  - frontend/src/routes/(protected)/chat/+page.svelte
  - frontend/src/lib/components/ui/thinking-bubble.svelte
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User sees assistant response stream in real-time"
    - "User sees thinking content stream live before response"
    - "Thinking section shows 'Thought for Xs' after completion"
    - "Attached images are encoded and sent to model"
    - "Non-multimodal models receive text-only messages"
  artifacts:
    - path: "backend/mlx_manager/routers/chat.py"
      provides: "Chat streaming endpoint"
      exports: ["router"]
    - path: "frontend/src/routes/(protected)/chat/+page.svelte"
      provides: "Streaming chat UI with multimodal support"
      contains: "EventSource"
    - path: "frontend/src/lib/components/ui/thinking-bubble.svelte"
      provides: "Thinking display with timing"
      contains: "Thought for"
  key_links:
    - from: "chat/+page.svelte"
      to: "/api/chat/completions"
      via: "EventSource SSE connection"
      pattern: "EventSource.*api/chat"
    - from: "backend chat.py"
      to: "mlx-openai-server"
      via: "httpx proxy"
      pattern: "httpx.*chat/completions"
---

<objective>
Implement streaming chat completions with multimodal support and thinking display.

Purpose: Enable real-time response streaming, live thinking content display, and proper image/video encoding for multimodal models. This transforms the chat from request-response to streaming SSE.

Output: Backend streaming endpoint and frontend SSE consumer with "Thought for Xs" display.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-chat-multimodal-support/05-CONTEXT.md
@.planning/phases/05-chat-multimodal-support/05-RESEARCH.md
@.planning/phases/05-chat-multimodal-support/05-01-SUMMARY.md
@frontend/src/routes/(protected)/chat/+page.svelte
@frontend/src/lib/components/ui/thinking-bubble.svelte
@backend/mlx_manager/routers/__init__.py
@backend/mlx_manager/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create backend chat streaming endpoint</name>
  <files>
    backend/mlx_manager/routers/chat.py
    backend/mlx_manager/routers/__init__.py
    backend/mlx_manager/main.py
  </files>
  <action>
Create a new chat router that proxies to mlx-openai-server with SSE streaming and thinking tag parsing.

**1. Create backend/mlx_manager/routers/chat.py:**

```python
"""Chat completions router with streaming support."""

import json
import time
from typing import AsyncGenerator

import httpx
from fastapi import APIRouter, Depends, HTTPException, Body
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlmodel.ext.asyncio.session import AsyncSession

from ..database import get_db
from ..models import ServerProfile
from .auth import get_current_user

router = APIRouter(prefix="/chat", tags=["chat"])


class ChatRequest(BaseModel):
    """Chat completion request."""

    profile_id: int
    messages: list[dict]  # OpenAI-compatible message format


@router.post("/completions")
async def chat_completions(
    request: ChatRequest = Body(...),
    db: AsyncSession = Depends(get_db),
    _user=Depends(get_current_user),
):
    """
    Stream chat completions from mlx-openai-server.

    Proxies to the MLX server running for the given profile.
    Parses thinking tags and emits typed chunks for frontend rendering.
    """
    # Get profile
    profile = await db.get(ServerProfile, request.profile_id)
    if not profile:
        raise HTTPException(status_code=404, detail="Profile not found")

    # Build MLX server URL
    server_url = f"http://{profile.host}:{profile.port}/v1/chat/completions"

    # Check if profile has reasoning parser enabled
    has_reasoning = bool(profile.reasoning_parser)

    async def generate() -> AsyncGenerator[str, None]:
        thinking_start: float | None = None
        in_thinking = False
        thinking_buffer = ""
        response_buffer = ""

        async with httpx.AsyncClient(timeout=300.0) as client:
            try:
                async with client.stream(
                    "POST",
                    server_url,
                    json={
                        "model": profile.model_path,
                        "messages": request.messages,
                        "stream": True,
                    },
                ) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        yield f"data: {json.dumps({'type': 'error', 'content': error_text.decode()})}\n\n"
                        return

                    async for line in response.aiter_lines():
                        if not line.startswith("data: "):
                            continue

                        data_str = line[6:]  # Remove "data: " prefix
                        if data_str == "[DONE]":
                            break

                        try:
                            data = json.loads(data_str)
                            delta = data.get("choices", [{}])[0].get("delta", {})
                            content = delta.get("content", "")

                            if not content:
                                continue

                            # Parse thinking tags if reasoning enabled
                            if has_reasoning:
                                i = 0
                                while i < len(content):
                                    # Check for opening tag
                                    if content[i:i+7] == "<think>":
                                        in_thinking = True
                                        thinking_start = time.time()
                                        i += 7
                                        continue

                                    # Check for closing tag
                                    if content[i:i+8] == "</think>":
                                        in_thinking = False
                                        duration = time.time() - thinking_start if thinking_start else 0
                                        yield f"data: {json.dumps({'type': 'thinking_done', 'duration': round(duration, 1)})}\n\n"
                                        i += 8
                                        continue

                                    # Emit character
                                    char = content[i]
                                    if in_thinking:
                                        yield f"data: {json.dumps({'type': 'thinking', 'content': char})}\n\n"
                                    else:
                                        yield f"data: {json.dumps({'type': 'response', 'content': char})}\n\n"
                                    i += 1
                            else:
                                # No reasoning parser, emit all as response
                                yield f"data: {json.dumps({'type': 'response', 'content': content})}\n\n"

                        except json.JSONDecodeError:
                            continue

                    yield f"data: {json.dumps({'type': 'done'})}\n\n"

            except httpx.ConnectError:
                yield f"data: {json.dumps({'type': 'error', 'content': 'Failed to connect to MLX server. Is it running?'})}\n\n"
            except Exception as e:
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )
```

**2. Update backend/mlx_manager/routers/__init__.py:**
Add the chat router to the imports and __all__ list:

```python
from .chat import router as chat_router
```

Add to __all__: `"chat_router"`

**3. Update backend/mlx_manager/main.py:**
Register the chat router. Find where other routers are included and add:

```python
from .routers import chat_router
# ...
app.include_router(chat_router, prefix="/api")
```
  </action>
  <verify>
1. Run `cd backend && ruff check .` - should pass
2. Run `cd backend && ruff format --check .` - should pass
3. Run `cd backend && mypy mlx_manager` - should pass
4. Run `cd backend && pytest tests/ -v` - existing tests should pass
  </verify>
  <done>
- Chat streaming endpoint at POST /api/chat/completions
- Thinking tag parsing with duration tracking
- SSE events with type: thinking | thinking_done | response | error | done
- Proper error handling for connection failures
  </done>
</task>

<task type="auto">
  <name>Task 2: Update ThinkingBubble for streaming with timing</name>
  <files>frontend/src/lib/components/ui/thinking-bubble.svelte</files>
  <action>
Update the ThinkingBubble component to support streaming thinking content and display "Thought for Xs" summary.

Key changes:
1. Add optional `duration` prop for "Thought for Xs" display
2. Add `streaming` prop to indicate if thinking is still in progress
3. Show "Thinking..." while streaming, "Thought for Xs" when done
4. Use bits-ui Collapsible for proper accessibility (already in project)

```svelte
<script lang="ts">
  import { Collapsible } from 'bits-ui';
  import { ChevronDown, ChevronRight, Brain, Loader2 } from 'lucide-svelte';

  interface Props {
    content: string;
    duration?: number; // seconds
    streaming?: boolean;
    defaultExpanded?: boolean;
  }

  let { content, duration, streaming = false, defaultExpanded = false }: Props = $props();

  let expanded = $state(getInitialExpanded());
  function getInitialExpanded() {
    return defaultExpanded || streaming; // Auto-expand while streaming
  }

  // Auto-collapse when streaming finishes
  $effect(() => {
    if (!streaming && duration !== undefined) {
      expanded = false;
    }
  });

  const label = $derived.by(() => {
    if (streaming) return 'Thinking...';
    if (duration !== undefined) return `Thought for ${duration.toFixed(1)}s`;
    return 'Thinking';
  });
</script>

<div class="my-2">
  <Collapsible.Root bind:open={expanded}>
    <Collapsible.Trigger
      class="flex items-center gap-2 text-sm text-muted-foreground hover:text-foreground transition-colors"
    >
      {#if expanded}
        <ChevronDown class="w-4 h-4" />
      {:else}
        <ChevronRight class="w-4 h-4" />
      {/if}
      {#if streaming}
        <Loader2 class="w-4 h-4 animate-spin" />
      {:else}
        <Brain class="w-4 h-4" />
      {/if}
      <span>{label}</span>
    </Collapsible.Trigger>
    <Collapsible.Content>
      <div class="mt-2 pl-6 border-l-2 border-muted text-sm text-muted-foreground italic whitespace-pre-wrap">
        {content}
      </div>
    </Collapsible.Content>
  </Collapsible.Root>
</div>
```
  </action>
  <verify>Run `npm run check` in frontend/ - should pass</verify>
  <done>
- ThinkingBubble shows "Thinking..." while streaming
- ThinkingBubble shows "Thought for Xs" after completion
- Auto-expands during streaming, auto-collapses when done
- Uses bits-ui Collapsible for accessibility
  </done>
</task>

<task type="auto">
  <name>Task 3: Update chat page for streaming and multimodal</name>
  <files>frontend/src/routes/(protected)/chat/+page.svelte</files>
  <action>
Replace the non-streaming fetch with EventSource SSE streaming. Add image encoding for multimodal messages.

**Key changes:**

1. **Add streaming state:**
```typescript
let streamingThinking = $state('');
let streamingResponse = $state('');
let thinkingDuration = $state<number | undefined>(undefined);
let isStreaming = $state(false);
let activeEventSource: EventSource | null = null;
```

2. **Add base64 encoding utility:**
```typescript
async function encodeFileAsBase64(file: File): Promise<string> {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => resolve(reader.result as string);
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
}
```

3. **Import ContentPart type:**
```typescript
import type { Attachment, ContentPart } from '$lib/api/types';
```

4. **Build multimodal message content:**
```typescript
async function buildMessageContent(text: string, attachments: Attachment[]): Promise<string | ContentPart[]> {
  if (attachments.length === 0) {
    return text;
  }

  const parts: ContentPart[] = [{ type: 'text', text }];

  for (const attachment of attachments) {
    const base64 = await encodeFileAsBase64(attachment.file);
    parts.push({
      type: 'image_url',
      image_url: { url: base64 }
    });
  }

  return parts;
}
```

5. **Replace handleSubmit with streaming version:**
```typescript
async function handleSubmit(e: Event) {
  e.preventDefault();
  if (!input.trim() || !selectedProfile || loading) return;

  const userMessage = input.trim();
  input = '';
  error = null;

  // Build message content (with attachments if any)
  const content = await buildMessageContent(userMessage, attachments);

  // Clear attachments
  for (const attachment of attachments) {
    URL.revokeObjectURL(attachment.preview);
  }
  attachments = [];

  // Add user message to UI (display text only for user messages)
  messages.push({ role: 'user', content: userMessage });

  // Reset streaming state
  streamingThinking = '';
  streamingResponse = '';
  thinkingDuration = undefined;
  isStreaming = true;
  loading = true;

  // Build messages array for API (use full content with images)
  const apiMessages = messages.slice(0, -1).map(m => ({
    role: m.role,
    content: m.content
  }));
  apiMessages.push({ role: 'user', content });

  // Connect to SSE
  const params = new URLSearchParams();
  // We need to POST, so use fetch for initial request, then parse SSE manually
  // Actually, EventSource only supports GET. Use fetch with ReadableStream instead.

  try {
    const response = await fetch('/api/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${authStore.token}`,
      },
      body: JSON.stringify({
        profile_id: selectedProfile.id,
        messages: apiMessages,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}`);
    }

    const reader = response.body?.getReader();
    if (!reader) throw new Error('No response body');

    const decoder = new TextDecoder();
    let buffer = '';

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split('\n');
      buffer = lines.pop() || ''; // Keep incomplete line in buffer

      for (const line of lines) {
        if (!line.startsWith('data: ')) continue;

        try {
          const data = JSON.parse(line.slice(6));

          switch (data.type) {
            case 'thinking':
              streamingThinking += data.content;
              break;
            case 'thinking_done':
              thinkingDuration = data.duration;
              break;
            case 'response':
              streamingResponse += data.content;
              break;
            case 'error':
              error = data.content;
              break;
            case 'done':
              // Finalize message
              const finalContent = streamingThinking
                ? `<think>${streamingThinking}</think>${streamingResponse}`
                : streamingResponse;
              messages.push({ role: 'assistant', content: finalContent });
              break;
          }
        } catch {
          // Ignore parse errors
        }
      }
    }
  } catch (e) {
    error = e instanceof Error ? e.message : 'Failed to send message';
    // Remove the user message on error
    messages.pop();
  } finally {
    loading = false;
    isStreaming = false;
  }
}
```

6. **Update the loading indicator to show streaming content:**
Replace the simple loader with streaming content display:

```svelte
{#if loading}
  <div class="flex gap-3">
    <div class="w-8 h-8 rounded-full bg-primary/10 flex items-center justify-center flex-shrink-0">
      <Bot class="w-5 h-5 text-primary" />
    </div>
    <div class="max-w-[80%] rounded-lg px-4 py-2 bg-muted">
      {#if streamingThinking}
        <ThinkingBubble
          content={streamingThinking}
          duration={thinkingDuration}
          streaming={thinkingDuration === undefined}
          defaultExpanded={true}
        />
      {/if}
      {#if streamingResponse}
        <Markdown content={streamingResponse} />
      {:else if !streamingThinking}
        <Loader2 class="w-5 h-5 animate-spin" />
      {/if}
    </div>
  </div>
{/if}
```

7. **Update ThinkingBubble in completed messages to pass duration:**
The existing parseThinking function already handles completed messages. The ThinkingBubble for completed messages doesn't need duration (it was rendered without streaming).

8. **Import authStore for auth header:**
```typescript
import { authStore } from '$stores';
```

9. **Add cancel streaming function (optional but nice):**
```typescript
function cancelStream() {
  // If we want to add a stop button later
}
```

Note: The messages in state store user-visible text only. The full multimodal content is built on-the-fly for the API call and includes base64 images.
  </action>
  <verify>
1. `cd frontend && npm run check` - should pass
2. `cd frontend && npm run lint` - should pass
3. Start dev servers and test:
   - Send a message, verify response streams in real-time
   - Test with a thinking model, verify thinking streams and shows duration
   - Test with attachments on multimodal model
  </verify>
  <done>
- Chat responses stream in real-time via SSE
- Thinking content streams live with "Thinking..." label
- "Thought for Xs" shows after thinking completes
- Attachments encoded as base64 and sent to model
- Proper error handling for connection issues
  </done>
</task>

</tasks>

<verification>
1. Backend quality: `cd backend && ruff check . && ruff format --check . && mypy mlx_manager && pytest -v`
2. Frontend quality: `cd frontend && npm run check && npm run lint`
3. Integration test:
   - Start a text model server, send message, verify streaming response
   - Start a thinking model server (with reasoning_parser), verify thinking + response stream
   - Start a multimodal model server, attach image, verify it's sent
</verification>

<success_criteria>
- POST /api/chat/completions streams SSE events
- Thinking content streams with timing
- "Thought for Xs" displays after thinking completes
- Images encoded and sent with messages
- No quality gate failures
</success_criteria>

<output>
After completion, create `.planning/phases/05-chat-multimodal-support/05-02-SUMMARY.md`
</output>
