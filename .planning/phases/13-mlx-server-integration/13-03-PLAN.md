---
phase: 13-mlx-server-integration
plan: 03
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - backend/mlx_manager/routers/chat.py
  - frontend/src/routes/(protected)/chat/+page.svelte
autonomous: true

must_haves:
  truths:
    - "Chat UI sends requests directly to embedded /v1/chat/completions"
    - "Thinking tag parsing still works"
    - "Tool calls still work"
    - "Streaming responses work correctly"
  artifacts:
    - path: "backend/mlx_manager/routers/chat.py"
      provides: "Direct inference via embedded server"
      contains: "generate_chat_completion"
    - path: "frontend/src/routes/(protected)/chat/+page.svelte"
      provides: "Updated chat endpoint"
  key_links:
    - from: "backend/mlx_manager/routers/chat.py"
      to: "backend/mlx_manager/mlx_server/services/inference.py"
      via: "direct function call"
      pattern: "generate_chat_completion"
---

<objective>
Wire Chat UI to embedded MLX Server

Purpose: Update the chat router and frontend to use the embedded MLX Server's /v1/chat/completions endpoint directly instead of proxying to an external mlx-openai-server process.

Output: Chat functionality works with the embedded server, maintaining existing features (streaming, thinking tags, tool calls).
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/mlx_manager/routers/chat.py
@backend/mlx_manager/mlx_server/api/v1/chat.py
@frontend/src/routes/(protected)/chat/+page.svelte
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite chat router to use embedded inference</name>
  <files>backend/mlx_manager/routers/chat.py</files>
  <action>
Rewrite chat.py to call the embedded MLX Server directly instead of proxying:

1. Remove httpx proxy code - no longer proxying to external server

2. Import inference functions from MLX Server:
   ```python
   from mlx_manager.mlx_server.services.inference import generate_chat_completion
   from mlx_manager.mlx_server.services.vision import generate_vision_completion
   from mlx_manager.mlx_server.models.detection import detect_model_type
   from mlx_manager.mlx_server.models.types import ModelType
   from mlx_manager.mlx_server.services.audit import audit_service
   ```

3. Update ChatRequest model:
   - Change from `profile_id: int` to `model: str` (model path directly)
   - Or keep profile_id and look up the model_path from the profile

4. Rewrite `chat_completions()` endpoint:
   ```python
   @router.post("/completions")
   async def chat_completions(
       request: ChatRequest,
       db: AsyncSession = Depends(get_db),
       _user=Depends(get_current_user),
   ):
       # Get model path from profile
       profile = await db.get(ServerProfile, request.profile_id)
       if not profile:
           raise HTTPException(status_code=404, detail="Profile not found")

       model_id = profile.model_path

       # Detect if vision model or has images
       model_type = detect_model_type(model_id)
       has_images = any(
           isinstance(m.get("content"), list) and
           any(p.get("type") == "image_url" for p in m["content"])
           for m in request.messages
       )

       async def generate():
           # Call embedded inference directly
           if has_images or model_type == ModelType.VISION:
               # Vision path
               gen = await generate_vision_completion(...)
           else:
               # Text path
               gen = await generate_chat_completion(
                   model_id=model_id,
                   messages=request.messages,
                   max_tokens=4096,
                   temperature=0.7,
                   stream=True,
               )

           # Stream responses with thinking tag parsing
           async for chunk in gen:
               # Parse thinking tags and emit appropriate event types
               ...
               yield f"data: {json.dumps(data)}\n\n"

           yield f"data: {json.dumps({'type': 'done'})}\n\n"

       return StreamingResponse(generate(), media_type="text/event-stream")
   ```

5. Preserve thinking tag parsing logic:
   - Keep the character-by-character <think>...</think> parsing
   - Keep support for reasoning_content field from server

6. Preserve tool call handling:
   - Keep the tool_calls detection and forwarding
   - Keep the tool_calls_done event

7. Update error handling:
   - Remove httpx.ConnectError handling (no external connection)
   - Add proper exception handling for inference errors

Key difference: Instead of streaming from httpx client, stream directly from generate_chat_completion() async generator.
  </action>
  <verify>
- `cd backend && python -c "from mlx_manager.routers.chat import router"` imports without error
- No references to httpx in chat.py (except maybe for tool execution if needed)
- Chat endpoint compiles without syntax errors
  </verify>
  <done>Chat router calls embedded inference directly</done>
</task>

<task type="auto">
  <name>Task 2: Update frontend chat to handle embedded server responses</name>
  <files>frontend/src/routes/(protected)/chat/+page.svelte</files>
  <action>
Update the chat page to work with the new embedded server model:

1. The current implementation POSTs to `/api/chat/completions` which is the MLX Manager chat router
   - This still works since we updated the backend in Task 1
   - No change needed to the fetch URL

2. However, review the response handling:
   - The SSE format from the rewritten backend should match what frontend expects
   - Verify: type='thinking', type='response', type='tool_call', type='done' events

3. Remove "server not running" error handling:
   - The embedded server is always running
   - Remove/update error messages like "Failed to connect to MLX server. Is it running?"
   - Change to "Model not available" or "Loading model..." type messages

4. Update the profile selection logic:
   - Currently shows only "running servers" - but now all profiles can use the embedded server
   - Change `runningProfiles` to just `profileStore.profiles` since any profile can use embedded inference
   - Or keep the concept but interpret "running" differently (model is loaded)

5. Consider showing model loading state:
   - When a model needs to load, it takes time
   - Show "Loading model..." indicator during first request to a new model

6. Update error messages to reflect embedded architecture:
   ```typescript
   // Old:
   'Failed to connect to MLX server. Is it running?'
   // New:
   'Model loading failed. Check if the model exists locally.'
   ```

Note: Most of the SSE parsing logic should remain unchanged since we're keeping the same event format on the backend.
  </action>
  <verify>
- Frontend builds without errors: `cd frontend && npm run build`
- No TypeScript errors: `cd frontend && npm run check`
- Profile selector shows all profiles (not just "running" ones)
  </verify>
  <done>Frontend chat page works with embedded server model</done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Start the server:
   ```bash
   cd backend && uvicorn mlx_manager.main:app --port 8080
   ```

2. Start frontend (dev mode):
   ```bash
   cd frontend && npm run dev
   ```

3. Test chat flow:
   - Navigate to Chat page
   - Select any profile (doesn't need to be "started")
   - Send a message
   - Verify response streams back
   - Verify thinking tags are parsed if using a thinking model

4. Test error cases:
   - Request with a model that doesn't exist locally
   - Should show appropriate error message

5. Verify audit logging:
   - Check that chat requests create audit log entries
</verification>

<success_criteria>
- Chat UI works with embedded server
- Streaming responses work
- Thinking tag parsing preserved
- Tool calls work (if using tool-capable model)
- No "server not running" errors for valid profiles
</success_criteria>

<output>
After completion, create `.planning/phases/13-mlx-server-integration/13-03-SUMMARY.md`
</output>
