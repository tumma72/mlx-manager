---
phase: 13-mlx-server-integration
plan: 03
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - backend/mlx_manager/routers/chat.py
  - frontend/src/routes/(protected)/chat/+page.svelte
autonomous: true

must_haves:
  truths:
    - "Chat UI sends requests directly to embedded /v1/chat/completions"
    - "Thinking tag parsing still works"
    - "Tool calls still work"
    - "Streaming responses work correctly"
  artifacts:
    - path: "backend/mlx_manager/routers/chat.py"
      provides: "Direct inference via embedded server"
      contains: "generate_chat_completion"
    - path: "frontend/src/routes/(protected)/chat/+page.svelte"
      provides: "Updated chat endpoint"
  key_links:
    - from: "backend/mlx_manager/routers/chat.py"
      to: "backend/mlx_manager/mlx_server/services/inference.py"
      via: "direct async generator consumption"
      pattern: "async for chunk in"
---

<objective>
Wire Chat UI to embedded MLX Server

Purpose: Update the chat router and frontend to use the embedded MLX Server's /v1/chat/completions endpoint directly instead of proxying to an external mlx-openai-server process.

Output: Chat functionality works with the embedded server, maintaining existing features (streaming, thinking tags, tool calls).
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/mlx_manager/routers/chat.py
@backend/mlx_manager/mlx_server/api/v1/chat.py
@frontend/src/routes/(protected)/chat/+page.svelte
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite chat router to use embedded inference</name>
  <files>backend/mlx_manager/routers/chat.py</files>
  <action>
Rewrite chat.py to call the embedded MLX Server directly instead of proxying. This requires transitioning from httpx streaming to direct async generator consumption.

**Step 1: Remove httpx proxy pattern**
Delete all httpx-related code:
```python
# DELETE these patterns:
async with httpx.AsyncClient() as client:
    async with client.stream("POST", url, ...) as response:
        async for chunk in response.aiter_lines():
            ...
```

**Step 2: Import inference functions from MLX Server**
```python
from mlx_manager.mlx_server.services.inference import generate_chat_completion
from mlx_manager.mlx_server.services.vision import generate_vision_completion
from mlx_manager.mlx_server.models.detection import detect_model_type
from mlx_manager.mlx_server.models.types import ModelType
from mlx_manager.mlx_server.services.audit import audit_service
```

**Step 3: Replace httpx streaming with direct async generator consumption**

OLD pattern (httpx proxy):
```python
async with client.stream("POST", url, json=payload) as response:
    async for line in response.aiter_lines():
        if line.startswith("data: "):
            data = json.loads(line[6:])
            yield f"data: {json.dumps(data)}\n\n"
```

NEW pattern (direct generator):
```python
# Call the inference function directly - it returns an async generator
generator = generate_chat_completion(
    model_id=model_id,
    messages=request.messages,
    max_tokens=4096,
    temperature=0.7,
    stream=True,
)

# Consume the async generator directly (NOT via httpx)
async for chunk in generator:
    # chunk is already a dict with 'choices' containing 'delta'
    # Parse thinking tags and emit appropriate event types
    content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
    yield f"data: {json.dumps({'type': 'response', 'content': content})}\n\n"
```

**Step 4: Update error handling**
Remove httpx-specific error handling:
```python
# DELETE:
except httpx.ConnectError:
    yield f"data: {json.dumps({'type': 'error', 'message': 'Failed to connect'})}\n\n"

# ADD:
except Exception as e:
    logger.error(f"Inference error: {e}")
    yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
```

**Step 5: Full rewritten endpoint structure**
```python
@router.post("/completions")
async def chat_completions(
    request: ChatRequest,
    db: AsyncSession = Depends(get_db),
    _user=Depends(get_current_user),
):
    # Get model path from profile
    profile = await db.get(ServerProfile, request.profile_id)
    if not profile:
        raise HTTPException(status_code=404, detail="Profile not found")

    model_id = profile.model_path

    # Detect if vision model or has images
    model_type = detect_model_type(model_id)
    has_images = any(
        isinstance(m.get("content"), list) and
        any(p.get("type") == "image_url" for p in m["content"])
        for m in request.messages
    )

    async def generate():
        try:
            # Call embedded inference directly - returns async generator
            if has_images or model_type == ModelType.VISION:
                gen = generate_vision_completion(...)
            else:
                gen = generate_chat_completion(
                    model_id=model_id,
                    messages=request.messages,
                    max_tokens=4096,
                    temperature=0.7,
                    stream=True,
                )

            # Stream directly from async generator (no httpx layer)
            async for chunk in gen:
                # Process chunk and emit SSE events
                # Keep existing thinking tag parsing logic
                ...
                yield f"data: {json.dumps(data)}\n\n"

            yield f"data: {json.dumps({'type': 'done'})}\n\n"

        except Exception as e:
            logger.error(f"Chat inference error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

**Step 6: Preserve existing functionality**
- Keep the character-by-character <think>...</think> parsing
- Keep support for reasoning_content field from server
- Keep tool_calls detection and forwarding
- Keep tool_calls_done event

Key difference: Instead of parsing SSE from httpx response, consume the async generator directly from generate_chat_completion().
  </action>
  <verify>
- `cd backend && python -c "from mlx_manager.routers.chat import router"` imports without error
- `grep -c "httpx" backend/mlx_manager/routers/chat.py` returns 0 (no httpx references)
- `grep -c "async for chunk in" backend/mlx_manager/routers/chat.py` returns at least 1
- Chat endpoint compiles without syntax errors
  </verify>
  <done>Chat router calls embedded inference directly via async generator</done>
</task>

<task type="auto">
  <name>Task 2: Update frontend chat to handle embedded server responses</name>
  <files>frontend/src/routes/(protected)/chat/+page.svelte</files>
  <action>
Update the chat page to work with the new embedded server model:

1. The current implementation POSTs to `/api/chat/completions` which is the MLX Manager chat router
   - This still works since we updated the backend in Task 1
   - No change needed to the fetch URL

2. However, review the response handling:
   - The SSE format from the rewritten backend should match what frontend expects
   - Verify: type='thinking', type='response', type='tool_call', type='done' events

3. Remove "server not running" error handling:
   - The embedded server is always running
   - Remove/update error messages like "Failed to connect to MLX server. Is it running?"
   - Change to "Model not available" or "Loading model..." type messages

4. Update the profile selection logic:
   - Currently shows only "running servers" - but now all profiles can use the embedded server
   - Change `runningProfiles` to just `profileStore.profiles` since any profile can use embedded inference
   - Or keep the concept but interpret "running" differently (model is loaded)

5. Consider showing model loading state:
   - When a model needs to load, it takes time
   - Show "Loading model..." indicator during first request to a new model

6. Update error messages to reflect embedded architecture:
   ```typescript
   // Old:
   'Failed to connect to MLX server. Is it running?'
   // New:
   'Model loading failed. Check if the model exists locally.'
   ```

Note: Most of the SSE parsing logic should remain unchanged since we're keeping the same event format on the backend.
  </action>
  <verify>
- Frontend builds without errors: `cd frontend && npm run build`
- No TypeScript errors: `cd frontend && npm run check`
- Profile selector shows all profiles (not just "running" ones)
- `grep -c "Is it running" frontend/src/routes/` returns 0 (old error message removed)
  </verify>
  <done>Frontend chat page works with embedded server model</done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Start the server:
   ```bash
   cd backend && uvicorn mlx_manager.main:app --port 10242
   ```

2. Start frontend (dev mode):
   ```bash
   cd frontend && npm run dev
   ```

3. Test chat flow (REQUIRED - not just build check):
   - Navigate to http://localhost:5173/chat
   - Select any profile (doesn't need to be "started")
   - Send a message like "Hello"
   - Verify response streams back character by character
   - Verify no "server not running" errors appear

4. Test thinking tags (if using thinking model):
   - Send a complex question
   - Verify thinking content appears in collapsible section
   - Verify response content appears after thinking

5. Test error cases:
   - Request with a model that doesn't exist locally
   - Should show "Model not available" or similar (not "server not running")

6. Verify audit logging:
   - Check that chat requests create audit log entries
</verification>

<success_criteria>
- Chat UI works with embedded server
- Streaming responses work (character by character)
- Thinking tag parsing preserved
- Tool calls work (if using tool-capable model)
- No "server not running" errors for valid profiles
- httpx completely removed from chat.py
</success_criteria>

<output>
After completion, create `.planning/phases/13-mlx-server-integration/13-03-SUMMARY.md`
</output>
