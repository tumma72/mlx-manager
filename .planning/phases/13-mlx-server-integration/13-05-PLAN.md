---
phase: 13-mlx-server-integration
plan: 05
type: execute
wave: 3
depends_on: ["13-02", "13-03", "13-04"]
files_modified:
  - backend/tests/test_main.py
  - backend/tests/mlx_server/test_profiles.py
  - backend/pyproject.toml
autonomous: false

must_haves:
  truths:
    - "No references to mlx-openai-server in codebase"
    - "All tests pass or are updated for embedded model"
    - "Audit logs populate during chat"
    - "Application starts and serves requests correctly"
  artifacts:
    - path: "backend/tests/test_main.py"
      provides: "Updated tests for embedded server"
  key_links: []
---

<objective>
Verify complete integration and update tests

Purpose: Ensure the integration is complete by checking for any remaining mlx-openai-server references, updating broken tests, and verifying end-to-end functionality including audit logging.

Output: Clean codebase with no legacy references, passing tests, and working end-to-end chat flow.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Remove mlx-openai-server dependency</name>
  <files>backend/pyproject.toml</files>
  <action>
Remove mlx-openai-server from dependencies:

1. Check pyproject.toml for mlx-openai-server dependency
2. Remove it from [project.dependencies] if present
3. Run `uv lock` or `pip install -e .` to update lock file

Also check for any other related dependencies that are no longer needed:
- Any parsers or utilities from mlx-openai-server ecosystem

The mlx-lm, mlx-vlm, and mlx-embeddings dependencies should remain (used by MLX Server).
  </action>
  <verify>
- `grep "mlx-openai-server" backend/pyproject.toml` returns nothing
- `cd backend && pip install -e .` completes without mlx-openai-server
  </verify>
  <done>mlx-openai-server dependency removed from project</done>
</task>

<task type="auto">
  <name>Task 2: Search and fix remaining mlx-openai-server references</name>
  <files>Various</files>
  <action>
Comprehensive search for any remaining references:

1. Search Python files:
   ```bash
   grep -r "mlx-openai-server" backend/
   grep -r "mlx_openai_server" backend/
   ```

2. Search for subprocess-related patterns:
   ```bash
   grep -r "subprocess.Popen" backend/mlx_manager/
   grep -r "server_manager" backend/mlx_manager/
   ```

3. Search documentation:
   ```bash
   grep -r "mlx-openai-server" docs/
   grep -r "mlx-openai-server" CLAUDE.md
   ```

4. Search frontend:
   ```bash
   grep -r "mlx-openai-server" frontend/
   ```

5. For each reference found:
   - If in test file: Update or remove test
   - If in docs: Update documentation
   - If in code: Fix the reference

6. Update CLAUDE.md if it mentions mlx-openai-server:
   - Update to describe embedded MLX Server
   - Remove subprocess management references
   - Update architecture description
  </action>
  <verify>
- `grep -r "mlx-openai-server" . --include="*.py" --include="*.md" --include="*.svelte" 2>/dev/null | grep -v node_modules | grep -v .git` returns minimal or no results
- Any remaining references are intentional (e.g., historical comments in CHANGELOG)
  </verify>
  <done>No active mlx-openai-server references remain in codebase</done>
</task>

<task type="auto">
  <name>Task 3: Update broken tests</name>
  <files>backend/tests/</files>
  <action>
Identify and fix tests broken by the integration changes:

1. Run full test suite to identify failures:
   ```bash
   cd backend && pytest -v 2>&1 | tee test_output.txt
   ```

2. Common test updates needed:

   a) Tests that mock server_manager:
      - Remove these mocks
      - Update to test embedded server behavior
      - Or remove tests that tested subprocess management

   b) Tests for servers router:
      - Update to test new simplified endpoints
      - Remove tests for start/stop/restart subprocess endpoints

   c) Tests for chat router:
      - Update to test direct inference calls
      - Remove httpx proxy mocking

   d) Tests that check for mlx-openai-server binary:
      - Remove or update these checks

3. Add new tests for embedded behavior:
   - Test that /v1/models returns loaded models
   - Test that settings changes affect pool
   - Test that chat works without external server

4. Update test fixtures if needed:
   - Remove fixtures that set up server_manager mocks
   - Add fixtures for model pool mocking

Run tests after each fix to track progress:
```bash
cd backend && pytest -v -x  # Stop on first failure
```
  </action>
  <verify>
- `cd backend && pytest -v` passes (or shows only expected skips)
- No tests reference server_manager
- No tests try to start subprocess
  </verify>
  <done>All tests pass with embedded server model</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete MLX Server integration:
1. MLX Server mounted at /v1 prefix in MLX Manager app
2. Legacy subprocess management code removed
3. Chat UI works with embedded server
4. Settings control embedded server configuration
5. Tests updated for new architecture
  </what-built>
  <how-to-verify>
1. Start the server:
   ```bash
   cd backend && uvicorn mlx_manager.main:app --port 8080
   ```

2. Start frontend:
   ```bash
   cd frontend && npm run dev
   ```

3. Navigate to http://localhost:5173

4. Test complete flow:
   a. Go to Chat page
   b. Select any profile (doesn't need to be "started" anymore)
   c. Send a message like "Hello, how are you?"
   d. Verify response streams back

5. Check Settings:
   a. Go to Settings page
   b. Verify Model Pool settings show live pool status
   c. Try changing memory limit and verify it applies

6. Check Audit Logs:
   a. After sending chat messages, check the Admin panel
   b. Verify audit logs show the chat requests

7. Verify no "server not running" type errors appear when using valid models
  </how-to-verify>
  <resume-signal>Type "approved" if integration works end-to-end, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
After completing all tasks:

1. Final reference check:
   ```bash
   # Should return nothing or only historical references
   grep -rn "mlx-openai-server" . --include="*.py" --include="*.md" --include="*.svelte" 2>/dev/null | grep -v node_modules | grep -v .git | grep -v CHANGELOG
   ```

2. Full test suite:
   ```bash
   cd backend && pytest -v --tb=short
   ```

3. Type checking:
   ```bash
   cd backend && mypy mlx_manager
   ```

4. Lint check:
   ```bash
   cd backend && ruff check .
   ```

5. Frontend checks:
   ```bash
   cd frontend && npm run check && npm run lint
   ```

6. End-to-end manual test:
   - Start server
   - Load chat page
   - Send message
   - Receive response
   - Check audit logs show entry
</verification>

<success_criteria>
- No mlx-openai-server references in active code
- All tests pass
- Type checking passes
- Linting passes
- Chat works end-to-end with embedded server
- Audit logs populate during chat
- Settings changes affect embedded server
</success_criteria>

<output>
After completion, create `.planning/phases/13-mlx-server-integration/13-05-SUMMARY.md`
</output>
