---
phase: 13-mlx-server-integration
plan: 04
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - backend/mlx_manager/routers/settings.py
  - backend/mlx_manager/mlx_server/models/pool.py
  - backend/mlx_manager/mlx_server/services/cloud/router.py
autonomous: true

must_haves:
  truths:
    - "Model Pool Settings changes update the embedded server's model pool"
    - "Cloud Provider Settings connect to the embedded server's routing"
    - "Memory limit changes affect actual memory management"
    - "Preload models actually load in the model pool"
  artifacts:
    - path: "backend/mlx_manager/routers/settings.py"
      provides: "Settings that apply to embedded server"
      contains: "get_model_pool"
    - path: "backend/mlx_manager/mlx_server/models/pool.py"
      provides: "Dynamic configuration support"
      contains: "update_config"
  key_links:
    - from: "backend/mlx_manager/routers/settings.py"
      to: "backend/mlx_manager/mlx_server/models/pool.py"
      via: "direct pool configuration"
      pattern: "model_pool\\.update"
---

<objective>
Wire Settings to embedded MLX Server configuration

Purpose: Make the Model Pool Settings and Cloud Provider Settings actually control the embedded server's behavior rather than storing configuration that was never used.

Output: Settings changes immediately affect the running embedded server - memory limits, preload models, and cloud routing rules.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/mlx_manager/routers/settings.py
@backend/mlx_manager/mlx_server/models/pool.py
@backend/mlx_manager/mlx_server/services/cloud/router.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dynamic configuration to ModelPoolManager</name>
  <files>backend/mlx_manager/mlx_server/models/pool.py</files>
  <action>
Add methods to ModelPoolManager for runtime configuration updates:

1. Add `update_memory_limit(self, memory_gb: float = None, memory_pct: float = None)`:
   ```python
   def update_memory_limit(
       self,
       memory_gb: float | None = None,
       memory_pct: float | None = None
   ) -> None:
       """Update memory limit at runtime.

       Args:
           memory_gb: Absolute memory limit in GB
           memory_pct: Memory limit as percentage of system memory (0.0-1.0)
       """
       if memory_pct is not None:
           self._memory_limit_pct = memory_pct
           self.max_memory_gb = self._get_effective_memory_limit()
       elif memory_gb is not None:
           self._memory_limit_pct = None
           self.max_memory_gb = memory_gb

       # Update MLX memory limit
       from mlx_manager.mlx_server.utils.memory import set_memory_limit
       set_memory_limit(self.max_memory_gb)

       logger.info(f"Memory limit updated to {self.max_memory_gb:.1f}GB")
   ```

2. Add `update_max_models(self, max_models: int)`:
   ```python
   def update_max_models(self, max_models: int) -> None:
       """Update maximum hot models at runtime."""
       self.max_models = max_models
       logger.info(f"Max models updated to {max_models}")
   ```

3. Add async `apply_preload_list(self, model_ids: list[str])`:
   ```python
   async def apply_preload_list(self, model_ids: list[str]) -> dict[str, str]:
       """Preload specified models, unload others.

       Returns dict of model_id -> status ('loaded', 'failed', 'already_loaded')
       """
       results = {}

       # Preload new models
       for model_id in model_ids:
           if self.is_loaded(model_id):
               self._models[model_id].preloaded = True
               results[model_id] = "already_loaded"
           else:
               try:
                   await self.preload_model(model_id)
                   results[model_id] = "loaded"
               except Exception as e:
                   logger.error(f"Failed to preload {model_id}: {e}")
                   results[model_id] = f"failed: {e}"

       # Mark models not in preload list as evictable
       for model_id, loaded in self._models.items():
           if model_id not in model_ids:
               loaded.preloaded = False

       return results
   ```

4. Add `get_status(self)` method for status reporting:
   ```python
   def get_status(self) -> dict:
       """Get current pool status."""
       return {
           "max_memory_gb": self.max_memory_gb,
           "current_memory_gb": self._current_memory_gb(),
           "max_models": self.max_models,
           "loaded_models": [
               {
                   "model_id": m.model_id,
                   "size_gb": m.size_gb,
                   "preloaded": m.preloaded,
                   "last_used": m.last_used,
               }
               for m in self._models.values()
           ],
       }
   ```
  </action>
  <verify>
- `cd backend && python -c "from mlx_manager.mlx_server.models.pool import ModelPoolManager; p = ModelPoolManager(); p.update_memory_limit(memory_gb=24)"`
- No import errors
  </verify>
  <done>ModelPoolManager supports runtime configuration updates</done>
</task>

<task type="auto">
  <name>Task 2: Wire settings router to model pool</name>
  <files>backend/mlx_manager/routers/settings.py</files>
  <action>
Update settings.py pool endpoints to actually apply changes to the embedded server:

1. Import model pool:
   ```python
   from mlx_manager.mlx_server.models.pool import get_model_pool
   ```

2. Update `get_pool_config()` endpoint:
   - In addition to database config, include live pool status
   - Add current memory usage, loaded models count

3. Update `update_pool_config()` endpoint:
   - After saving to database, apply changes to the running pool:
   ```python
   @router.put("/pool", response_model=ServerConfigResponse)
   async def update_pool_config(
       current_user: Annotated[User, Depends(get_current_user)],
       data: ServerConfigUpdate,
       session: AsyncSession = Depends(get_db),
   ):
       # ... existing database update code ...

       # Apply changes to running pool
       try:
           pool = get_model_pool()

           # Update memory limit
           if data.memory_limit_value is not None:
               if config.memory_limit_mode == "percent":
                   pool.update_memory_limit(memory_pct=data.memory_limit_value / 100)
               else:  # "gb"
                   pool.update_memory_limit(memory_gb=data.memory_limit_value)

           # Apply preload list
           if data.preload_models is not None:
               preload_result = await pool.apply_preload_list(data.preload_models)
               logger.info(f"Preload result: {preload_result}")

       except RuntimeError:
           # Pool not initialized (shouldn't happen in embedded mode)
           logger.warning("Model pool not initialized, settings saved but not applied")

       return ServerConfigResponse(...)
   ```

4. Add new endpoint for pool status:
   ```python
   @router.get("/pool/status")
   async def get_pool_status(
       current_user: Annotated[User, Depends(get_current_user)],
   ):
       """Get live model pool status."""
       try:
           pool = get_model_pool()
           return pool.get_status()
       except RuntimeError:
           return {"status": "not_initialized"}
   ```

5. Ensure cloud provider settings are used by the router:
   - The BackendRouter in mlx_server already reads from database
   - Verify it uses the same database as MLX Manager
  </action>
  <verify>
- `cd backend && python -c "from mlx_manager.routers.settings import router"` imports without error
- No syntax errors in the file
- PUT /api/settings/pool endpoint calls pool.update_memory_limit
  </verify>
  <done>Settings router applies changes to embedded server model pool</done>
</task>

<task type="auto">
  <name>Task 3: Ensure cloud router uses shared database</name>
  <files>backend/mlx_manager/mlx_server/services/cloud/router.py</files>
  <action>
Verify and update BackendRouter to use the shared MLX Manager database:

1. Check how BackendRouter loads credentials and routing rules:
   - It should read from CloudCredential and BackendMapping models
   - These should be in the MLX Manager's database

2. If BackendRouter uses its own database connection:
   - Update to use the shared database session
   - Import `get_db` from mlx_manager.database (not mlx_server.database)

3. Add method to refresh routing rules from database:
   ```python
   async def refresh_rules(self) -> None:
       """Reload routing rules from database."""
       # Re-fetch BackendMapping entries ordered by priority
       ...
   ```

4. Ensure credential decryption uses shared encryption service:
   ```python
   from mlx_manager.services.encryption_service import decrypt_api_key
   ```

5. Consider adding cache invalidation:
   - When settings router updates rules, call router.refresh_rules()
   - Or use a TTL-based cache that auto-refreshes

Note: The key goal is that changes made via /api/settings/rules endpoints actually affect routing behavior in the embedded MLX Server.
  </action>
  <verify>
- Cloud routing rules created via API affect actual request routing
- Provider credentials saved via API are used for cloud requests
- `grep -r "get_db" backend/mlx_manager/mlx_server/services/cloud/` shows shared database usage
  </verify>
  <done>Cloud router uses shared database for credentials and routing rules</done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Start the server:
   ```bash
   cd backend && uvicorn mlx_manager.main:app --port 8080
   ```

2. Test pool configuration:
   ```bash
   # Get current pool status
   curl -H "Authorization: Bearer $TOKEN" http://localhost:8080/api/settings/pool/status

   # Update memory limit
   curl -X PUT -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"memory_limit_mode": "gb", "memory_limit_value": 24}' \
     http://localhost:8080/api/settings/pool

   # Verify change applied
   curl -H "Authorization: Bearer $TOKEN" http://localhost:8080/api/settings/pool/status
   ```

3. Test preload models:
   ```bash
   # Set preload list (requires model to be downloaded)
   curl -X PUT -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"preload_models": ["mlx-community/Qwen3-0.6B-4bit"]}' \
     http://localhost:8080/api/settings/pool

   # Verify model loaded
   curl -H "Authorization: Bearer $TOKEN" http://localhost:8080/api/settings/pool/status
   ```

4. Test cloud routing:
   - Create a routing rule via API
   - Make a chat request that matches the rule
   - Verify request routes to cloud provider (if credentials configured)
</verification>

<success_criteria>
- Memory limit changes take immediate effect on model pool
- Preload models setting actually loads models
- Cloud routing rules affect request routing
- Pool status endpoint shows live information
</success_criteria>

<output>
After completion, create `.planning/phases/13-mlx-server-integration/13-04-SUMMARY.md`
</output>
