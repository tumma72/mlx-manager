---
phase: 09-continuous-batching
plan: 05
type: execute
wave: 3
depends_on: ["09-03", "09-04"]
files_modified:
  - backend/mlx_manager/mlx_server/services/batching/scheduler.py
  - backend/mlx_manager/mlx_server/services/batching/batch_inference.py
  - backend/tests/mlx_server/batching/test_batch_inference.py
autonomous: true

must_haves:
  truths:
    - "Scheduler generates tokens for multiple requests per step"
    - "Each request maintains isolated KV cache state"
    - "Token generation uses Queue-based threading for MLX Metal affinity"
    - "Stop tokens detected per-request (from model adapter)"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/batch_inference.py"
      provides: "Batch token generation using mlx-lm"
      exports: ["BatchInferenceEngine"]
  key_links:
    - from: "scheduler.py"
      to: "batch_inference.py"
      via: "BatchInferenceEngine for _batch_step"
      pattern: "BatchInferenceEngine"
    - from: "batch_inference.py"
      to: "inference.py patterns"
      via: "Queue-based threading for MLX"
      pattern: "threading\\.Thread"
---

<objective>
Batch inference engine with MLX generation integration

Purpose: Connect the scheduler to actual MLX model generation, ensuring each request maintains isolated KV state and respects Metal thread affinity.

Output: BatchInferenceEngine that generates tokens for multiple requests per step while maintaining cache isolation
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-continuous-batching/09-CONTEXT.md
@.planning/phases/09-continuous-batching/09-RESEARCH.md

Key pitfalls from RESEARCH.md:
- MLX Thread Affinity Violation: Use dedicated Thread + Queue
- KV Cache State Contamination: Each request needs isolated state
- Request Join Mid-Generation: Wait for step to complete

Prior plan artifacts:
@backend/mlx_manager/mlx_server/services/batching/scheduler.py (ContinuousBatchingScheduler)
@backend/mlx_manager/mlx_server/services/batching/prefix_cache.py (PrefixCache)

Existing patterns:
@backend/mlx_manager/mlx_server/services/inference.py (Queue-based threading, make_sampler)
@backend/mlx_manager/mlx_server/models/adapters/base.py (get_stop_tokens)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batch inference engine</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/batch_inference.py
  </files>
  <action>
Create `BatchInferenceEngine` for multi-request token generation:

1. `__init__(self, model, tokenizer, adapter, prefix_cache: PrefixCache | None = None)`:
   - Store model, tokenizer, adapter
   - `self._prefix_cache = prefix_cache`
   - `self._stop_token_ids = set(adapter.get_stop_tokens(tokenizer))`
   - `self._generation_lock = threading.Lock()` - MLX thread safety

2. `generate_tokens_for_batch(requests: list[BatchRequest], sampler) -> dict[str, tuple[str, int, bool]]`:
   """Generate one token for each request in batch.

   Returns dict mapping request_id -> (token_text, token_id, is_stop)

   CRITICAL: This runs in a dedicated thread to maintain MLX Metal context.
   """
   - For each request:
     - Build prompt from prompt_tokens + generated_tokens
     - Call mlx_lm's generate_step or stream_generate for single token
     - Check if token_id in stop_token_ids
     - Store result

   NOTE: mlx-lm doesn't support true batched generation yet (Issue #548).
   For now, generate sequentially but within single thread to maintain
   Metal context. Future: native batch KV cache when mlx-lm supports it.

3. Helper: `_prepare_request_context(request: BatchRequest) -> str`:
   - Decode prompt_tokens + generated_tokens back to text
   - Or maintain text incrementally on request object

4. Integration with prefix cache:
   - Before generating, check prefix_cache.lookup_prefix()
   - If matched, skip KV computation for matched portion
   - After generating new blocks, cache_prefix() for reuse

5. Thread-safe wrapper using Queue pattern (CRITICAL - must match inference.py):
   - `async generate_batch_step(requests: list[BatchRequest], sampler) -> dict`:
     - Create `result_queue: Queue[dict | Exception]` for passing results
     - Define `run_generation()` function that:
       - Calls `generate_tokens_for_batch()` synchronously
       - Puts result dict in queue, or exception on error
     - Start `threading.Thread(target=run_generation, daemon=True)`
     - Use `loop.run_in_executor(None, lambda: result_queue.get(timeout=60))`
     - Return results dict or raise exception
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching.batch_inference import BatchInferenceEngine
import inspect

# Verify class exists and has required structure
assert hasattr(BatchInferenceEngine, 'generate_batch_step'), 'Missing generate_batch_step'
assert hasattr(BatchInferenceEngine, 'generate_tokens_for_batch'), 'Missing generate_tokens_for_batch'

# Verify threading pattern is implemented by checking source
source = inspect.getsource(BatchInferenceEngine)
assert 'threading.Thread' in source, 'Missing threading.Thread for MLX affinity'
assert 'Queue' in source, 'Missing Queue for thread communication'
assert 'daemon=True' in source, 'Thread should be daemon'

print('BatchInferenceEngine with Queue-based threading pattern OK')
"`
  </verify>
  <done>BatchInferenceEngine with thread-safe batch generation using Queue pattern</done>
</task>

<task type="auto">
  <name>Task 2: Integrate batch inference into scheduler</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/scheduler.py
  </files>
  <action>
Connect scheduler to BatchInferenceEngine:

1. Update `__init__` to accept optional model/tokenizer/adapter:
   - `model = None, tokenizer = None, adapter = None`
   - Create `BatchInferenceEngine` if model provided
   - Create `PrefixCache` if block_manager provided

2. Implement `_batch_step()` properly:
```python
async def _batch_step(self):
    """Generate one token for all running requests."""
    if not self._inference_engine:
        return  # No model loaded

    # Create sampler (reuse for batch)
    from mlx_lm.sample_utils import make_sampler
    sampler = make_sampler(temp=1.0, top_p=1.0)  # TODO: per-request settings

    # Generate tokens for batch
    results = await self._inference_engine.generate_batch_step(
        self.running, sampler
    )

    # Process results
    for request in self.running:
        if request.request_id in results:
            token_text, token_id, is_stop = results[request.request_id]

            if not is_stop:
                # Add token to request's generated list
                request.generated_tokens.append(token_id)
                # Push to output queue for streaming
                await request.output_queue.put(token_text)
            else:
                # Mark for completion (don't yield stop token)
                request.status = RequestStatus.COMPLETED
```

3. Add `set_model(model, tokenizer, adapter)` method:
   - Allow setting/switching model at runtime
   - Creates new BatchInferenceEngine
   - Creates new PrefixCache

4. Handle per-request sampling parameters:
   - Store temperature/top_p on BatchRequest
   - Pass to sampler creation (future: per-request samplers)
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching import ContinuousBatchingScheduler, PagedBlockManager
from mlx_manager.mlx_server.services.batching.batch_inference import BatchInferenceEngine
import inspect

pm = PagedBlockManager(num_blocks=100)
scheduler = ContinuousBatchingScheduler(model_id='test', block_manager=pm)

# Verify set_model exists
assert hasattr(scheduler, 'set_model'), 'Missing set_model method'

# Verify BatchInferenceEngine uses Queue-based threading
source = inspect.getsource(BatchInferenceEngine)
assert 'threading.Thread' in source, 'BatchInferenceEngine missing threading.Thread'
assert 'Queue' in source, 'BatchInferenceEngine missing Queue'
assert 'run_in_executor' in source or 'get(timeout' in source, 'Missing async result retrieval'

# Verify scheduler references BatchInferenceEngine
sched_source = inspect.getsource(ContinuousBatchingScheduler)
assert 'BatchInferenceEngine' in sched_source or '_inference_engine' in sched_source, 'Scheduler missing inference engine reference'

print('Integration with Queue-based threading pattern OK')
"`
  </verify>
  <done>Scheduler integrated with BatchInferenceEngine, threading pattern verified</done>
</task>

<task type="auto">
  <name>Task 3: Add batch inference tests</name>
  <files>
    backend/tests/mlx_server/batching/test_batch_inference.py
  </files>
  <action>
Create tests for batch inference (mock MLX):

1. Test BatchInferenceEngine initialization:
   - Stores model, tokenizer, adapter
   - Extracts stop tokens from adapter

2. Test generate_tokens_for_batch (mocked):
   - Mock mlx_lm.stream_generate to return predictable tokens
   - Verify each request gets a token
   - Verify stop token detection

3. Test prefix cache integration:
   - Mock prefix_cache.lookup_prefix to return cached blocks
   - Verify generation skips cached portion
   - Verify new blocks cached after generation

4. Test thread safety:
   - Verify generation runs in dedicated thread
   - Verify Queue-based result passing

5. Test scheduler + inference integration:
   - Mock BatchInferenceEngine.generate_batch_step
   - Verify scheduler calls it with running requests
   - Verify tokens pushed to output_queue

Use unittest.mock for MLX components.
Focus on control flow and integration - actual MLX tested in Plan 06.
  </action>
  <verify>`cd backend && python -m pytest tests/mlx_server/batching/test_batch_inference.py -v` all tests pass</verify>
  <done>Unit tests for batch inference engine and scheduler integration</done>
</task>

</tasks>

<verification>
1. `cd backend && python -m pytest tests/mlx_server/batching/ -v` - all batching tests pass
2. `cd backend && ruff check mlx_manager/mlx_server/services/batching/` - no lint errors
3. `cd backend && mypy mlx_manager/mlx_server/services/batching/` - no type errors
</verification>

<success_criteria>
- BatchInferenceEngine generates tokens for multiple requests
- MLX Metal thread affinity respected via dedicated thread + Queue pattern
- Each request maintains isolated state (no cross-contamination)
- Stop tokens detected per-request using adapter
- Prefix cache reduces redundant computation
- All quality gates pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-05-SUMMARY.md`
</output>
