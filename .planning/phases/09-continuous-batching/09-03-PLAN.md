---
phase: 09-continuous-batching
plan: 03
type: execute
wave: 2
depends_on: ["09-02"]
files_modified:
  - backend/mlx_manager/mlx_server/services/batching/prefix_cache.py
  - backend/mlx_manager/mlx_server/services/batching/__init__.py
  - backend/tests/mlx_server/batching/test_prefix_cache.py
autonomous: true

must_haves:
  truths:
    - "Requests with identical prompt prefixes share cached KV blocks"
    - "Prefix cache lookup returns existing block IDs for matching hash"
    - "Reference counting prevents eviction of in-use prefix blocks"
    - "Prefix cache is per-model (no cross-model sharing)"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/prefix_cache.py"
      provides: "Hash-based prefix cache with reference counting"
      exports: ["PrefixCache"]
  key_links:
    - from: "prefix_cache.py"
      to: "block_manager.py"
      via: "Marks blocks as prefix_cached"
      pattern: "mark_as_prefix_cached"
    - from: "prefix_cache.py"
      to: "block.py"
      via: "Uses BLOCK_SIZE for hash computation"
      pattern: "BLOCK_SIZE"
---

<objective>
Prefix caching for KV block sharing across requests

Purpose: Enable requests with common prompt prefixes (system prompts, few-shot examples) to share KV cache blocks, reducing redundant computation and memory usage. vLLM APC reports 5.8x speedup on shared prefixes.

Output: PrefixCache with hash-based prefix matching, reference counting, and LRU eviction under memory pressure
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-continuous-batching/09-CONTEXT.md
@.planning/phases/09-continuous-batching/09-RESEARCH.md

Key decisions from CONTEXT.md:
- Per-model prefix cache (no cross-model sharing)
- No TTL - LRU eviction only under memory pressure
- Reference counting prevents eviction of in-use blocks

Prior plan artifacts:
@backend/mlx_manager/mlx_server/services/batching/block.py (BLOCK_SIZE, KVBlock)
@backend/mlx_manager/mlx_server/services/batching/block_manager.py (PagedBlockManager)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement hash-based prefix cache</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/prefix_cache.py
  </files>
  <action>
Create `PrefixCache` class with hash-based prefix matching:

1. Hash computation function:
```python
def compute_block_hash(
    token_ids: list[int],
    block_start: int,
    block_size: int,
    prev_hash: int = 0
) -> int:
    """Compute content-based hash for a block of tokens.

    Chain hashes to include position context - same tokens at different
    positions produce different hashes.
    """
    block_tokens = tuple(token_ids[block_start:block_start + block_size])
    return hash((prev_hash, block_tokens))
```

2. `PrefixCache` class:
   - `__init__(self, model_id: str, block_manager: PagedBlockManager)`
   - `_hash_to_blocks: dict[int, list[int]]` - hash -> list of physical block IDs
   - `_block_to_hash: dict[int, int]` - reverse lookup for cleanup

3. Methods:
   - `lookup_prefix(prompt_tokens: list[int]) -> tuple[list[int], int]`:
     - Compute hashes for each complete block in prompt
     - Return (list of cached block IDs, number of tokens matched)
     - Increment ref_count on matched blocks

   - `cache_prefix(prompt_tokens: list[int], block_ids: list[int]) -> None`:
     - Compute hashes for each block
     - Store in _hash_to_blocks
     - Mark blocks as prefix_cached in block_manager

   - `release_prefix(block_ids: list[int]) -> None`:
     - Decrement ref_count on each block
     - Don't remove from cache (LRU eviction handles cleanup)

   - `clear() -> None`: Clear all cache entries, unmark blocks

4. Thread safety: Use asyncio.Lock for cache operations

5. Per-model scoping: model_id stored but primarily for logging/debugging.
   Each PrefixCache instance is per-model (scheduler creates one per model).
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching.prefix_cache import PrefixCache, compute_block_hash
from mlx_manager.mlx_server.services.batching import PagedBlockManager

# Test hash computation
h1 = compute_block_hash([1,2,3], 0, 3)
h2 = compute_block_hash([1,2,3], 0, 3)
assert h1 == h2, 'Same tokens should produce same hash'

# Test chained hashes differ
h3 = compute_block_hash([4,5,6], 0, 3, prev_hash=h1)
h4 = compute_block_hash([4,5,6], 0, 3, prev_hash=0)
assert h3 != h4, 'Different prev_hash should produce different hash'

print('OK')
"`
  </verify>
  <done>PrefixCache with hash-based lookup, caching, and reference counting</done>
</task>

<task type="auto">
  <name>Task 2: Add prefix cache integration with block manager</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/prefix_cache.py
    backend/mlx_manager/mlx_server/services/batching/__init__.py
  </files>
  <action>
Complete integration between prefix cache and block manager:

1. In `lookup_prefix`:
   - For each matched block, call `block_manager.touch(block_id)` to update last_used
   - This prevents LRU eviction of recently-used prefix blocks

2. In `cache_prefix`:
   - Call `block_manager.mark_as_prefix_cached(block_id, hash_value)` for each block
   - Increment ref_count via `block_manager.get_block(block_id).ref_count += 1`

3. Add helper method `get_cache_stats() -> dict`:
   - Return {"cached_blocks": n, "unique_prefixes": m}

4. Add `evict_unused() -> int` method:
   - Find prefix cache entries where all blocks have ref_count=0
   - Remove from cache, unmark in block_manager
   - Return count of blocks freed
   - Called by scheduler when memory pressure detected

5. Update `__init__.py` to export: PrefixCache, compute_block_hash
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching import PrefixCache, PagedBlockManager

pm = PagedBlockManager(num_blocks=100)
pc = PrefixCache(model_id='test-model', block_manager=pm)

# Allocate some blocks and cache them
blocks = [pm.allocate() for _ in range(3)]
tokens = list(range(96))  # 3 blocks worth (32 * 3)

pc.cache_prefix(tokens, blocks)
stats = pc.get_cache_stats()
assert stats['cached_blocks'] == 3
print('OK')
"`
  </verify>
  <done>PrefixCache integrated with PagedBlockManager for block marking and eviction</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for prefix cache</name>
  <files>
    backend/tests/mlx_server/batching/test_prefix_cache.py
  </files>
  <action>
Create comprehensive tests for prefix caching:

1. Test hash computation:
   - Same tokens produce same hash
   - Different tokens produce different hash
   - Chained hashes (with prev_hash) differ from unchained
   - Position matters (same tokens at block 0 vs block 1 differ)

2. Test prefix lookup:
   - Empty cache returns empty list and 0 matched tokens
   - After caching, lookup returns correct blocks
   - Partial match (first 2 of 3 blocks match) returns partial result
   - No match returns empty

3. Test caching:
   - cache_prefix stores blocks correctly
   - Blocks are marked as prefix_cached in block_manager
   - Same prefix cached twice doesn't duplicate

4. Test reference counting:
   - lookup_prefix increments ref_count
   - release_prefix decrements ref_count
   - Multiple lookups accumulate ref_count

5. Test eviction:
   - evict_unused removes ref_count=0 entries
   - In-use entries (ref_count > 0) preserved
   - Freed blocks return to free pool

6. Test per-model isolation:
   - Two PrefixCache instances (different model_id) don't share state
  </action>
  <verify>`cd backend && python -m pytest tests/mlx_server/batching/test_prefix_cache.py -v` all tests pass</verify>
  <done>Unit tests covering hash computation, lookup, caching, and eviction</done>
</task>

</tasks>

<verification>
1. `cd backend && python -m pytest tests/mlx_server/batching/test_prefix_cache.py -v` - all tests pass
2. `cd backend && ruff check mlx_manager/mlx_server/services/batching/` - no lint errors
3. `cd backend && mypy mlx_manager/mlx_server/services/batching/` - no type errors
</verification>

<success_criteria>
- Hash-based prefix matching identifies common prefixes
- Cached blocks shared across requests via ref_count
- LRU eviction respects ref_count (only evicts unused)
- Per-model cache isolation maintained
- All quality gates pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-03-SUMMARY.md`
</output>
