---
phase: 09-continuous-batching
plan: 04
type: execute
wave: 2
depends_on: ["09-01", "09-02"]
files_modified:
  - backend/mlx_manager/mlx_server/services/batching/scheduler.py
  - backend/mlx_manager/mlx_server/services/batching/__init__.py
  - backend/tests/mlx_server/batching/test_scheduler.py
autonomous: true

must_haves:
  truths:
    - "Scheduler processes multiple requests per generation step"
    - "New requests wait for current step to complete before joining batch"
    - "Completed requests immediately free their slot for waiting requests"
    - "Scheduler respects max_batch_size limit"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/scheduler.py"
      provides: "ContinuousBatchingScheduler with iteration-level scheduling"
      exports: ["ContinuousBatchingScheduler"]
  key_links:
    - from: "scheduler.py"
      to: "priority_queue.py"
      via: "PriorityQueueWithAging for request ordering"
      pattern: "PriorityQueueWithAging"
    - from: "scheduler.py"
      to: "block_manager.py"
      via: "Block allocation for prompts"
      pattern: "PagedBlockManager"
    - from: "scheduler.py"
      to: "request.py"
      via: "BatchRequest state management"
      pattern: "BatchRequest"
---

<objective>
Continuous batching scheduler with iteration-level loop

Purpose: Implement the core scheduler that processes multiple requests per token generation step, achieving 2-4x throughput improvement by maximizing GPU utilization.

Output: ContinuousBatchingScheduler with submit(), scheduling_loop(), and batch generation
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-continuous-batching/09-CONTEXT.md
@.planning/phases/09-continuous-batching/09-RESEARCH.md

Key decisions from CONTEXT.md:
- New requests wait for current step to complete (no mid-generation interrupt)
- Immediate request removal when complete (true continuous batching)
- Memory-based batch size limit, admin can override
- Adaptive timing: wait longer when idle, process immediately under load

Prior plan artifacts:
@backend/mlx_manager/mlx_server/services/batching/types.py (RequestStatus, Priority)
@backend/mlx_manager/mlx_server/services/batching/request.py (BatchRequest)
@backend/mlx_manager/mlx_server/services/batching/priority_queue.py (PriorityQueueWithAging)
@backend/mlx_manager/mlx_server/services/batching/block_manager.py (PagedBlockManager)

Existing patterns:
@backend/mlx_manager/mlx_server/services/inference.py (Queue-based threading for MLX Metal)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement scheduler core structure</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/scheduler.py
  </files>
  <action>
Create `ContinuousBatchingScheduler` class skeleton:

1. `__init__(self, model_id: str, block_manager: PagedBlockManager, max_batch_size: int = 8, idle_wait_ms: float = 50.0, load_wait_ms: float = 5.0)`:
   - Store model_id for logging
   - `self.block_manager = block_manager`
   - `self.max_batch_size = max_batch_size`
   - `self.idle_wait_ms / load_wait_ms` - adaptive timing
   - `self.running: list[BatchRequest] = []` - currently generating
   - `self.waiting = PriorityQueueWithAging()` - pending requests
   - `self._shutdown = False`
   - `self._generation_thread: threading.Thread | None = None`
   - `self._step_lock = asyncio.Lock()` - prevents mid-step modifications

2. Request lifecycle methods:
   - `async submit(request: BatchRequest) -> AsyncGenerator[str, None]`:
     - Add request to waiting queue
     - Yield tokens from request.output_queue
     - Return when output_queue receives None (completion signal)

   - `_allocate_prompt_blocks(request: BatchRequest) -> BlockTable`:
     - Calculate blocks needed for prompt_tokens
     - Allocate via block_manager
     - Return populated BlockTable

   - `_release_blocks(request: BatchRequest) -> None`:
     - Release all blocks in request's block_table
     - Clear block_table

3. State query methods:
   - `get_stats() -> dict`: Return {"running": n, "waiting": m, "max_batch": k}
   - `is_running() -> bool`: Return not self._shutdown

Note: Actual batch_step implementation in Task 2. This establishes structure.
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching.scheduler import ContinuousBatchingScheduler
from mlx_manager.mlx_server.services.batching import PagedBlockManager

pm = PagedBlockManager(num_blocks=100)
scheduler = ContinuousBatchingScheduler(model_id='test', block_manager=pm)
stats = scheduler.get_stats()
assert stats['running'] == 0
assert stats['waiting'] == 0
print('OK')
"`
  </verify>
  <done>ContinuousBatchingScheduler structure with submit, block allocation, and state queries</done>
</task>

<task type="auto">
  <name>Task 2: Implement scheduling loop and batch generation</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/scheduler.py
  </files>
  <action>
Implement the iteration-level scheduling loop:

1. `async _scheduling_loop()` - main loop:
```python
async def _scheduling_loop(self):
    """Main iteration-level scheduling loop."""
    while not self._shutdown:
        # Adaptive wait: longer when idle to accumulate requests
        wait_ms = self.idle_wait_ms if not self.running else self.load_wait_ms

        async with self._step_lock:
            # Fill batch from waiting queue
            while len(self.running) < self.max_batch_size and not self.waiting.empty():
                request = await self.waiting.get()
                request.status = RequestStatus.PREFILLING
                request.started_at = time.time()
                request.block_table = self._allocate_prompt_blocks(request)
                self.running.append(request)
                request.status = RequestStatus.RUNNING

            if self.running:
                # Generate one token for all running requests
                await self._batch_step()

                # Remove completed requests (true continuous batching)
                completed = [r for r in self.running if r.is_complete()]
                for request in completed:
                    request.status = RequestStatus.COMPLETED
                    await request.output_queue.put(None)  # Signal completion
                    self._release_blocks(request)
                    self.running.remove(request)

        if not self.running and self.waiting.empty():
            await asyncio.sleep(wait_ms / 1000.0)
```

2. `async _batch_step()` - single token generation for batch:
   - For now, implement sequential generation (one request at a time)
   - Each request: call model's generate_step equivalent
   - Put generated token into request.output_queue
   - Update request.generated_tokens

   CRITICAL: Use existing Queue-based threading pattern from inference.py
   MLX Metal requires thread affinity - run generation in dedicated thread

3. `async start()` - start the scheduling loop:
   - Create asyncio.Task for _scheduling_loop
   - Store reference for shutdown

4. `async stop()` - graceful shutdown:
   - Set self._shutdown = True
   - Wait for running requests to complete (with timeout)
   - Cancel remaining waiting requests

5. Update `__init__.py` to export ContinuousBatchingScheduler
  </action>
  <verify>
`cd backend && python -c "
import asyncio
from mlx_manager.mlx_server.services.batching import (
    ContinuousBatchingScheduler, PagedBlockManager, BatchRequest, Priority
)

async def test():
    pm = PagedBlockManager(num_blocks=100)
    scheduler = ContinuousBatchingScheduler(model_id='test', block_manager=pm)

    # Just test structure - actual generation requires model
    r = BatchRequest(
        request_id='test-1',
        model_id='test',
        prompt_tokens=[1, 2, 3],
        max_tokens=10,
        priority=Priority.NORMAL
    )
    await scheduler.waiting.put(r)
    assert scheduler.waiting.qsize() == 1
    print('OK')

asyncio.run(test())
"`
  </verify>
  <done>Scheduling loop with batch filling, step execution, and completion handling</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for scheduler</name>
  <files>
    backend/tests/mlx_server/batching/test_scheduler.py
  </files>
  <action>
Create tests for scheduler behavior (mock actual generation):

1. Test scheduler initialization:
   - Stats show zero running/waiting
   - is_running() returns True before shutdown

2. Test request submission to queue:
   - submit() adds request to waiting queue
   - Multiple requests queue in priority order

3. Test batch filling:
   - Mock _batch_step to do nothing
   - Verify requests move from waiting to running
   - Verify max_batch_size respected

4. Test completion handling:
   - Mock request.is_complete() to return True
   - Verify request removed from running list
   - Verify blocks released
   - Verify output_queue receives None

5. Test shutdown:
   - stop() sets _shutdown flag
   - Waiting requests cancelled

6. Test adaptive timing:
   - Idle state uses idle_wait_ms
   - Load state uses load_wait_ms

Use pytest-asyncio for async tests.
Mock time.sleep/asyncio.sleep to avoid real delays.
Mock actual MLX generation (will be integration tested in Plan 05).
  </action>
  <verify>`cd backend && python -m pytest tests/mlx_server/batching/test_scheduler.py -v` all tests pass</verify>
  <done>Unit tests for scheduler lifecycle, batch management, and shutdown</done>
</task>

</tasks>

<verification>
1. `cd backend && python -m pytest tests/mlx_server/batching/test_scheduler.py -v` - all tests pass
2. `cd backend && ruff check mlx_manager/mlx_server/services/batching/` - no lint errors
3. `cd backend && mypy mlx_manager/mlx_server/services/batching/` - no type errors
</verification>

<success_criteria>
- Scheduler processes requests from priority queue
- Multiple requests can run concurrently up to max_batch_size
- Completed requests immediately free slots
- New requests join at step boundaries (not mid-generation)
- Graceful shutdown completes in-flight requests
- All quality gates pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-04-SUMMARY.md`
</output>
