---
phase: 09-continuous-batching
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/mlx_server/services/batching/__init__.py
  - backend/mlx_manager/mlx_server/services/batching/types.py
  - backend/mlx_manager/mlx_server/services/batching/request.py
  - backend/mlx_manager/mlx_server/services/batching/priority_queue.py
  - backend/tests/mlx_server/batching/test_types.py
  - backend/tests/mlx_server/batching/test_priority_queue.py
autonomous: true

must_haves:
  truths:
    - "Request status transitions through WAITING -> PREFILLING -> RUNNING -> COMPLETED"
    - "Priority queue returns highest priority request first"
    - "Waiting requests age over time (priority increases)"
    - "After sufficient wait time, low priority request beats normal priority"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/types.py"
      provides: "RequestStatus and Priority enums"
      exports: ["RequestStatus", "Priority"]
    - path: "backend/mlx_manager/mlx_server/services/batching/request.py"
      provides: "BatchRequest dataclass with state management"
      exports: ["BatchRequest"]
    - path: "backend/mlx_manager/mlx_server/services/batching/priority_queue.py"
      provides: "Priority queue with aging mechanism"
      exports: ["PriorityQueueWithAging"]
  key_links:
    - from: "priority_queue.py"
      to: "request.py"
      via: "BatchRequest dataclass"
      pattern: "BatchRequest"
    - from: "priority_queue.py"
      to: "types.py"
      via: "Priority enum for base_priority"
      pattern: "Priority\\."
---

<objective>
Foundation types and priority queue for continuous batching

Purpose: Establish the type system and request management infrastructure that the scheduler will use to track requests and prioritize their execution.

Output: BatchRequest dataclass, Priority/RequestStatus enums, and PriorityQueueWithAging with aging mechanism to prevent starvation
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-continuous-batching/09-CONTEXT.md
@.planning/phases/09-continuous-batching/09-RESEARCH.md

Reference existing patterns:
@backend/mlx_manager/mlx_server/services/inference.py (Queue-based threading pattern)
@backend/mlx_manager/mlx_server/models/pool.py (asyncio.Lock pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batching types and request dataclass</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/__init__.py
    backend/mlx_manager/mlx_server/services/batching/types.py
    backend/mlx_manager/mlx_server/services/batching/request.py
  </files>
  <action>
Create the batching services module with foundational types:

1. Create `types.py` with:
   - `RequestStatus` enum: WAITING, PREFILLING, RUNNING, COMPLETED, CANCELLED
   - `Priority` enum: HIGH=0, NORMAL=1, LOW=2 (lower numeric = higher priority)

2. Create `request.py` with `BatchRequest` dataclass:
   - Fields: request_id (str), model_id (str), prompt_tokens (list[int]), max_tokens (int), priority (Priority)
   - State fields: status (RequestStatus, default WAITING), generated_tokens (list[int], default [])
   - Streaming: output_queue (asyncio.Queue for token delivery)
   - Timing: created_at (float, time.time()), started_at (float | None)
   - Block management: block_table (optional, for later plans)
   - Properties:
     - `base_priority` -> int: returns priority.value
     - `is_complete` -> bool: checks max_tokens reached or terminal status
     - `effective_tokens` -> int: len(prompt_tokens) + len(generated_tokens)

3. Create `__init__.py` exporting: RequestStatus, Priority, BatchRequest

Use dataclasses with `field(default_factory=...)` for mutable defaults.
  </action>
  <verify>
`python -c "from mlx_manager.mlx_server.services.batching import RequestStatus, Priority, BatchRequest; print('OK')"` succeeds
  </verify>
  <done>RequestStatus, Priority enums and BatchRequest dataclass importable with all required fields</done>
</task>

<task type="auto">
  <name>Task 2: Implement priority queue with aging</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/priority_queue.py
    backend/mlx_manager/mlx_server/services/batching/__init__.py
  </files>
  <action>
Create `PriorityQueueWithAging` class that prevents starvation:

1. Internal `QueueEntry` dataclass (order=True for heapq):
   - effective_priority: float (compare=True)
   - entry_count: int (compare=True, tie-breaker for FIFO)
   - request: BatchRequest (compare=False)
   - entry_time: float (compare=False)

2. `PriorityQueueWithAging` class:
   - `__init__(self, aging_rate: float = 0.1)` - rate of priority decrease per second
   - `_heap: list[QueueEntry]` - min-heap
   - `_entry_counter: int` - monotonic counter for FIFO tie-breaking
   - `_aging_rate: float`
   - `_lock: asyncio.Lock` - thread safety for async context

3. Methods:
   - `async put(request: BatchRequest)` - add to queue with initial priority
   - `async get() -> BatchRequest` - pop highest priority (after aging)
   - `async peek() -> BatchRequest | None` - view top without removing
   - `_update_priorities()` - recalculate effective_priority for all entries based on wait time
   - `empty() -> bool`
   - `qsize() -> int`

4. Aging formula: `effective_priority = base_priority - (wait_time * aging_rate)`
   - With rate=0.1, LOW (2) becomes NORMAL (1) after 10s, HIGH (0) after 20s

5. After `_update_priorities()`, call `heapq.heapify(self._heap)` to restore heap property

6. Update `__init__.py` to export `PriorityQueueWithAging`
  </action>
  <verify>
`cd backend && python -c "
import asyncio
from mlx_manager.mlx_server.services.batching import PriorityQueueWithAging, BatchRequest, Priority
async def test():
    q = PriorityQueueWithAging()
    r = BatchRequest(request_id='test', model_id='m', prompt_tokens=[1], max_tokens=10, priority=Priority.NORMAL)
    await q.put(r)
    got = await q.get()
    assert got.request_id == 'test'
    print('OK')
asyncio.run(test())
"` passes
  </verify>
  <done>PriorityQueueWithAging with async put/get, aging mechanism, and heap-based priority ordering</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for types and priority queue</name>
  <files>
    backend/tests/mlx_server/batching/__init__.py
    backend/tests/mlx_server/batching/test_types.py
    backend/tests/mlx_server/batching/test_priority_queue.py
  </files>
  <action>
Create comprehensive tests:

1. Create `tests/mlx_server/batching/__init__.py` (empty)

2. Create `test_types.py`:
   - Test RequestStatus enum has all 5 states
   - Test Priority enum ordering (HIGH < NORMAL < LOW numerically)
   - Test BatchRequest creation with defaults
   - Test BatchRequest.base_priority returns correct value
   - Test BatchRequest.is_complete when max_tokens reached
   - Test BatchRequest.is_complete when status is COMPLETED/CANCELLED

3. Create `test_priority_queue.py`:
   - Test put/get single request
   - Test priority ordering (HIGH returned before LOW)
   - Test FIFO within same priority
   - Test aging: after wait, LOW can beat NORMAL
     - Create LOW request, wait 15 seconds (mock time.time), add NORMAL
     - LOW should be returned first (effective_priority < NORMAL)
   - Test empty() and qsize()
   - Test peek() doesn't remove item

Use `pytest.mark.asyncio` for async tests.
Mock `time.time()` for aging tests to avoid real delays.
  </action>
  <verify>`cd backend && python -m pytest tests/mlx_server/batching/ -v` all tests pass</verify>
  <done>Unit tests covering types, priority queue ordering, and aging mechanism</done>
</task>

</tasks>

<verification>
1. `cd backend && python -m pytest tests/mlx_server/batching/ -v` - all tests pass
2. `cd backend && ruff check mlx_manager/mlx_server/services/batching/` - no lint errors
3. `cd backend && mypy mlx_manager/mlx_server/services/batching/` - no type errors
</verification>

<success_criteria>
- RequestStatus and Priority enums defined with correct values
- BatchRequest tracks request state through lifecycle
- PriorityQueueWithAging returns higher priority requests first
- Aging mechanism prevents starvation (verified by test)
- All quality gates pass (ruff, mypy, pytest)
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-01-SUMMARY.md`
</output>
