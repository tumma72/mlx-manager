---
phase: 09-continuous-batching
plan: 08
type: execute
wave: 5
depends_on: ["09-07"]
files_modified:
  - backend/mlx_manager/mlx_server/services/batching/scheduler_manager.py
  - backend/tests/mlx_server/batching/test_scheduler_manager.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "configure_scheduler() wires BatchInferenceEngine to scheduler via set_model()"
    - "Scheduler uses real inference engine when model is configured"
    - "Tests verify inference engine is actually configured"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/scheduler_manager.py"
      provides: "configure_scheduler implementation"
      contains: "scheduler.set_model"
    - path: "backend/tests/mlx_server/batching/test_scheduler_manager.py"
      provides: "Tests verifying engine configuration"
      contains: "_inference_engine"
  key_links:
    - from: "scheduler_manager.py:configure_scheduler"
      to: "scheduler.set_model"
      via: "method call with model, tokenizer, adapter"
      pattern: "scheduler\\.set_model\\(model.*tokenizer.*adapter\\)"
---

<objective>
Wire BatchInferenceEngine in SchedulerManager.configure_scheduler()

Purpose: Close the critical gap where the scheduler infrastructure exists but never receives a configured inference engine. Without this fix, batching generates placeholder tokens instead of actual MLX inference.

Output: Working configure_scheduler() that calls scheduler.set_model(), plus tests verifying the wiring.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/09-continuous-batching/09-VERIFICATION.md

# Source files to modify
@backend/mlx_manager/mlx_server/services/batching/scheduler_manager.py
@backend/mlx_manager/mlx_server/services/batching/scheduler.py
@backend/tests/mlx_server/batching/test_scheduler_manager.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire set_model in configure_scheduler</name>
  <files>backend/mlx_manager/mlx_server/services/batching/scheduler_manager.py</files>
  <action>
Update configure_scheduler() method (lines 97-125) to actually call scheduler.set_model():

1. After `await self.get_scheduler(model_id)` retrieves the scheduler
2. Add call: `scheduler.set_model(model, tokenizer, adapter)`
3. Remove the TODO comment and "stub engine" log message
4. Update log message to indicate successful configuration

The change is minimal - just add one line and update the logging:

```python
async def configure_scheduler(
    self,
    model_id: str,
    model: Any,
    tokenizer: Any,
    adapter: Any,
) -> None:
    """Configure a scheduler with model resources.

    Called when a model is loaded into the pool, to set up the
    BatchInferenceEngine for actual generation.

    Args:
        model_id: The model identifier
        model: The loaded MLX model
        tokenizer: The model's tokenizer
        adapter: The model adapter for chat template handling
    """
    scheduler = await self.get_scheduler(model_id)

    # Wire up the inference engine
    scheduler.set_model(model, tokenizer, adapter)

    logger.info(f"Scheduler configured for model {model_id}")
```

Note: Keep TYPE_CHECKING import for ModelAdapter if needed for type hints.
  </action>
  <verify>
Run `cd /Users/atomasini/Development/mlx-manager/backend && python -c "from mlx_manager.mlx_server.services.batching.scheduler_manager import SchedulerManager; print('Import OK')"` to verify syntax.
  </verify>
  <done>
configure_scheduler() calls scheduler.set_model(model, tokenizer, adapter) instead of doing nothing. TODO comment removed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tests verifying inference engine configuration</name>
  <files>backend/tests/mlx_server/batching/test_scheduler_manager.py</files>
  <action>
Add new test methods to TestSchedulerManagerConfigure class to verify:

1. When configure_scheduler is called with valid model/tokenizer/adapter, the scheduler's _inference_engine is set (not None)
2. When configure_scheduler is called with None values, set_model is still called (graceful handling)

Add these tests after the existing test_configure_idempotent:

```python
@pytest.mark.asyncio
async def test_configure_wires_inference_engine(self):
    """configure_scheduler wires BatchInferenceEngine via set_model."""
    from unittest.mock import Mock, patch

    mgr = init_scheduler_manager()

    # Create mock model resources
    mock_model = Mock(spec=[])
    mock_tokenizer = Mock(spec=[])
    mock_adapter = Mock(spec=[])

    # Configure with mocks
    with patch(
        "mlx_manager.mlx_server.services.batching.scheduler.BatchInferenceEngine"
    ) as MockEngine:
        # set_model creates the engine internally
        await mgr.configure_scheduler("test-model", mock_model, mock_tokenizer, mock_adapter)

    # Verify scheduler's set_model was called (it creates _inference_engine internally)
    scheduler = await mgr.get_scheduler("test-model")
    # The scheduler should have called set_model which sets _inference_engine
    # Since we patched BatchInferenceEngine, verify it was instantiated
    MockEngine.assert_called_once()

    await mgr.shutdown()

@pytest.mark.asyncio
async def test_configure_with_none_does_not_raise(self):
    """configure_scheduler handles None resources gracefully."""
    mgr = init_scheduler_manager()

    # Configure with None (tests backward compatibility)
    await mgr.configure_scheduler("test-model", None, None, None)

    # Scheduler exists but inference engine should remain None (set_model handles None)
    scheduler = await mgr.get_scheduler("test-model")
    # set_model is called but with None args, so it may not create engine
    # This test verifies no exception is raised

    await mgr.shutdown()
```

Also update the import section to include Mock and patch if not already present.
  </action>
  <verify>
Run `cd /Users/atomasini/Development/mlx-manager/backend && pytest tests/mlx_server/batching/test_scheduler_manager.py -v -k "configure"` to verify tests pass.
  </verify>
  <done>
Tests verify that configure_scheduler actually calls scheduler.set_model and wires the inference engine. All configure tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Run full test suite and verify gap closed</name>
  <files>None (verification only)</files>
  <action>
Run the complete batching test suite to ensure no regressions:

1. Run: `cd /Users/atomasini/Development/mlx-manager/backend && pytest tests/mlx_server/batching/ -v`
2. Verify all tests pass
3. Run: `cd /Users/atomasini/Development/mlx-manager/backend && ruff check mlx_manager/mlx_server/services/batching/scheduler_manager.py`
4. Run: `cd /Users/atomasini/Development/mlx-manager/backend && mypy mlx_manager/mlx_server/services/batching/scheduler_manager.py`

If any test fails, fix the issue. The existing tests should continue to pass since we're adding behavior, not changing existing behavior.
  </action>
  <verify>
All batching tests pass (171+ tests). Linting and type checking pass.
  </verify>
  <done>
Full test suite passes. scheduler_manager.py passes ruff and mypy checks. Gap is closed: configure_scheduler now wires the inference engine.
  </done>
</task>

</tasks>

<verification>
1. grep for "scheduler.set_model" in scheduler_manager.py returns a match
2. grep for "TODO" in scheduler_manager.py returns no matches related to configure_scheduler
3. All batching tests pass: `pytest tests/mlx_server/batching/ -v`
4. New test specifically checks _inference_engine is wired
</verification>

<success_criteria>
- [ ] configure_scheduler() calls scheduler.set_model(model, tokenizer, adapter)
- [ ] TODO comment removed from configure_scheduler()
- [ ] New test verifies inference engine is configured
- [ ] All 171+ batching tests pass
- [ ] ruff and mypy checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-08-SUMMARY.md`
</output>
