---
phase: 09-continuous-batching
plan: 06
type: execute
wave: 3
depends_on: ["09-04"]
files_modified:
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/mlx_manager/mlx_server/services/batching/scheduler_manager.py
  - backend/mlx_manager/mlx_server/services/batching/__init__.py
  - backend/mlx_manager/mlx_server/main.py
  - backend/tests/mlx_server/batching/test_scheduler_manager.py
autonomous: true

must_haves:
  truths:
    - "Chat requests routed through batching scheduler"
    - "Priority determined by API key tier"
    - "Scheduler creates per-model instances on demand"
    - "Graceful fallback to direct inference if scheduler unavailable"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/scheduler_manager.py"
      provides: "Global scheduler registry and routing"
      exports: ["SchedulerManager", "get_scheduler_manager"]
  key_links:
    - from: "chat.py"
      to: "scheduler_manager.py"
      via: "Route requests through scheduler"
      pattern: "get_scheduler_manager"
    - from: "scheduler_manager.py"
      to: "scheduler.py"
      via: "Per-model scheduler instances"
      pattern: "ContinuousBatchingScheduler"
    - from: "main.py"
      to: "scheduler_manager.py"
      via: "Lifespan initialization"
      pattern: "SchedulerManager"
---

<objective>
API integration and scheduler management

Purpose: Connect the batching scheduler to the chat completions endpoint, enabling requests to flow through the continuous batching system for improved throughput.

Output: SchedulerManager for per-model scheduler instances, chat.py integration, and priority routing from API keys
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-continuous-batching/09-CONTEXT.md
@.planning/phases/09-continuous-batching/09-RESEARCH.md

Key decisions from CONTEXT.md:
- Priority determined by API key tier (admin assigns)
- Endpoint-based override possible (e.g., /v1/batch/* forces low priority)
- System-determined priority, not client-requested

Prior plan artifacts:
@backend/mlx_manager/mlx_server/services/batching/scheduler.py (ContinuousBatchingScheduler)
@backend/mlx_manager/mlx_server/api/v1/chat.py (current chat endpoint)
@backend/mlx_manager/mlx_server/main.py (lifespan)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SchedulerManager singleton</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/scheduler_manager.py
    backend/mlx_manager/mlx_server/services/batching/__init__.py
  </files>
  <action>
Create `SchedulerManager` to manage per-model schedulers:

1. `SchedulerManager` class:
   - `__init__(self, block_pool_size: int = 1000, max_batch_size: int = 8)`:
     - `self._schedulers: dict[str, ContinuousBatchingScheduler]` - model_id -> scheduler
     - `self._block_managers: dict[str, PagedBlockManager]` - per-model block managers
     - `self._lock = asyncio.Lock()`
     - Store config defaults

   - `async get_scheduler(model_id: str) -> ContinuousBatchingScheduler`:
     - Return existing scheduler or create new one
     - Lazy initialization per model

   - `async configure_scheduler(model_id: str, model, tokenizer, adapter)`:
     - Called when model loaded into pool
     - Set up BatchInferenceEngine for scheduler

   - `get_priority_for_request(api_key: str | None, endpoint: str) -> Priority`:
     - Default: Priority.NORMAL
     - Endpoint override: `/v1/batch/*` -> Priority.LOW
     - API key tier lookup (placeholder for now - just return NORMAL)
     - Future: database lookup for API key -> tier mapping

   - `async shutdown()`:
     - Stop all schedulers gracefully

2. Singleton pattern:
```python
_scheduler_manager: SchedulerManager | None = None

def get_scheduler_manager() -> SchedulerManager:
    if _scheduler_manager is None:
        raise RuntimeError("SchedulerManager not initialized")
    return _scheduler_manager

def init_scheduler_manager(**kwargs) -> SchedulerManager:
    global _scheduler_manager
    _scheduler_manager = SchedulerManager(**kwargs)
    return _scheduler_manager
```

3. Update `__init__.py` to export: SchedulerManager, get_scheduler_manager, init_scheduler_manager
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching import init_scheduler_manager, get_scheduler_manager, Priority

mgr = init_scheduler_manager()
p = mgr.get_priority_for_request(None, '/v1/chat/completions')
assert p == Priority.NORMAL

p_low = mgr.get_priority_for_request(None, '/v1/batch/completions')
assert p_low == Priority.LOW
print('OK')
"`
  </verify>
  <done>SchedulerManager with per-model schedulers and priority determination</done>
</task>

<task type="auto">
  <name>Task 2: Integrate scheduler into chat endpoint</name>
  <files>
    backend/mlx_manager/mlx_server/api/v1/chat.py
  </files>
  <action>
Add batching path to chat completions:

1. Add feature flag check at top of `create_chat_completion`:
```python
from mlx_manager.mlx_server.config import get_settings
from mlx_manager.mlx_server.services.batching import (
    get_scheduler_manager, BatchRequest, Priority
)

settings = get_settings()
use_batching = settings.enable_batching  # New config flag
```

2. Create `_handle_batched_request()` function:
```python
async def _handle_batched_request(
    request: ChatCompletionRequest,
    priority: Priority,
) -> EventSourceResponse | ChatCompletionResponse:
    """Route request through batching scheduler."""
    from mlx_manager.mlx_server.models.pool import get_model_pool
    from mlx_manager.mlx_server.models.adapters import get_adapter

    pool = get_model_pool()
    loaded = await pool.get_model(request.model)
    adapter = get_adapter(request.model)

    # Get actual tokenizer
    actual_tokenizer = getattr(loaded.tokenizer, 'tokenizer', loaded.tokenizer)

    # Tokenize prompt
    messages = [{"role": m.role, "content": m.content} for m in request.messages]
    prompt = adapter.apply_chat_template(loaded.tokenizer, messages, add_generation_prompt=True)
    prompt_tokens = actual_tokenizer.encode(prompt)

    # Create batch request
    batch_request = BatchRequest(
        request_id=f"chatcmpl-{uuid.uuid4().hex[:12]}",
        model_id=request.model,
        prompt_tokens=prompt_tokens,
        max_tokens=request.max_tokens or 4096,
        priority=priority,
    )

    # Get scheduler and configure if needed
    mgr = get_scheduler_manager()
    scheduler = await mgr.get_scheduler(request.model)
    await mgr.configure_scheduler(request.model, loaded.model, loaded.tokenizer, adapter)

    # Submit and stream
    if request.stream:
        return await _stream_batched_response(batch_request, scheduler, request.model)
    else:
        return await _complete_batched_response(batch_request, scheduler, request.model)
```

3. Implement `_stream_batched_response()` with full OpenAI formatting:
```python
async def _stream_batched_response(
    batch_request: BatchRequest,
    scheduler: ContinuousBatchingScheduler,
    model: str,
) -> EventSourceResponse:
    """Stream tokens from scheduler as SSE events in OpenAI format."""
    import time

    async def generate_stream():
        created = int(time.time())

        # Submit to scheduler - returns async generator of tokens
        token_stream = scheduler.submit(batch_request)

        async for token_text in token_stream:
            # Format as OpenAI ChatCompletionChunk
            chunk = {
                "id": batch_request.request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": token_text},
                    "finish_reason": None,
                }],
            }
            yield {"data": json.dumps(chunk)}

        # Send final chunk with finish_reason
        final_chunk = {
            "id": batch_request.request_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop",
            }],
        }
        yield {"data": json.dumps(final_chunk)}
        yield {"data": "[DONE]"}

    return EventSourceResponse(generate_stream())
```

4. Implement `_complete_batched_response()` with full OpenAI formatting:
```python
async def _complete_batched_response(
    batch_request: BatchRequest,
    scheduler: ContinuousBatchingScheduler,
    model: str,
) -> ChatCompletionResponse:
    """Collect all tokens and return complete ChatCompletionResponse."""
    import time

    created = int(time.time())
    collected_tokens: list[str] = []

    # Submit to scheduler and collect all tokens
    token_stream = scheduler.submit(batch_request)
    async for token_text in token_stream:
        collected_tokens.append(token_text)

    # Build response text
    response_text = "".join(collected_tokens)

    # Build usage statistics
    # prompt_tokens from batch_request, completion_tokens from collected
    prompt_tokens = len(batch_request.prompt_tokens)
    completion_tokens = len(collected_tokens)

    return ChatCompletionResponse(
        id=batch_request.request_id,
        object="chat.completion",
        created=created,
        model=model,
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatMessage(role="assistant", content=response_text),
                finish_reason="stop",
            )
        ],
        usage=UsageInfo(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens,
        ),
    )
```

5. In `create_chat_completion`, add routing:
```python
if use_batching and not has_images:  # Batching for text-only for now
    try:
        mgr = get_scheduler_manager()
        priority = mgr.get_priority_for_request(
            api_key=request.headers.get("Authorization"),  # Extract from request
            endpoint="/v1/chat/completions"
        )
        return await _handle_batched_request(request, priority)
    except Exception as e:
        logger.warning(f"Batching unavailable, falling back to direct: {e}")
        # Fall through to direct inference
```
  </action>
  <verify>
`cd backend && python -c "
# Verify import works without runtime errors
from mlx_manager.mlx_server.api.v1.chat import router
import inspect

# Get source to verify helper functions exist
source_file = inspect.getfile(router)
with open(source_file) as f:
    source = f.read()

# Verify the helper functions are implemented
assert '_handle_batched_request' in source, 'Missing _handle_batched_request'
assert '_stream_batched_response' in source, 'Missing _stream_batched_response'
assert '_complete_batched_response' in source, 'Missing _complete_batched_response'

# Verify OpenAI response formatting is present
assert 'chat.completion.chunk' in source, 'Missing streaming chunk format'
assert 'ChatCompletionResponse' in source, 'Missing response type'

print('chat.py with batched handlers OK')
"`
  </verify>
  <done>Chat endpoint routes through scheduler with proper OpenAI response formatting</done>
</task>

<task type="auto">
  <name>Task 3: Initialize scheduler in main.py lifespan</name>
  <files>
    backend/mlx_manager/mlx_server/main.py
    backend/mlx_manager/mlx_server/config.py
    backend/tests/mlx_server/batching/test_scheduler_manager.py
  </files>
  <action>
1. Add config flag in `config.py`:
```python
class Settings(BaseSettings):
    # ... existing ...
    enable_batching: bool = False  # Disabled by default until stable
    batch_block_pool_size: int = 1000
    batch_max_batch_size: int = 8
```

2. Update `main.py` lifespan:
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ... existing pool init ...

    # Initialize scheduler manager if batching enabled
    if settings.enable_batching:
        from mlx_manager.mlx_server.services.batching import init_scheduler_manager
        scheduler_mgr = init_scheduler_manager(
            block_pool_size=settings.batch_block_pool_size,
            max_batch_size=settings.batch_max_batch_size,
        )
        logger.info("Batching scheduler initialized")

    yield

    # Cleanup
    if settings.enable_batching:
        await scheduler_mgr.shutdown()
```

3. Create `test_scheduler_manager.py`:
   - Test init_scheduler_manager / get_scheduler_manager
   - Test get_scheduler creates new scheduler
   - Test get_priority_for_request with different endpoints
   - Test shutdown stops all schedulers
  </action>
  <verify>
`cd backend && python -m pytest tests/mlx_server/batching/test_scheduler_manager.py -v` all tests pass
  </verify>
  <done>Scheduler initialized in lifespan, config flags added, manager tested</done>
</task>

</tasks>

<verification>
1. `cd backend && python -m pytest tests/mlx_server/batching/ -v` - all tests pass
2. `cd backend && ruff check mlx_manager/mlx_server/` - no lint errors
3. `cd backend && mypy mlx_manager/mlx_server/` - no type errors
4. Config flag `enable_batching` controls feature activation
</verification>

<success_criteria>
- SchedulerManager creates per-model schedulers on demand
- Chat requests route through scheduler when batching enabled
- Streaming response formats tokens as OpenAI ChatCompletionChunk SSE
- Non-streaming response builds complete ChatCompletionResponse with usage stats
- Priority determined by endpoint (with API key support stubbed)
- Graceful fallback to direct inference if scheduler fails
- Feature disabled by default for safety
- All quality gates pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-06-SUMMARY.md`
</output>
