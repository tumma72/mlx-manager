---
phase: 09-continuous-batching
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/mlx_server/services/batching/block.py
  - backend/mlx_manager/mlx_server/services/batching/block_manager.py
  - backend/mlx_manager/mlx_server/services/batching/__init__.py
  - backend/tests/mlx_server/batching/test_block_manager.py
autonomous: true

must_haves:
  truths:
    - "Block manager allocates fixed 32-token blocks"
    - "Block allocation fails when no free blocks available"
    - "Released blocks return to free pool"
    - "LRU eviction targets ref_count=0 blocks first"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/block.py"
      provides: "KVBlock and BlockTable dataclasses"
      exports: ["KVBlock", "BlockTable", "BLOCK_SIZE"]
    - path: "backend/mlx_manager/mlx_server/services/batching/block_manager.py"
      provides: "PagedBlockManager with allocation and eviction"
      exports: ["PagedBlockManager"]
  key_links:
    - from: "block_manager.py"
      to: "block.py"
      via: "KVBlock instances in blocks dict"
      pattern: "KVBlock\\("
    - from: "block_manager.py"
      to: "BLOCK_SIZE constant"
      via: "Block size calculation"
      pattern: "BLOCK_SIZE"
---

<objective>
Paged KV cache block manager for memory-efficient allocation

Purpose: Implement the foundation for paged KV cache management - fixed-size blocks that can be allocated non-contiguously, reducing memory waste from 60-80% to under 4%.

Output: KVBlock/BlockTable dataclasses and PagedBlockManager with allocation, release, and LRU eviction
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-continuous-batching/09-CONTEXT.md
@.planning/phases/09-continuous-batching/09-RESEARCH.md

Key decisions from CONTEXT.md:
- 32-token block size
- 85% memory pressure threshold triggers eviction
- Evict prefix cache blocks first (ref_count=0 only)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create KVBlock and BlockTable dataclasses</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/block.py
  </files>
  <action>
Create block-level data structures:

1. Define constant: `BLOCK_SIZE = 32` (tokens per block, from CONTEXT.md)

2. Create `KVBlock` dataclass:
   - block_id: int
   - ref_count: int = 0 (number of requests using this block)
   - last_used: float = field(default_factory=time.time)
   - is_prefix_cached: bool = False (whether block is in prefix cache)
   - prefix_hash: int | None = None (hash for prefix cache lookup)

3. Create `BlockTable` dataclass for per-request block mapping:
   - request_id: str
   - logical_to_physical: dict[int, int] = field(default_factory=dict)
   - num_tokens: int = 0

   Methods:
   - `current_logical_index() -> int`: returns num_tokens // BLOCK_SIZE
   - `needs_new_block() -> bool`: returns num_tokens % BLOCK_SIZE == 0 (at block boundary)
   - `add_token(physical_block_id: int) -> None`: increment num_tokens, update mapping if needed
   - `get_physical_blocks() -> list[int]`: return list of physical block IDs in logical order

Note: BlockTable is a logical-to-physical mapping. When a request generates tokens, it may need new physical blocks. The block manager handles actual allocation.
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching.block import KVBlock, BlockTable, BLOCK_SIZE
assert BLOCK_SIZE == 32
b = KVBlock(block_id=0)
assert b.ref_count == 0
t = BlockTable(request_id='test')
assert t.current_logical_index() == 0
print('OK')
"`
  </verify>
  <done>KVBlock and BlockTable dataclasses with BLOCK_SIZE constant</done>
</task>

<task type="auto">
  <name>Task 2: Implement PagedBlockManager</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/block_manager.py
    backend/mlx_manager/mlx_server/services/batching/__init__.py
  </files>
  <action>
Create `PagedBlockManager` class for block pool management:

1. `__init__(self, num_blocks: int)`:
   - `self.blocks: dict[int, KVBlock]` - all blocks indexed by ID
   - `self.free_blocks: list[int]` - available block IDs (stack for O(1) pop)
   - `self._lock: asyncio.Lock` - for thread safety
   - Initialize all blocks as free

2. Core methods:
   - `allocate() -> int`: Pop from free_blocks, increment ref_count, update last_used, return block_id
     - Raises `MemoryError("No free KV cache blocks")` if none available
   - `release(block_id: int) -> None`: Decrement ref_count. If ref_count <= 0 and not prefix_cached, add to free_blocks
   - `get_block(block_id: int) -> KVBlock`: Return block by ID
   - `touch(block_id: int) -> None`: Update last_used timestamp

3. Eviction methods:
   - `evict_lru_blocks(count: int = 1) -> int`: Evict up to `count` blocks with ref_count=0
     - Sort by last_used (oldest first)
     - If is_prefix_cached, clear that flag too
     - Return number evicted
   - `get_free_count() -> int`: Return len(free_blocks)
   - `get_stats() -> dict`: Return {"total": n, "free": m, "used": n-m}

4. Prefix cache support (used by Plan 03):
   - `mark_as_prefix_cached(block_id: int, prefix_hash: int) -> None`: Set is_prefix_cached=True, store hash
   - `unmark_prefix_cached(block_id: int) -> None`: Clear prefix cache flags

5. Update `__init__.py` to export: KVBlock, BlockTable, BLOCK_SIZE, PagedBlockManager
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching import PagedBlockManager
pm = PagedBlockManager(num_blocks=10)
assert pm.get_free_count() == 10
bid = pm.allocate()
assert pm.get_free_count() == 9
pm.release(bid)
assert pm.get_free_count() == 10
print('OK')
"`
  </verify>
  <done>PagedBlockManager with allocate, release, eviction, and prefix cache support</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for block manager</name>
  <files>
    backend/tests/mlx_server/batching/test_block_manager.py
  </files>
  <action>
Create comprehensive tests for block management:

1. Test KVBlock defaults
2. Test BlockTable logical index calculation
3. Test BlockTable needs_new_block at boundaries (0, 32, 64 tokens)

4. Test PagedBlockManager:
   - Test allocation decrements free count
   - Test release increments free count
   - Test allocate fails when empty (raises MemoryError)
   - Test evict_lru_blocks returns count evicted
   - Test eviction only affects ref_count=0 blocks
   - Test eviction prefers older blocks (by last_used)
   - Test prefix cached blocks with ref_count=0 CAN be evicted
   - Test prefix cached blocks have flags cleared on eviction

5. Test edge cases:
   - Allocate all blocks, release one, allocate again
   - Double release (should not duplicate in free_blocks)

Use synchronous tests where possible (no async needed for most block ops).
  </action>
  <verify>`cd backend && python -m pytest tests/mlx_server/batching/test_block_manager.py -v` all tests pass</verify>
  <done>Unit tests covering block allocation, release, eviction, and edge cases</done>
</task>

</tasks>

<verification>
1. `cd backend && python -m pytest tests/mlx_server/batching/test_block_manager.py -v` - all tests pass
2. `cd backend && ruff check mlx_manager/mlx_server/services/batching/` - no lint errors
3. `cd backend && mypy mlx_manager/mlx_server/services/batching/` - no type errors
</verification>

<success_criteria>
- BLOCK_SIZE is 32 tokens (from CONTEXT.md decision)
- Blocks can be allocated and released with ref_count tracking
- LRU eviction targets oldest blocks with ref_count=0
- Prefix cache flags supported for Plan 03 integration
- All quality gates pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-02-SUMMARY.md`
</output>
