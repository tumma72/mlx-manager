---
phase: 09-continuous-batching
plan: 07
type: execute
wave: 4
depends_on: ["09-05", "09-06"]
files_modified:
  - backend/mlx_manager/mlx_server/services/batching/benchmark.py
  - backend/tests/mlx_server/batching/test_benchmark.py
  - docs/BATCHING.md
autonomous: true

must_haves:
  truths:
    - "Benchmark measures throughput improvement from batching"
    - "Single-request baseline compared to batched requests"
    - "Results show tokens/second improvement"
    - "Documentation explains batching architecture"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/batching/benchmark.py"
      provides: "Throughput benchmarking utilities"
      exports: ["run_benchmark", "BenchmarkResult"]
    - path: "docs/BATCHING.md"
      provides: "Batching architecture documentation"
      min_lines: 100
  key_links:
    - from: "benchmark.py"
      to: "scheduler.py"
      via: "Measures scheduler throughput"
      pattern: "ContinuousBatchingScheduler"
---

<objective>
Benchmark and documentation for continuous batching

Purpose: Verify throughput improvement from batching implementation and document the architecture for future development and troubleshooting.

Output: Benchmark utility showing throughput comparison, and comprehensive documentation
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-continuous-batching/09-CONTEXT.md
@.planning/phases/09-continuous-batching/09-RESEARCH.md

Success criteria from ROADMAP.md:
- Benchmark shows measurable throughput improvement over single-request baseline
- vLLM-MLX achieved 3.4x improvement (328->1112 tok/s) - our target

Prior plan artifacts (all batching components):
@backend/mlx_manager/mlx_server/services/batching/ (full module)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark utility</name>
  <files>
    backend/mlx_manager/mlx_server/services/batching/benchmark.py
    backend/mlx_manager/mlx_server/services/batching/__init__.py
  </files>
  <action>
Create benchmarking utilities for throughput measurement:

1. `BenchmarkResult` dataclass:
```python
@dataclass
class BenchmarkResult:
    mode: str  # "single" or "batched"
    num_requests: int
    total_tokens: int
    total_time_seconds: float
    tokens_per_second: float
    avg_latency_ms: float
    p50_latency_ms: float
    p99_latency_ms: float
```

2. `async run_single_request_benchmark(model_id: str, prompts: list[str], max_tokens: int = 100) -> BenchmarkResult`:
   - Run each prompt sequentially through direct inference
   - Measure total time and per-request latency
   - Calculate tokens/second

3. `async run_batched_benchmark(model_id: str, prompts: list[str], max_tokens: int = 100) -> BenchmarkResult`:
   - Submit all prompts concurrently to scheduler
   - Measure total time for all to complete
   - Calculate tokens/second and latencies

4. `async run_comparison_benchmark(model_id: str, prompts: list[str], max_tokens: int = 100) -> dict`:
   - Run both single and batched benchmarks
   - Calculate improvement ratio
   - Return {"single": result1, "batched": result2, "speedup": ratio}

5. Helper prompts (short, medium, long variations):
```python
BENCHMARK_PROMPTS = [
    "What is 2+2?",
    "Explain quantum computing in simple terms.",
    "Write a haiku about programming.",
    # ... 10-20 prompts of varying lengths
]
```

6. CLI entry point (optional):
```python
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", required=True)
    parser.add_argument("--prompts", type=int, default=10)
    # Run and print results
```

7. Update `__init__.py` to export: run_benchmark, BenchmarkResult
  </action>
  <verify>
`cd backend && python -c "
from mlx_manager.mlx_server.services.batching.benchmark import BenchmarkResult
r = BenchmarkResult(
    mode='test', num_requests=10, total_tokens=100,
    total_time_seconds=1.0, tokens_per_second=100.0,
    avg_latency_ms=100.0, p50_latency_ms=90.0, p99_latency_ms=150.0
)
assert r.tokens_per_second == 100.0
print('OK')
"`
  </verify>
  <done>Benchmark utility for measuring throughput improvement</done>
</task>

<task type="auto">
  <name>Task 2: Create batching documentation</name>
  <files>
    docs/BATCHING.md
  </files>
  <action>
Create comprehensive documentation for batching system:

1. Overview section:
   - What is continuous batching
   - Why it improves throughput (GPU utilization)
   - Expected improvement (2-4x based on vLLM-MLX)

2. Architecture diagram (ASCII):
```
Request -> Priority Queue -> Scheduler -> Batch Inference -> Response
                   |              |
           Block Manager    Prefix Cache
```

3. Component descriptions:
   - PriorityQueueWithAging: Request ordering with starvation prevention
   - PagedBlockManager: Fixed-size block allocation (32 tokens)
   - PrefixCache: Hash-based prefix sharing
   - ContinuousBatchingScheduler: Iteration-level scheduling loop
   - BatchInferenceEngine: MLX generation with thread affinity

4. Configuration:
   - `enable_batching`: Feature flag
   - `batch_block_pool_size`: Number of KV cache blocks
   - `batch_max_batch_size`: Max concurrent requests per step

5. Priority system:
   - Levels: HIGH, NORMAL, LOW
   - Determination: API key tier + endpoint override
   - Aging: Prevents starvation (0.1/sec priority increase)

6. Limitations:
   - Text-only batching (vision deferred)
   - Sequential generation within batch (mlx-lm limitation)
   - Per-model batching (no cross-model batches)

7. Troubleshooting:
   - Memory pressure: Reduce block_pool_size
   - Low throughput: Check max_batch_size
   - Request starvation: Check priority aging

8. Future improvements:
   - Native mlx-lm batch KV cache (Issue #548)
   - Vision model batching
   - Dynamic batch sizing
  </action>
  <verify>`test -f docs/BATCHING.md && wc -l docs/BATCHING.md` shows > 100 lines</verify>
  <done>Comprehensive batching documentation</done>
</task>

<task type="auto">
  <name>Task 3: Add benchmark tests and summary</name>
  <files>
    backend/tests/mlx_server/batching/test_benchmark.py
  </files>
  <action>
Create tests for benchmark utilities:

1. Test BenchmarkResult creation and fields
2. Test tokens_per_second calculation
3. Test latency percentile calculation (mock data)
4. Test benchmark function signatures (mock actual inference)

The benchmark runs with real models are manual - unit tests verify the
benchmark infrastructure works correctly.

Also create a brief test that verifies all batching components integrate:
```python
def test_batching_module_complete():
    """Verify all batching exports available."""
    from mlx_manager.mlx_server.services.batching import (
        # Types
        RequestStatus, Priority, BatchRequest,
        # Block management
        KVBlock, BlockTable, BLOCK_SIZE, PagedBlockManager,
        # Priority queue
        PriorityQueueWithAging,
        # Prefix cache
        PrefixCache,
        # Scheduler
        ContinuousBatchingScheduler,
        # Manager
        SchedulerManager, get_scheduler_manager, init_scheduler_manager,
        # Benchmark
        BenchmarkResult,
    )
    assert BLOCK_SIZE == 32
    assert Priority.HIGH.value < Priority.LOW.value
```
  </action>
  <verify>`cd backend && python -m pytest tests/mlx_server/batching/test_benchmark.py -v` all tests pass</verify>
  <done>Benchmark tests and integration verification</done>
</task>

</tasks>

<verification>
1. `cd backend && python -m pytest tests/mlx_server/batching/ -v` - all batching tests pass
2. `cd backend && ruff check mlx_manager/mlx_server/services/batching/` - no lint errors
3. `cd backend && mypy mlx_manager/mlx_server/services/batching/` - no type errors
4. `docs/BATCHING.md` exists with >100 lines of documentation

Manual verification (after enabling batching):
- Start server with `MLX_SERVER_ENABLE_BATCHING=true`
- Run benchmark: `python -m mlx_manager.mlx_server.services.batching.benchmark --model mlx-community/Llama-3.2-3B-Instruct-4bit`
- Verify batched throughput > single throughput
</verification>

<success_criteria>
- BenchmarkResult captures throughput metrics
- Comparison benchmark shows speedup ratio
- Documentation explains architecture and configuration
- All components integrate correctly (verified by test)
- All quality gates pass
- Ready for manual throughput testing
</success_criteria>

<output>
After completion, create `.planning/phases/09-continuous-batching/09-07-SUMMARY.md`
</output>
