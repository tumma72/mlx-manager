---
phase: 06-bug-fixes-stability
plan: 12
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/routers/chat.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "GLM-4 thinking content is captured whether it arrives via reasoning_content or content field"
    - "Both <think> tag parsing and reasoning_content field extraction work"
    - "If model outputs thinking as plain text without tags, it still renders as response (acceptable fallback)"
  artifacts:
    - path: "backend/mlx_manager/routers/chat.py"
      provides: "Robust thinking extraction from both delta fields"
      contains: "reasoning_content"
  key_links:
    - from: "backend/mlx_manager/routers/chat.py"
      to: "mlx-openai-server"
      via: "Reads delta.reasoning_content and delta.content with think tags"
      pattern: "reasoning_content|<think>"
---

<objective>
Investigate and fix GLM-4 thinking not being parsed. The infrastructure is correctly wired (reasoning_parser=glm4_moe, backend reads reasoning_content, frontend shows ThinkingBubble). The issue is likely that the model's chat template doesn't activate thinking mode, causing output without <think> tags.

Purpose: Ensure thinking models show their reasoning in the collapsible ThinkingBubble
Output: Robust thinking detection that handles edge cases in GLM-4 output
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@backend/mlx_manager/routers/chat.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Harden thinking extraction and add diagnostic logging</name>
  <files>backend/mlx_manager/routers/chat.py</files>
  <action>
    The current code already handles both `reasoning_content` (from server-side parser) and `<think>` tags (from content field). The issue with GLM-4 is likely one of:

    A) The glm4_moe reasoning_parser extracts thinking into `reasoning_content` field but our code has a subtle bug in transition detection
    B) The model outputs thinking without tags and without reasoning_content (just plain text)

    Case B is unfixable without model/template changes. Case A is what we can fix.

    **Improvements to make:**

    1. Add `import logging` and create logger at top of file:
    ```python
    logger = logging.getLogger(__name__)
    ```

    2. Add debug logging for the first chunk received to help diagnose future issues:
    ```python
    # After parsing delta (line 89):
    first_chunk_logged = False  # Add before the async for loop

    # Inside the loop, after getting delta:
    if not first_chunk_logged:
        logger.debug(f"First chunk delta keys: {list(delta.keys())}, finish_reason: {data.get('choices', [{}])[0].get('finish_reason')}")
        first_chunk_logged = True
    ```

    3. Fix the thinking transition detection (line 108). Current code:
    ```python
    elif in_thinking and not reasoning and content:
    ```
    This misses the case where the LAST reasoning chunk is empty string "" and content starts. Fix to also check when reasoning_content was present in PRIOR chunks but is now absent:
    ```python
    elif in_thinking and content and not reasoning:
    ```
    (This is actually the same logic - but add a comment explaining it handles the transition from reasoning_content chunks to content chunks.)

    4. Handle edge case: some models emit both `reasoning_content` AND `content` in the same delta chunk. Current code processes reasoning first, then content, which could double-emit the transition. Add a guard:
    ```python
    # After handling reasoning (line 107):
    if reasoning:
        # ... existing reasoning handling ...
        continue  # Skip content processing for this chunk when reasoning is present

    # Then handle transition and content:
    if in_thinking and content:
        # Transition from thinking to response
        in_thinking = False
        duration = time.time() - thinking_start if thinking_start else 0
        done_data = {"type": "thinking_done", "duration": round(duration, 1)}
        yield f"data: {json.dumps(done_data)}\n\n"
    ```

    Wait - restructuring the if/elif to use a `continue` would change the existing behavior. Instead, keep the structure but add the edge case protection:

    Actually the current code structure is:
    ```python
    if reasoning:
        # handle reasoning
    elif in_thinking and not reasoning and content:
        # transition

    if content:
        # handle content (with think tag parsing or plain)
    ```

    This is correct - if reasoning is present, it skips the transition check. If reasoning is absent but we were in_thinking and now have content, it transitions. Then it processes content.

    The potential issue is: if reasoning_content and content are BOTH present in the same chunk, the `if reasoning:` block runs, then `if content:` also runs, which would emit the content as "thinking" (because in_thinking is still True). This is actually correct behavior for that case.

    **The actual fix needed:** The transition detection `elif in_thinking and not reasoning and content:` is correct. But there's a subtle bug: if the model stops sending reasoning_content but also doesn't send content for a few chunks (empty deltas), the transition never fires until content arrives. This is fine - the transition happens when content starts.

    **Real fix - handle finish_reason:** When `finish_reason` is "stop" and we're still `in_thinking`, emit thinking_done. This is already handled at lines 172-179 ("If still in thinking when stream ends"). Good.

    **What we should actually do:**

    Since the infrastructure is correct and the issue is likely model-side (chat template not activating thinking), the best we can do is:

    1. Add diagnostic logging (as described above)
    2. Add a heuristic: if the FIRST content chunk from the model looks like thinking (starts with common thinking indicators like "Let me", "I need to", "First,", "Okay, ") AND no reasoning_content was ever received, treat initial content as potential thinking. BUT this is too aggressive and could misclassify normal responses.

    **Conservative approach (recommended):**
    - Add debug logging to help diagnose
    - Add a comment documenting the GLM-4 limitation
    - Keep the existing robust parsing (it handles all cases where the server/model properly signals thinking)

    Implementation:
    1. Add `import logging` and logger
    2. Add `first_chunk_logged = False` before the async for loop
    3. Log the first chunk's delta keys at debug level
    4. Add a comment block explaining GLM-4 thinking requires the chat template to output `<think>` tags or the reasoning_parser to extract them into `reasoning_content`. If neither happens, thinking appears as regular response text.
    5. Ensure the `in_thinking` state is properly reset between messages (it already is - it's initialized inside generate())
  </action>
  <verify>
    Run `cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate && ruff check mlx_manager/routers/chat.py && mypy mlx_manager/routers/chat.py --ignore-missing-imports`
  </verify>
  <done>
    - Debug logging added for first chunk (helps diagnose thinking issues)
    - Code documented for GLM-4 thinking limitations
    - Existing reasoning_content and think-tag parsing confirmed correct
    - If GLM-4 model outputs thinking via either mechanism, it will be captured
    - If model doesn't output thinking tags (template issue), response displays as normal text (acceptable)
  </done>
</task>

</tasks>

<verification>
- `cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate && ruff check . && mypy mlx_manager --ignore-missing-imports`
- `cd /Users/atomasini/Development/mlx-manager/backend && pytest tests/ -v` passes
- grep confirms logging import and debug log in chat.py
</verification>

<success_criteria>
- Chat endpoint has diagnostic logging for thinking detection
- Both reasoning_content and think-tag parsing paths work correctly
- GLM-4 thinking captured when server's reasoning_parser extracts it
- Graceful fallback: thinking without tags shown as regular response (not broken)
- No regressions in thinking detection for Qwen3 or other models
</success_criteria>

<output>
After completion, create `.planning/phases/06-bug-fixes-stability/06-12-SUMMARY.md`
</output>
