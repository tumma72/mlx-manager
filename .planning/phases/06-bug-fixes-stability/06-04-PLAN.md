---
phase: 06-bug-fixes-stability
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/services/server_manager.py
autonomous: true

must_haves:
  truths:
    - "Server CPU gauge shows actual CPU usage (not always 0%)"
    - "Server memory gauge reflects real model memory usage including child processes"
    - "Server log files are cleaned up when process crashes or exits"
  artifacts:
    - path: "backend/mlx_manager/services/server_manager.py"
      provides: "Accurate metrics collection and log cleanup"
      contains: "children|cpu_percent.*interval"
  key_links:
    - from: "backend/mlx_manager/services/server_manager.py"
      to: "psutil.Process"
      via: "recursive child process metrics"
      pattern: "children.*recursive"
---

<objective>
Fix server CPU and memory gauge accuracy by collecting metrics from child processes and using proper CPU measurement intervals, and ensure log files are cleaned up on process exit.

Purpose: Server metrics currently show 0% CPU (because psutil.cpu_percent() needs an interval on first call) and low memory (because RSS only measures the parent process, not the model loaded in child processes). Log files accumulate without cleanup.
Output: Accurate CPU and memory metrics that reflect actual model resource usage; guaranteed log file cleanup.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/mlx_manager/services/server_manager.py
@backend/mlx_manager/types.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix CPU and memory metrics collection</name>
  <files>backend/mlx_manager/services/server_manager.py</files>
  <action>
    Fix `get_server_stats()` method to provide accurate metrics:

    1. **CPU fix**: `psutil.Process.cpu_percent()` returns 0.0 on the first call because it needs a time interval to measure against. Fix by passing `interval=0.1` (100ms) which makes it a blocking but accurate measurement. Alternatively, call `cpu_percent()` with `interval=None` and use a stored previous measurement. The simpler approach is `interval=0.1`:
    ```python
    cpu_percent=p.cpu_percent(interval=0.1)
    ```

    2. **Memory fix**: The mlx-openai-server spawns child processes that load the model into memory. `p.memory_info().rss` only measures the parent process. Fix by summing memory across all child processes:
    ```python
    try:
        children = p.children(recursive=True)
        total_rss = p.memory_info().rss
        for child in children:
            try:
                total_rss += child.memory_info().rss
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        memory_mb = round(total_rss / 1024 / 1024, 2)
        memory_percent = (total_rss / psutil.virtual_memory().total) * 100
    except psutil.NoSuchProcess:
        return None
    ```

    3. **CPU for children too**: Sum CPU across parent + children:
    ```python
    total_cpu = p.cpu_percent(interval=0.1)
    for child in children:
        try:
            total_cpu += child.cpu_percent(interval=0)  # Non-blocking for children (uses delta from last call)
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    ```

    Note: The `interval=0.1` on parent adds 100ms latency to the API call. This is acceptable for a status endpoint called every few seconds. If this causes issues, switch to a background metrics collector pattern, but try the simple approach first.

    Update the return statement:
    ```python
    return ServerStats(
        pid=proc.pid,
        memory_mb=memory_mb,
        memory_percent=memory_percent,
        cpu_percent=round(total_cpu, 1),
        status=p.status(),
        create_time=p.create_time(),
    )
    ```
  </action>
  <verify>
    Run `cd backend && ruff check . && ruff format --check . && mypy mlx_manager` — no errors.
    Run `pytest -v` — all tests pass.
    Manual test: Start a server with a model, check `/api/servers` — CPU should show non-zero values after model loads, memory should reflect model size (several GB for a 4-bit quantized model).
  </verify>
  <done>CPU gauge shows actual usage > 0% during inference. Memory gauge reflects total process tree memory including loaded model. Values match expectations (e.g., 4-bit 7B model should show ~4-5 GB memory).</done>
</task>

<task type="auto">
  <name>Task 2: Ensure log file cleanup on process exit/crash</name>
  <files>backend/mlx_manager/services/server_manager.py</files>
  <action>
    Ensure server log files are properly cleaned up when processes exit. Currently, log files are created in `start_server()` but only cleaned up if the process fails immediately. Add cleanup in these scenarios:

    1. **In `stop_server()`**: After successfully stopping a process, close the log file handle and optionally delete the log file (or keep for debugging but close the handle properly):
    ```python
    # After proc.terminate()/kill() and proc.wait():
    if profile_id in self._log_files:
        try:
            self._log_files[profile_id].close()
        except OSError:
            pass
        del self._log_files[profile_id]
    ```
    This already exists partially (lines 135-140). Verify it works in all paths.

    2. **In `get_process_status()`**: When a dead process is detected (exit_code is not None), ensure the log file handle is closed:
    ```python
    # After: del self.processes[profile_id]
    if hasattr(self, "_log_files") and profile_id in self._log_files:
        try:
            self._log_files[profile_id].close()
        except OSError:
            pass
        del self._log_files[profile_id]
    ```

    3. **In `get_all_running()`**: When a dead process is found during iteration (line 331), also clean up its log file handle.

    4. **In `cleanup()`**: Already calls `stop_server()` for all processes. Verify this properly closes all log file handles.

    5. **Add a `_cleanup_log_file()` helper** to avoid code duplication:
    ```python
    def _cleanup_log_file(self, profile_id: int) -> None:
        """Close and clean up log file handle for a profile."""
        if hasattr(self, "_log_files") and profile_id in self._log_files:
            try:
                self._log_files[profile_id].close()
            except OSError as e:
                logger.debug(f"Failed to close log file for profile_id={profile_id}: {e}")
            del self._log_files[profile_id]
    ```
    Use this helper in all cleanup paths.
  </action>
  <verify>
    Run `cd backend && ruff check . && ruff format --check . && mypy mlx_manager` — no errors.
    Run `pytest -v` — all tests pass.
    Manual test: Start a server, kill it externally (`kill <pid>`), verify the log file handle is cleaned up on next status check.
  </verify>
  <done>Log file handles are properly closed in all process exit paths (normal stop, crash, external kill). No file handle leaks remain. A _cleanup_log_file helper prevents code duplication.</done>
</task>

</tasks>

<verification>
- `cd backend && ruff check . && ruff format --check . && mypy mlx_manager && pytest -v` all pass
- Server metrics show realistic CPU and memory values for running models
- Log file handles are cleaned up in all exit paths
</verification>

<success_criteria>
- CPU gauge shows non-zero values during model inference
- Memory gauge shows model-accurate values (GB-scale for large models, not MB-scale)
- Log file handles are closed in stop, crash, and cleanup scenarios
- All quality checks and tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-bug-fixes-stability/06-04-SUMMARY.md`
</output>
