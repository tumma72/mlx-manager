---
phase: 06-bug-fixes-stability
plan: 11
type: execute
wave: 1
depends_on: []
files_modified:
  - frontend/src/lib/api/client.ts
  - frontend/src/lib/api/types.ts
  - backend/mlx_manager/routers/chat.py
  - frontend/src/routes/(protected)/chat/+page.svelte
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "User can toggle MCP tools on/off in chat UI"
    - "When tools enabled, tools array is sent with chat completions request"
    - "Model tool_calls are parsed from streaming response and displayed"
    - "Tool calls are auto-executed via /api/mcp/execute"
    - "Tool results are sent back to model as role:tool messages"
    - "Complete tool-use conversation loop works end-to-end"
  artifacts:
    - path: "frontend/src/lib/api/client.ts"
      provides: "MCP API client methods (listTools, executeToolCall)"
      contains: "mcp"
    - path: "frontend/src/lib/api/types.ts"
      provides: "ToolCall, ToolResult types"
      contains: "ToolCall"
    - path: "backend/mlx_manager/routers/chat.py"
      provides: "ChatRequest with tools field, tool_calls SSE events"
      contains: "tools"
    - path: "frontend/src/routes/(protected)/chat/+page.svelte"
      provides: "Tools toggle, tool call display, auto-execution loop"
      contains: "toolsEnabled"
  key_links:
    - from: "frontend/src/routes/(protected)/chat/+page.svelte"
      to: "frontend/src/lib/api/client.ts"
      via: "mcp.listTools() and mcp.executeTool()"
      pattern: "mcp\\.(listTools|executeTool)"
    - from: "frontend/src/routes/(protected)/chat/+page.svelte"
      to: "backend/mlx_manager/routers/chat.py"
      via: "tools array in chat request body"
      pattern: "tools.*tool_choice"
    - from: "backend/mlx_manager/routers/chat.py"
      to: "mlx-openai-server"
      via: "tools forwarded in JSON body"
      pattern: "tools.*request\\.tools"
---

<objective>
Integrate MCP mock tools with the chat interface so models can call tools (weather, calculator) during conversation. The backend MCP endpoints exist but the frontend has no client for them, and the chat proxy doesn't forward tools.

Purpose: Enable end-to-end tool-use testing with tool-capable models
Output: Working tool-use loop: user message -> model requests tool -> tool executes -> result sent back -> model responds
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@backend/mlx_manager/routers/chat.py
@backend/mlx_manager/routers/mcp.py
@frontend/src/lib/api/client.ts
@frontend/src/lib/api/types.ts
@frontend/src/routes/(protected)/chat/+page.svelte
</context>

<tasks>

<task type="auto">
  <name>Task 1: Backend chat proxy + Frontend API types for tools</name>
  <files>
    backend/mlx_manager/routers/chat.py
    frontend/src/lib/api/client.ts
    frontend/src/lib/api/types.ts
  </files>
  <action>
    **Backend (chat.py):**

    1. Add `tools` and `tool_choice` fields to ChatRequest:
    ```python
    class ChatRequest(BaseModel):
        profile_id: int
        messages: list[dict]
        tools: list[dict] | None = None       # OpenAI tool definitions
        tool_choice: str | None = None        # "auto", "none", or specific
    ```

    2. In the generate() function, modify the JSON body sent to mlx-openai-server (line 67-71) to include tools when present:
    ```python
    body: dict = {
        "model": profile.model_path,
        "messages": request.messages,
        "stream": True,
    }
    if request.tools:
        body["tools"] = request.tools
        body["tool_choice"] = request.tool_choice or "auto"
    ```

    3. In the streaming response parser (after line 90), handle tool_calls in delta:
    ```python
    # Handle tool calls from model
    tool_calls = delta.get("tool_calls")
    if tool_calls:
        for tc in tool_calls:
            tool_data = {
                "type": "tool_call",
                "tool_call": tc,  # {index, id, function: {name, arguments}}
            }
            yield f"data: {json.dumps(tool_data)}\n\n"

    # Check finish_reason for tool_calls
    finish_reason = data.get("choices", [{}])[0].get("finish_reason")
    if finish_reason == "tool_calls":
        yield f"data: {json.dumps({'type': 'tool_calls_done'})}\n\n"
    ```

    **Frontend types (types.ts):**

    Add after the Attachment interface:
    ```typescript
    export interface ToolCall {
      id: string;
      function: {
        name: string;
        arguments: string; // JSON string
      };
    }

    export interface ToolDefinition {
      type: "function";
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }
    ```

    **Frontend API client (client.ts):**

    Add an `mcp` section after the existing API sections:
    ```typescript
    export const mcp = {
      listTools: async (): Promise<ToolDefinition[]> => {
        const res = await fetch(`${API_BASE}/mcp/tools`, {
          headers: getAuthHeaders(),
        });
        return handleResponse(res);
      },

      executeTool: async (name: string, args: Record<string, unknown>): Promise<Record<string, unknown>> => {
        const res = await fetch(`${API_BASE}/mcp/execute`, {
          method: "POST",
          headers: { ...getAuthHeaders(), "Content-Type": "application/json" },
          body: JSON.stringify({ name, arguments: args }),
        });
        return handleResponse(res);
      },
    };
    ```

    Import ToolDefinition type in client.ts if needed for the return type.
  </action>
  <verify>
    Run `cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate && ruff check mlx_manager/routers/chat.py && mypy mlx_manager/routers/chat.py --ignore-missing-imports`
    Run `cd /Users/atomasini/Development/mlx-manager/frontend && npm run check`
  </verify>
  <done>
    - ChatRequest accepts tools and tool_choice fields
    - Backend forwards tools to mlx-openai-server
    - Backend emits "tool_call" and "tool_calls_done" SSE events
    - Frontend has mcp.listTools() and mcp.executeTool() API methods
    - ToolCall and ToolDefinition types defined
  </done>
</task>

<task type="auto">
  <name>Task 2: Chat UI tools toggle and tool-use conversation loop</name>
  <files>frontend/src/routes/(protected)/chat/+page.svelte</files>
  <action>
    Add tool-use integration to the chat page:

    **1. State and imports:**
    - Import `mcp` from `$api` (the client)
    - Import `Wrench` icon from lucide-svelte
    - Import `ToolCall, ToolDefinition` from `$lib/api/types`
    - Add state:
      ```typescript
      let toolsEnabled = $state(false);
      let availableTools = $state<ToolDefinition[]>([]);
      let toolsLoaded = $state(false);
      ```

    **2. Load tools on mount:**
    Add to the existing onMount or create one:
    ```typescript
    import { onMount } from 'svelte';
    onMount(async () => {
      try {
        availableTools = await mcp.listTools();
        toolsLoaded = true;
      } catch { /* tools not available */ }
    });
    ```

    **3. Tools toggle button:**
    Add a toggle button in the chat input bar (before the attach button, around line 699):
    ```svelte
    {#if toolsLoaded}
      <Button
        type="button"
        variant={toolsEnabled ? "default" : "ghost"}
        size="icon"
        onclick={() => toolsEnabled = !toolsEnabled}
        title={toolsEnabled ? "Tools enabled" : "Enable tools"}
      >
        <Wrench class="w-4 h-4" />
      </Button>
    {/if}
    ```

    **4. Modify sendWithRetry to include tools:**
    In the fetch call body for chat/completions (around line 238-244), add tools:
    ```typescript
    const body: Record<string, unknown> = {
      profile_id: selectedProfile.id,
      messages: apiMessages,
    };
    if (toolsEnabled && availableTools.length > 0) {
      body.tools = availableTools;
      body.tool_choice = 'auto';
    }
    ```

    **5. Handle tool_call SSE events in the stream reader:**
    In the existing SSE parsing section (where it handles data.type === 'response', 'thinking', etc.), add handling for tool calls:

    ```typescript
    // Accumulate tool calls
    let pendingToolCalls: Array<{id: string; name: string; arguments: string}> = [];

    // In the event loop:
    } else if (data.type === 'tool_call') {
      const tc = data.tool_call;
      // Tool calls come in chunks - accumulate by index
      const existing = pendingToolCalls.find(t => t.id === tc.id);
      if (existing) {
        // Append arguments (streamed in chunks)
        existing.arguments += tc.function?.arguments || '';
      } else if (tc.id) {
        pendingToolCalls.push({
          id: tc.id,
          name: tc.function?.name || '',
          arguments: tc.function?.arguments || '',
        });
      }
    } else if (data.type === 'tool_calls_done') {
      // Execute all pending tool calls
      for (const toolCall of pendingToolCalls) {
        // Show tool call in assistant message
        const toolCallText = `\n\n---\n**Tool call:** \`${toolCall.name}(${toolCall.arguments})\`\n`;
        assistantContent += toolCallText;
        messages = [...messages.slice(0, -1), { role: 'assistant', content: assistantContent }];

        // Execute the tool
        try {
          const parsedArgs = JSON.parse(toolCall.arguments);
          const result = await mcp.executeTool(toolCall.name, parsedArgs);
          const resultText = `**Result:** \`${JSON.stringify(result)}\`\n---\n\n`;
          assistantContent += resultText;
          messages = [...messages.slice(0, -1), { role: 'assistant', content: assistantContent }];

          // Add tool result to API messages for follow-up
          apiMessages.push({ role: 'assistant', content: '', tool_calls: [{ id: toolCall.id, type: 'function', function: { name: toolCall.name, arguments: toolCall.arguments } }] } as any);
          apiMessages.push({ role: 'tool' as any, tool_call_id: toolCall.id, content: JSON.stringify(result) } as any);
        } catch (e) {
          assistantContent += `**Error:** Tool execution failed\n---\n\n`;
          messages = [...messages.slice(0, -1), { role: 'assistant', content: assistantContent }];
        }
      }

      // Send follow-up request with tool results so model can continue
      // Make another streaming request with the updated messages
      pendingToolCalls = [];
      // Continue the conversation - model will generate final response
      // This is handled by making a new fetch to chat/completions with tool results
      const followUpBody: Record<string, unknown> = {
        profile_id: selectedProfile!.id,
        messages: apiMessages,
      };
      if (toolsEnabled && availableTools.length > 0) {
        followUpBody.tools = availableTools;
        followUpBody.tool_choice = 'auto';
      }

      // Make follow-up streaming request (reuse the SSE parsing logic)
      // For simplicity, append the model's follow-up response to the same assistant message
      const followUpRes = await fetch('/api/chat/completions', {
        method: 'POST',
        headers: { ...getAuthHeaders(), 'Content-Type': 'application/json' },
        body: JSON.stringify(followUpBody),
      });

      if (followUpRes.ok && followUpRes.body) {
        const followReader = followUpRes.body.getReader();
        const followDecoder = new TextDecoder();
        let followBuffer = '';

        while (true) {
          const { done: followDone, value: followValue } = await followReader.read();
          if (followDone) break;

          followBuffer += followDecoder.decode(followValue, { stream: true });
          const followLines = followBuffer.split('\n');
          followBuffer = followLines.pop() || '';

          for (const followLine of followLines) {
            if (!followLine.startsWith('data: ')) continue;
            const followDataStr = followLine.slice(6);
            if (followDataStr === '[DONE]') continue;
            try {
              const followData = JSON.parse(followDataStr);
              if (followData.type === 'response') {
                assistantContent += followData.content;
                messages = [...messages.slice(0, -1), { role: 'assistant', content: assistantContent }];
              }
            } catch { /* skip parse errors */ }
          }
        }
      }
    }
    ```

    **Important:** The `apiMessages` variable and `assistantContent` variable need to be accessible in the tool_calls_done handler. They are already in scope from the sendWithRetry function. Also import `getAuthHeaders` from the API client if not already imported (it may be private - check and export if needed, or use the auth store token directly).

    **6. Auth headers for follow-up request:**
    The simplest approach is to import the auth utility. Check if `getAuthHeaders` is exported from client.ts. If not, use the auth store directly:
    ```typescript
    import { authStore } from '$stores';
    // Then in the fetch: headers: { 'Authorization': `Bearer ${authStore.token}`, 'Content-Type': 'application/json' }
    ```
  </action>
  <verify>
    Run `cd /Users/atomasini/Development/mlx-manager/frontend && npm run check` to verify TypeScript compiles.
    Verify the tools toggle button renders and the SSE handling compiles.
  </verify>
  <done>
    - Wrench icon toggle in chat input bar (visible when tools loaded)
    - Toggling on sends tools array with chat requests
    - Tool calls displayed in chat as formatted blocks
    - Tools auto-executed via /api/mcp/execute
    - Results sent back to model, model generates final response
    - Full tool-use loop works end-to-end
  </done>
</task>

</tasks>

<verification>
- `cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate && ruff check . && mypy mlx_manager --ignore-missing-imports`
- `cd /Users/atomasini/Development/mlx-manager/frontend && npm run check && npm run lint`
- Backend: grep confirms tools in ChatRequest and SSE tool_call emission
- Frontend: grep confirms mcp client, tools toggle, tool_call handling
</verification>

<success_criteria>
- Tools toggle visible in chat UI when MCP tools are available
- Enabling tools sends tools array to backend
- Backend forwards tools to mlx-openai-server
- Model tool_calls parsed from stream and displayed
- Tool calls auto-executed via /api/mcp/execute
- Tool results sent back to model for final response
- Complete conversation with tool use works end-to-end
</success_criteria>

<output>
After completion, create `.planning/phases/06-bug-fixes-stability/06-11-SUMMARY.md`
</output>
