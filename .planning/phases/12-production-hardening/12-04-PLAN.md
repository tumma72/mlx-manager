---
phase: 12-production-hardening
plan: 04
type: execute
wave: 2
depends_on: ["12-02"]
files_modified:
  - backend/mlx_manager/mlx_server/models/audit.py
  - backend/mlx_manager/mlx_server/services/audit.py
  - backend/mlx_manager/mlx_server/database.py
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/mlx_manager/mlx_server/api/v1/completions.py
  - backend/mlx_manager/mlx_server/api/v1/embeddings.py
  - backend/mlx_manager/mlx_server/api/v1/messages.py
  - backend/tests/mlx_server/test_audit.py
autonomous: true

must_haves:
  truths:
    - "Every inference request creates an audit log entry"
    - "Audit log captures: timestamp, model, backend_type, duration, status, tokens"
    - "Prompt/response content is NEVER stored (privacy requirement)"
    - "Audit writes happen in background (non-blocking)"
    - "Errors are logged with same schema as successes"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/audit.py"
      provides: "AuditLog SQLModel"
      exports: ["AuditLog", "AuditLogResponse"]
    - path: "backend/mlx_manager/mlx_server/services/audit.py"
      provides: "Audit logging service with background writes"
      exports: ["AuditService", "audit_service"]
    - path: "backend/mlx_manager/mlx_server/database.py"
      provides: "Database session management for MLX server"
      exports: ["get_session", "init_db"]
  key_links:
    - from: "api/v1/chat.py"
      to: "services/audit.py"
      via: "audit_service.log_request() call"
      pattern: "audit_service\\.log_request"
    - from: "services/audit.py"
      to: "models/audit.py"
      via: "AuditLog model creation"
      pattern: "AuditLog\\("
---

<objective>
Implement request audit logging with background writes

Purpose: PROD-04 requires audit log capturing timestamp, model, backend type, duration, status, and token count. Per CONTEXT.md: NEVER store prompt/response content (privacy-first), 30-day retention, same schema for errors and successes.

Output: All inference endpoints log requests to SQLite with background writes
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-production-hardening/12-CONTEXT.md
@.planning/phases/12-production-hardening/12-RESEARCH.md
@backend/mlx_manager/mlx_server/config.py
@backend/mlx_manager/mlx_server/api/v1/chat.py
@backend/mlx_manager/models.py
@backend/mlx_manager/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AuditLog model and database setup for MLX server</name>
  <files>
    backend/mlx_manager/mlx_server/models/audit.py
    backend/mlx_manager/mlx_server/database.py
    backend/mlx_manager/mlx_server/config.py
  </files>
  <action>
1. Add database_path setting to `mlx_server/config.py`:
   ```python
   # Audit log database (shared with manager or separate)
   database_path: str = Field(
       default="~/.mlx-manager/mlx-server.db",
       description="Path to audit log database",
   )
   audit_retention_days: int = Field(
       default=30,
       description="Days to retain audit logs before cleanup",
   )
   ```

2. Create `backend/mlx_manager/mlx_server/models/audit.py`:
   ```python
   """Audit log models for request tracking.

   PRIVACY REQUIREMENT: Never store prompt/response content.
   Only store metadata: model, backend, duration, status, tokens.
   """

   from datetime import UTC, datetime

   from pydantic import Field as PydanticField
   from sqlmodel import Field, SQLModel


   class AuditLogBase(SQLModel):
       """Base audit log fields - metadata only, no content."""

       request_id: str = Field(index=True, description="Request ID for correlation")
       timestamp: datetime = Field(
           default_factory=lambda: datetime.now(UTC),
           index=True,
           description="Request timestamp",
       )
       model: str = Field(index=True, description="Model ID requested")
       backend_type: str = Field(
           index=True,
           description="Backend: local, openai, anthropic",
       )
       endpoint: str = Field(description="API endpoint path")
       duration_ms: int = Field(description="Request duration in milliseconds")
       status: str = Field(
           index=True,
           description="Request status: success, error, timeout",
       )

       # Token counts (optional - may not be available for all backends)
       prompt_tokens: int | None = Field(default=None, description="Prompt token count")
       completion_tokens: int | None = Field(
           default=None, description="Completion token count"
       )
       total_tokens: int | None = Field(default=None, description="Total token count")

       # Error info (only for failed requests)
       error_type: str | None = Field(
           default=None, description="Exception type if error"
       )
       error_message: str | None = Field(
           default=None, description="Error message if error"
       )


   class AuditLog(AuditLogBase, table=True):
       """Audit log database table."""

       __tablename__ = "audit_logs"  # type: ignore[assignment]

       id: int | None = Field(default=None, primary_key=True)


   class AuditLogResponse(AuditLogBase):
       """API response model for audit log entries."""

       id: int


   class AuditLogFilter(SQLModel):
       """Query filters for audit log retrieval."""

       model: str | None = None
       backend_type: str | None = None
       status: str | None = None
       start_time: datetime | None = None
       end_time: datetime | None = None
       limit: int = PydanticField(default=100, le=1000)
       offset: int = 0
   ```

3. Create `backend/mlx_manager/mlx_server/database.py`:
   ```python
   """Database setup for MLX Server audit logging."""

   import logging
   from collections.abc import AsyncGenerator
   from contextlib import asynccontextmanager
   from datetime import UTC, datetime, timedelta
   from pathlib import Path

   from sqlalchemy import delete
   from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
   from sqlmodel import SQLModel

   from mlx_manager.mlx_server.config import get_settings

   logger = logging.getLogger(__name__)

   # Lazily initialized engine and session factory
   _engine = None
   _async_session = None


   def _get_engine():
       """Get or create the database engine."""
       global _engine
       if _engine is None:
           settings = get_settings()
           db_path = Path(settings.database_path).expanduser()
           db_path.parent.mkdir(parents=True, exist_ok=True)

           _engine = create_async_engine(
               f"sqlite+aiosqlite:///{db_path}",
               echo=False,
               future=True,
           )
       return _engine


   def _get_session_factory():
       """Get or create the session factory."""
       global _async_session
       if _async_session is None:
           _async_session = async_sessionmaker(
               _get_engine(),
               class_=AsyncSession,
               expire_on_commit=False,
           )
       return _async_session


   async def init_db() -> None:
       """Initialize the database and create tables."""
       # Import models to register them
       from mlx_manager.mlx_server.models.audit import AuditLog  # noqa: F401

       engine = _get_engine()
       async with engine.begin() as conn:
           await conn.run_sync(SQLModel.metadata.create_all)
       logger.info("MLX Server database initialized")


   @asynccontextmanager
   async def get_session() -> AsyncGenerator[AsyncSession, None]:
       """Get an async database session."""
       session_factory = _get_session_factory()
       async with session_factory() as session:
           try:
               yield session
               await session.commit()
           except Exception:
               await session.rollback()
               raise
           finally:
               await session.close()


   async def cleanup_old_logs() -> int:
       """Delete audit logs older than retention period.

       Returns:
           Number of deleted records.
       """
       from mlx_manager.mlx_server.models.audit import AuditLog

       settings = get_settings()
       cutoff = datetime.now(UTC) - timedelta(days=settings.audit_retention_days)

       async with get_session() as session:
           result = await session.execute(
               delete(AuditLog).where(AuditLog.timestamp < cutoff)
           )
           deleted = result.rowcount
           if deleted > 0:
               logger.info(f"Cleaned up {deleted} audit logs older than {settings.audit_retention_days} days")
           return deleted


   def reset_for_testing() -> None:
       """Reset database state for testing."""
       global _engine, _async_session
       _engine = None
       _async_session = None
   ```
  </action>
  <verify>
    - `python -c "from mlx_manager.mlx_server.models.audit import AuditLog, AuditLogResponse; print('OK')"`
    - `python -c "from mlx_manager.mlx_server.database import init_db, get_session; print('OK')"`
    - `ls backend/mlx_manager/mlx_server/models/audit.py backend/mlx_manager/mlx_server/database.py`
  </verify>
  <done>AuditLog model and database module exist</done>
</task>

<task type="auto">
  <name>Task 2: Create audit service with background writes</name>
  <files>
    backend/mlx_manager/mlx_server/services/audit.py
  </files>
  <action>
Create `backend/mlx_manager/mlx_server/services/audit.py`:

```python
"""Audit logging service with non-blocking background writes.

PRIVACY REQUIREMENT: This service NEVER receives or stores prompt/response content.
Only request metadata is logged: model, backend, duration, status, tokens.
"""

import asyncio
import logging
import time
from collections.abc import Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import UTC, datetime
from typing import Any

from mlx_manager.mlx_server.database import get_session
from mlx_manager.mlx_server.models.audit import AuditLog

logger = logging.getLogger(__name__)


@dataclass
class RequestContext:
    """Context for tracking request lifecycle.

    Created at request start, completed at request end.
    """

    request_id: str
    model: str
    endpoint: str
    backend_type: str = "local"
    start_time: float = field(default_factory=time.time)

    # Set when request completes
    status: str | None = None
    prompt_tokens: int | None = None
    completion_tokens: int | None = None
    total_tokens: int | None = None
    error_type: str | None = None
    error_message: str | None = None

    @property
    def duration_ms(self) -> int:
        """Calculate duration from start_time to now."""
        return int((time.time() - self.start_time) * 1000)


class AuditService:
    """Service for writing audit logs with background task execution.

    Uses asyncio.create_task for non-blocking writes.
    Maintains an in-memory buffer for WebSocket broadcasting.
    """

    def __init__(self, buffer_size: int = 100) -> None:
        self._buffer_size = buffer_size
        self._recent_logs: list[dict[str, Any]] = []
        self._subscribers: set[Callable[[dict[str, Any]], None]] = set()

    @asynccontextmanager
    async def track_request(
        self,
        request_id: str,
        model: str,
        endpoint: str,
        backend_type: str = "local",
    ):
        """Context manager for tracking request lifecycle.

        Usage:
            async with audit_service.track_request(req_id, model, endpoint) as ctx:
                # ... do work ...
                ctx.prompt_tokens = 100
                ctx.completion_tokens = 50
        """
        ctx = RequestContext(
            request_id=request_id,
            model=model,
            endpoint=endpoint,
            backend_type=backend_type,
        )
        try:
            yield ctx
            ctx.status = "success"
        except asyncio.TimeoutError:
            ctx.status = "timeout"
            ctx.error_type = "TimeoutError"
            ctx.error_message = "Request timed out"
            raise
        except Exception as e:
            ctx.status = "error"
            ctx.error_type = type(e).__name__
            ctx.error_message = str(e)[:500]  # Truncate long error messages
            raise
        finally:
            # Always log, even on error
            asyncio.create_task(self._write_log(ctx))

    async def log_request(
        self,
        request_id: str,
        model: str,
        endpoint: str,
        backend_type: str,
        duration_ms: int,
        status: str,
        prompt_tokens: int | None = None,
        completion_tokens: int | None = None,
        total_tokens: int | None = None,
        error_type: str | None = None,
        error_message: str | None = None,
    ) -> None:
        """Log a completed request (alternative to track_request context manager).

        For cases where context manager pattern doesn't fit.
        """
        log_entry = AuditLog(
            request_id=request_id,
            timestamp=datetime.now(UTC),
            model=model,
            endpoint=endpoint,
            backend_type=backend_type,
            duration_ms=duration_ms,
            status=status,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens,
            error_type=error_type,
            error_message=error_message,
        )
        asyncio.create_task(self._write_log_entry(log_entry))

    async def _write_log(self, ctx: RequestContext) -> None:
        """Write audit log from RequestContext."""
        log_entry = AuditLog(
            request_id=ctx.request_id,
            timestamp=datetime.now(UTC),
            model=ctx.model,
            endpoint=ctx.endpoint,
            backend_type=ctx.backend_type,
            duration_ms=ctx.duration_ms,
            status=ctx.status or "unknown",
            prompt_tokens=ctx.prompt_tokens,
            completion_tokens=ctx.completion_tokens,
            total_tokens=ctx.total_tokens,
            error_type=ctx.error_type,
            error_message=ctx.error_message,
        )
        await self._write_log_entry(log_entry)

    async def _write_log_entry(self, log_entry: AuditLog) -> None:
        """Write log entry to database and notify subscribers."""
        try:
            async with get_session() as session:
                session.add(log_entry)
                await session.commit()
                await session.refresh(log_entry)

            # Add to recent buffer
            log_dict = log_entry.model_dump()
            log_dict["timestamp"] = log_dict["timestamp"].isoformat()
            self._recent_logs.append(log_dict)
            if len(self._recent_logs) > self._buffer_size:
                self._recent_logs.pop(0)

            # Notify subscribers (for WebSocket broadcast)
            for callback in self._subscribers:
                try:
                    callback(log_dict)
                except Exception as e:
                    logger.warning(f"Subscriber callback error: {e}")

        except Exception as e:
            logger.error(f"Failed to write audit log: {e}")

    def subscribe(self, callback: Callable[[dict[str, Any]], None]) -> None:
        """Subscribe to new log entries for live updates."""
        self._subscribers.add(callback)

    def unsubscribe(self, callback: Callable[[dict[str, Any]], None]) -> None:
        """Unsubscribe from log entries."""
        self._subscribers.discard(callback)

    def get_recent_logs(self) -> list[dict[str, Any]]:
        """Get recent logs from in-memory buffer."""
        return list(self._recent_logs)


# Global singleton
audit_service = AuditService()
```
  </action>
  <verify>
    - `python -c "from mlx_manager.mlx_server.services.audit import audit_service, AuditService; print('OK')"`
    - `ls backend/mlx_manager/mlx_server/services/audit.py`
  </verify>
  <done>Audit service with background writes and subscription support exists</done>
</task>

<task type="auto">
  <name>Task 3: Integrate audit logging into inference endpoints and add tests</name>
  <files>
    backend/mlx_manager/mlx_server/api/v1/chat.py
    backend/mlx_manager/mlx_server/api/v1/completions.py
    backend/mlx_manager/mlx_server/api/v1/embeddings.py
    backend/mlx_manager/mlx_server/api/v1/messages.py
    backend/mlx_manager/mlx_server/main.py
    backend/tests/mlx_server/test_audit.py
  </files>
  <action>
1. Update `mlx_server/main.py` to initialize audit database:
   ```python
   # In lifespan function, after model pool init:
   from mlx_manager.mlx_server.database import init_db as init_audit_db

   @asynccontextmanager
   async def lifespan(app: FastAPI):
       # Startup
       logger.info("MLX Server starting...")
       await init_audit_db()  # Add this
       # ... rest of startup
   ```

2. Update `api/v1/chat.py` - add audit logging:
   At the top of `create_chat_completion`:
   ```python
   from mlx_manager.mlx_server.services.audit import audit_service
   import uuid

   @router.post("/v1/chat/completions", response_model=None)
   async def create_chat_completion(request: ChatCompletionRequest):
       request_id = f"chatcmpl-{uuid.uuid4().hex[:12]}"

       async with audit_service.track_request(
           request_id=request_id,
           model=request.model,
           endpoint="/v1/chat/completions",
           backend_type="local",  # Updated by cloud routing if used
       ) as ctx:
           # ... existing logic ...
           # Update ctx with tokens when available:
           # ctx.prompt_tokens = usage.prompt_tokens
           # ctx.completion_tokens = usage.completion_tokens
   ```

   For streaming, the context manager wraps the entire stream generation.

3. Update `api/v1/completions.py` - same pattern

4. Update `api/v1/embeddings.py` - same pattern

5. Update `api/v1/messages.py` - same pattern for Anthropic endpoint

6. Create `backend/tests/mlx_server/test_audit.py`:
   ```python
   """Tests for audit logging service."""

   import asyncio
   from datetime import UTC, datetime

   import pytest

   from mlx_manager.mlx_server.models.audit import AuditLog
   from mlx_manager.mlx_server.services.audit import AuditService, RequestContext


   class TestRequestContext:
       """Test RequestContext dataclass."""

       def test_duration_ms_calculation(self) -> None:
           """duration_ms calculates time since start."""
           import time

           ctx = RequestContext(
               request_id="test-123",
               model="test-model",
               endpoint="/v1/test",
           )
           time.sleep(0.1)  # 100ms
           duration = ctx.duration_ms

           assert 100 <= duration <= 200  # Allow some variance


   class TestAuditService:
       """Test AuditService."""

       @pytest.fixture
       def service(self) -> AuditService:
           return AuditService(buffer_size=10)

       @pytest.mark.asyncio
       async def test_track_request_success(self, service: AuditService) -> None:
           """track_request logs successful requests."""
           logged_entries: list[dict] = []
           service.subscribe(lambda entry: logged_entries.append(entry))

           # Mock the database write
           async def mock_write(entry: AuditLog) -> None:
               service._recent_logs.append(entry.model_dump())

           service._write_log_entry = mock_write

           async with service.track_request(
               request_id="req-123",
               model="test-model",
               endpoint="/v1/chat/completions",
           ) as ctx:
               ctx.prompt_tokens = 100
               ctx.completion_tokens = 50

           # Wait for background task
           await asyncio.sleep(0.1)

           assert len(service._recent_logs) == 1
           log = service._recent_logs[0]
           assert log["request_id"] == "req-123"
           assert log["status"] == "success"
           assert log["prompt_tokens"] == 100

       @pytest.mark.asyncio
       async def test_track_request_error(self, service: AuditService) -> None:
           """track_request logs errors with error details."""
           async def mock_write(entry: AuditLog) -> None:
               service._recent_logs.append(entry.model_dump())

           service._write_log_entry = mock_write

           with pytest.raises(ValueError):
               async with service.track_request(
                   request_id="req-err",
                   model="test-model",
                   endpoint="/v1/test",
               ):
                   raise ValueError("Test error")

           await asyncio.sleep(0.1)

           assert len(service._recent_logs) == 1
           log = service._recent_logs[0]
           assert log["status"] == "error"
           assert log["error_type"] == "ValueError"
           assert "Test error" in log["error_message"]

       def test_subscribe_unsubscribe(self, service: AuditService) -> None:
           """Subscribers can be added and removed."""
           callback = lambda x: None

           service.subscribe(callback)
           assert callback in service._subscribers

           service.unsubscribe(callback)
           assert callback not in service._subscribers

       def test_buffer_size_limit(self, service: AuditService) -> None:
           """Recent logs buffer respects size limit."""
           for i in range(20):
               service._recent_logs.append({"id": i})
               if len(service._recent_logs) > service._buffer_size:
                   service._recent_logs.pop(0)

           assert len(service._recent_logs) == 10
           assert service._recent_logs[0]["id"] == 10  # Oldest kept


   class TestAuditLogModel:
       """Test AuditLog SQLModel."""

       def test_audit_log_creation(self) -> None:
           """AuditLog can be created with required fields."""
           log = AuditLog(
               request_id="test-123",
               model="test-model",
               backend_type="local",
               endpoint="/v1/chat/completions",
               duration_ms=1500,
               status="success",
           )

           assert log.request_id == "test-123"
           assert log.timestamp is not None
           assert log.prompt_tokens is None  # Optional

       def test_audit_log_with_tokens(self) -> None:
           """AuditLog can include token counts."""
           log = AuditLog(
               request_id="test-456",
               model="gpt-4",
               backend_type="openai",
               endpoint="/v1/chat/completions",
               duration_ms=500,
               status="success",
               prompt_tokens=100,
               completion_tokens=200,
               total_tokens=300,
           )

           assert log.total_tokens == 300

       def test_audit_log_error(self) -> None:
           """AuditLog can capture error details."""
           log = AuditLog(
               request_id="test-err",
               model="test-model",
               backend_type="local",
               endpoint="/v1/test",
               duration_ms=100,
               status="error",
               error_type="RuntimeError",
               error_message="Something went wrong",
           )

           assert log.status == "error"
           assert log.error_type == "RuntimeError"
   ```
  </action>
  <verify>
    - `cd backend && pytest tests/mlx_server/test_audit.py -v`
    - `grep "audit_service" backend/mlx_manager/mlx_server/api/v1/chat.py`
    - `grep "init_audit_db" backend/mlx_manager/mlx_server/main.py`
    - Verify database initialization:
      ```bash
      python -c "
      import asyncio
      from mlx_manager.mlx_server.database import init_db, get_session
      from mlx_manager.mlx_server.config import get_settings
      from pathlib import Path

      async def test_init():
          await init_db()
          settings = get_settings()
          db_path = Path(settings.database_path).expanduser()
          assert db_path.exists(), f'Database file not created at {db_path}'

          # Verify table schema by checking it's queryable
          async with get_session() as session:
              from sqlalchemy import text
              result = await session.execute(text('SELECT sql FROM sqlite_master WHERE name=\"audit_logs\"'))
              schema = result.scalar()
              assert 'request_id' in schema
              assert 'timestamp' in schema
              assert 'model' in schema
              print('Database and audit_logs table verified')

      asyncio.run(test_init())
      "
      ```
  </verify>
  <done>Audit logging integrated into all inference endpoints with tests and database verification</done>
</task>

</tasks>

<verification>
Run audit tests:
```bash
cd backend && source .venv/bin/activate
pytest tests/mlx_server/test_audit.py -v
```

Run full test suite:
```bash
pytest tests/mlx_server/ -v
```

Verify no prompt/response content is stored:
```bash
# Search for any content-related fields in audit log
grep -r "content" backend/mlx_manager/mlx_server/models/audit.py
# Should return nothing related to request/response content
```

Verify database initialization:
```bash
python -c "
import asyncio
from mlx_manager.mlx_server.database import init_db
from mlx_manager.mlx_server.config import get_settings
from pathlib import Path

async def verify():
    await init_db()
    db_path = Path(get_settings().database_path).expanduser()
    print(f'Database exists: {db_path.exists()}')
    print(f'Database path: {db_path}')

asyncio.run(verify())
"
```
</verification>

<success_criteria>
- AuditLog model exists with required fields (no content fields)
- Audit service uses background writes (asyncio.create_task)
- All inference endpoints use audit_service.track_request
- Errors are logged with same schema as successes
- In-memory buffer exists for WebSocket broadcasting
- Database initialization in lifespan
- Database file is created and audit_logs table has correct schema
- All audit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/12-production-hardening/12-04-SUMMARY.md`
</output>
