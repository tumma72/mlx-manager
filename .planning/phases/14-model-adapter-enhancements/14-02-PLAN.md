---
phase: 14-model-adapter-enhancements
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - backend/mlx_manager/mlx_server/models/adapters/parsers/__init__.py
  - backend/mlx_manager/mlx_server/models/adapters/parsers/base.py
  - backend/mlx_manager/mlx_server/models/adapters/parsers/llama.py
  - backend/mlx_manager/mlx_server/models/adapters/parsers/qwen.py
  - backend/mlx_manager/mlx_server/models/adapters/parsers/glm4.py
  - backend/mlx_manager/mlx_server/models/adapters/llama.py
  - backend/mlx_manager/mlx_server/models/adapters/qwen.py
  - backend/mlx_manager/mlx_server/models/adapters/glm4.py
  - backend/mlx_manager/mlx_server/models/adapters/registry.py
autonomous: true

must_haves:
  truths:
    - "Llama tool calls in <function=name>{args}</function> format are parsed correctly"
    - "Qwen tool calls in Hermes <tool_call>{json}</tool_call> format are parsed correctly"
    - "GLM4 tool calls in XML format are parsed correctly"
    - "Adapters indicate tool calling support via supports_tool_calling()"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/adapters/parsers/llama.py"
      provides: "Llama tool call parser"
      contains: "class LlamaToolParser"
    - path: "backend/mlx_manager/mlx_server/models/adapters/parsers/qwen.py"
      provides: "Qwen tool call parser"
      contains: "class QwenToolParser"
    - path: "backend/mlx_manager/mlx_server/models/adapters/parsers/glm4.py"
      provides: "GLM4 tool call parser"
      contains: "class GLM4ToolParser"
    - path: "backend/mlx_manager/mlx_server/models/adapters/glm4.py"
      provides: "GLM4 model adapter"
      contains: "class GLM4Adapter"
  key_links:
    - from: "adapters/llama.py"
      to: "parsers/llama.py"
      via: "LlamaToolParser import"
      pattern: "from.*parsers.*import.*LlamaToolParser"
    - from: "adapters/qwen.py"
      to: "parsers/qwen.py"
      via: "QwenToolParser import"
      pattern: "from.*parsers.*import.*QwenToolParser"
---

<objective>
Implement model-specific tool call parsers for Llama, Qwen, and GLM4 model families, and wire them into the corresponding adapters.

Purpose: Enable tool/function calling by parsing model output into OpenAI-compatible ToolCall format. Each model family has its own unique output format that needs dedicated parsing.

Output: Three tool parsers in parsers/ directory, GLM4 adapter, and updated Llama/Qwen adapters with tool calling support.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-model-adapter-enhancements/14-RESEARCH.md
@backend/mlx_manager/mlx_server/models/adapters/base.py
@backend/mlx_manager/mlx_server/models/adapters/llama.py
@backend/mlx_manager/mlx_server/models/adapters/qwen.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Tool Parser Base and Llama Parser</name>
  <files>
backend/mlx_manager/mlx_server/models/adapters/parsers/__init__.py
backend/mlx_manager/mlx_server/models/adapters/parsers/base.py
backend/mlx_manager/mlx_server/models/adapters/parsers/llama.py
  </files>
  <action>
Create the parsers/ directory structure and implement:

1. parsers/__init__.py - Export all parsers
2. parsers/base.py - ToolCallParser abstract base class with:
   - parse(text: str) -> list[dict] - Parse tool calls from output
   - format_tools(tools: list[dict]) -> str - Format tools for prompt injection

3. parsers/llama.py - LlamaToolParser implementing:
   - Parse Llama 3.x tool call formats:
     - XML style: `<function=name>{"param": "value"}</function>`
     - Python style: `<|python_tag|>tool.call(query="...")<|eom_id|>`
   - format_tools() generates Llama-style tool documentation
   - Return OpenAI-compatible format: {"id": str, "type": "function", "function": {"name": str, "arguments": str}}
   - Use uuid.uuid4().hex[:8] for tool call IDs with "call_" prefix

Reference: 14-RESEARCH.md Pattern 2 and Example 2
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.adapters.parsers.llama import LlamaToolParser
parser = LlamaToolParser()
# Test XML-style parsing
result = parser.parse('<function=get_weather>{\"city\": \"SF\"}</function>')
assert len(result) == 1
assert result[0]['function']['name'] == 'get_weather'
print('Llama parser works:', result)
"
```
  </verify>
  <done>
ToolCallParser base class exists. LlamaToolParser parses `<function=name>{args}</function>` format and formats tools for Llama prompts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Qwen and GLM4 Parsers</name>
  <files>
backend/mlx_manager/mlx_server/models/adapters/parsers/qwen.py
backend/mlx_manager/mlx_server/models/adapters/parsers/glm4.py
  </files>
  <action>
Implement Qwen and GLM4 tool parsers:

1. parsers/qwen.py - QwenToolParser:
   - Parse Hermes-style format: `<tool_call>{"name": "func", "arguments": {...}}</tool_call>`
   - Handle multiple tool calls in single response
   - format_tools() generates Qwen/Hermes-style tool documentation
   - Reference: 14-RESEARCH.md Example 1

2. parsers/glm4.py - GLM4ToolParser:
   - Parse GLM4 XML format with `<tool_call>` markers and nested `<arg_key>/<arg_value>` tags
   - Handle duplicate tool call markers (known GLM4 bug per 14-RESEARCH.md Pitfall 6)
   - Deduplicate by content hash

Update parsers/__init__.py to export all three parsers.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.adapters.parsers import QwenToolParser, GLM4ToolParser, LlamaToolParser

# Test Qwen parser
qwen = QwenToolParser()
result = qwen.parse('<tool_call>{\"name\": \"search\", \"arguments\": {\"query\": \"test\"}}</tool_call>')
assert len(result) == 1
assert result[0]['function']['name'] == 'search'
print('Qwen parser works')

# Test GLM4 parser
glm4 = GLM4ToolParser()
result = glm4.parse('<tool_call><name>calc</name><arguments>{\"x\": 5}</arguments></tool_call>')
assert len(result) == 1
print('GLM4 parser works')
"
```
  </verify>
  <done>
QwenToolParser parses Hermes-style tool calls. GLM4ToolParser parses XML format with deduplication.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire Parsers into Adapters and Add GLM4 Adapter</name>
  <files>
backend/mlx_manager/mlx_server/models/adapters/llama.py
backend/mlx_manager/mlx_server/models/adapters/qwen.py
backend/mlx_manager/mlx_server/models/adapters/glm4.py
backend/mlx_manager/mlx_server/models/adapters/registry.py
  </files>
  <action>
1. Update LlamaAdapter:
   - Add supports_tool_calling() -> True
   - Add parse_tool_calls() using LlamaToolParser
   - Add format_tools_for_prompt() using LlamaToolParser
   - Add get_tool_call_stop_tokens() returning </function> and <|eom_id|> tokens

2. Update QwenAdapter:
   - Add supports_tool_calling() -> True
   - Add parse_tool_calls() using QwenToolParser
   - Add format_tools_for_prompt() using QwenToolParser
   - Add get_tool_call_stop_tokens() returning </tool_call> token

3. Create GLM4Adapter (new file):
   - Implement family property returning "glm4"
   - Implement apply_chat_template (GLM4 uses ChatML-like format)
   - Implement get_stop_tokens
   - Add supports_tool_calling() -> True
   - Add parse_tool_calls() using GLM4ToolParser
   - Add format_tools_for_prompt() using GLM4ToolParser

4. Update registry.py:
   - Import and register GLM4Adapter
   - Update detect_model_family() to recognize "glm" and "chatglm" patterns

Note: get_tool_call_stop_tokens uses tokenizer.convert_tokens_to_ids() to look up token IDs, not hardcoded values.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.adapters import get_adapter
from mlx_manager.mlx_server.models.adapters.registry import detect_model_family

# Test family detection
assert detect_model_family('mlx-community/Llama-3.2-3B-Instruct') == 'llama'
assert detect_model_family('Qwen/Qwen2.5-7B-Instruct') == 'qwen'
assert detect_model_family('THUDM/glm-4-9b-chat') == 'glm4'

# Test adapter tool support
llama = get_adapter('mlx-community/Llama-3.2-3B-Instruct')
assert llama.supports_tool_calling() == True

qwen = get_adapter('Qwen/Qwen2.5-7B-Instruct')
assert qwen.supports_tool_calling() == True

glm4 = get_adapter('THUDM/glm-4-9b-chat')
assert glm4.supports_tool_calling() == True

print('All adapters support tool calling')
"
```
  </verify>
  <done>
Llama, Qwen, and GLM4 adapters implement tool calling. GLM4Adapter created and registered. All three adapters return True for supports_tool_calling().
  </done>
</task>

</tasks>

<verification>
- [ ] parsers/ directory exists with base.py, llama.py, qwen.py, glm4.py
- [ ] LlamaToolParser parses `<function=name>{args}</function>` format
- [ ] QwenToolParser parses `<tool_call>{json}</tool_call>` format
- [ ] GLM4ToolParser parses XML tool calls with deduplication
- [ ] All three adapters return True for supports_tool_calling()
- [ ] GLM4Adapter registered in registry
- [ ] detect_model_family() recognizes "glm" pattern
- [ ] Quality gates pass (ruff, mypy)
</verification>

<success_criteria>
1. Three model-specific tool parsers exist and parse their respective formats
2. LlamaAdapter, QwenAdapter, GLM4Adapter implement tool calling methods
3. GLM4Adapter created and registered
4. detect_model_family() recognizes GLM4 models
5. Parsers return OpenAI-compatible ToolCall format
6. All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-model-adapter-enhancements/14-02-SUMMARY.md`
</output>
