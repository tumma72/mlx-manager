---
phase: 14-model-adapter-enhancements
plan: 08
type: execute
wave: 4
depends_on: ["14-07"]
files_modified:
  - backend/mlx_manager/mlx_server/services/response_processor.py
  - backend/mlx_manager/mlx_server/services/inference.py
  - backend/tests/mlx_server/test_response_processor.py
autonomous: true

must_haves:
  truths:
    - "StreamingProcessor filters patterns during token generation"
    - "Tool call markers never reach the client in streaming"
    - "Reasoning content extracted in streaming mode"
    - "Streaming and non-streaming have feature parity"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/response_processor.py"
      provides: "StreamingProcessor class for filtering during generation"
      contains: "StreamingProcessor"
---

<objective>
Add streaming-aware response processing that filters tool call markers and extracts reasoning during token generation, not just after.

Purpose: Currently streaming yields raw tokens including `<tool_call>` markers to the client, then processes after. This shows ugly XML to users. We should buffer when a pattern starts, process when it ends, and never yield raw markers.

Output: StreamingProcessor class that integrates with the token generation loop.
</objective>

<context>
Current streaming issues:
1. Reasoning extraction not done in streaming (only non-streaming)
2. Tool call markers visible to client during streaming
3. Post-processing happens after all tokens yielded

mlx-omni-server approach:
- Buffer when pattern start detected (e.g., "<tool_call>")
- Continue buffering until pattern end
- Process complete pattern, don't yield raw content
- Yield cleaned content and structured data separately
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create StreamingProcessor Class</name>
  <files>backend/mlx_manager/mlx_server/services/response_processor.py</files>
  <action>
Add StreamingProcessor to response_processor.py:

```python
class StreamingProcessor:
    """Streaming-aware processor that filters patterns during generation.

    Buffers tokens when pattern start markers are detected, processes
    complete patterns, and yields clean content to client.

    Usage:
        processor = StreamingProcessor()
        for token in generation:
            output, should_yield = processor.feed(token)
            if should_yield and output:
                yield output
        # After generation
        result = processor.finalize()
    """

    # Pattern start markers to detect
    PATTERN_STARTS = [
        "<think>", "<thinking>", "<reasoning>", "<reflection>",
        "<tool_call>", "<function=",
    ]

    # Pattern end markers (map start -> end)
    PATTERN_ENDS = {
        "<think>": "</think>",
        "<thinking>": "</thinking>",
        "<reasoning>": "</reasoning>",
        "<reflection>": "</reflection>",
        "<tool_call>": "</tool_call>",
        "<function=": "</function>",
    }

    def __init__(self, response_processor: ResponseProcessor | None = None):
        self._processor = response_processor or get_response_processor()
        self._buffer = ""
        self._pending_buffer = ""  # For partial marker detection
        self._in_pattern = False
        self._pattern_start = ""
        self._accumulated = ""  # Full response for final processing
        self._yielded_content = ""  # What we've yielded to client

    def feed(self, token: str) -> tuple[str | None, bool]:
        """Feed a token, get (output_text, should_yield).

        Returns:
            (text, True) - Yield this text to client
            (None, False) - Don't yield, buffering pattern
            ("", True) - Yield empty (end of clean segment)
        """
        self._accumulated += token

        if self._in_pattern:
            # We're inside a pattern, buffer until end
            self._buffer += token
            end_marker = self.PATTERN_ENDS.get(self._pattern_start, "")
            if end_marker and end_marker in self._buffer:
                # Pattern complete, process it (but don't yield raw content)
                self._in_pattern = False
                self._buffer = ""
                self._pattern_start = ""
                return (None, False)
            return (None, False)

        # Check if token starts or contains a pattern start
        combined = self._pending_buffer + token
        self._pending_buffer = ""

        for start in self.PATTERN_STARTS:
            if start in combined:
                # Pattern start found
                idx = combined.index(start)
                before = combined[:idx]
                self._in_pattern = True
                self._pattern_start = start
                self._buffer = combined[idx:]
                if before:
                    self._yielded_content += before
                    return (before, True)
                return (None, False)

            # Check for partial match at end (e.g., "<tool" might become "<tool_call>")
            for i in range(1, len(start)):
                if combined.endswith(start[:i]):
                    self._pending_buffer = combined[-(i):]
                    to_yield = combined[:-(i)]
                    if to_yield:
                        self._yielded_content += to_yield
                        return (to_yield, True)
                    return (None, False)

        # No pattern detected, yield token
        self._yielded_content += combined
        return (combined, True)

    def finalize(self) -> ParseResult:
        """Finalize and get complete ParseResult.

        Called after all tokens processed. Uses ResponseProcessor
        to extract structured data from accumulated text.
        """
        # Process full accumulated text
        result = self._processor.process(self._accumulated)
        return result

    def get_pending_content(self) -> str:
        """Get any buffered content not yet yielded."""
        return self._pending_buffer + self._buffer
```

Key behaviors:
- Partial marker detection (e.g., "<tool" buffered in case it becomes "<tool_call>")
- Pattern content never yielded to client
- Final processing extracts tool_calls and reasoning from accumulated text
- Content field in result is the clean version
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.services.response_processor import StreamingProcessor

processor = StreamingProcessor()

# Simulate streaming tokens
tokens = ['Hello', ' ', '<', 'think', '>', 'analyzing', '</think>', ' world']
outputs = []
for token in tokens:
    output, should_yield = processor.feed(token)
    if should_yield and output:
        outputs.append(output)

result = processor.finalize()

# Should have yielded 'Hello ' and ' world', not the think tags
yielded = ''.join(outputs)
print(f'Yielded to client: {repr(yielded)}')
assert '<think>' not in yielded
assert 'analyzing' not in yielded
print('Think tags filtered from stream: OK')

# Result should have reasoning extracted
assert result.reasoning is not None
print(f'Reasoning extracted: {repr(result.reasoning)}')
print('StreamingProcessor working correctly!')
"
```
  </verify>
  <done>
StreamingProcessor class created with pattern detection, buffering, and filtering during token generation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate StreamingProcessor into Inference</name>
  <files>backend/mlx_manager/mlx_server/services/inference.py</files>
  <action>
Update _stream_chat_generate() to use StreamingProcessor:

1. Import StreamingProcessor:
```python
from mlx_manager.mlx_server.services.response_processor import (
    get_response_processor, StreamingProcessor, ParseResult
)
```

2. Create processor at start of streaming:
```python
stream_processor = StreamingProcessor()
```

3. In token loop, use processor.feed():
```python
# OLD:
# yield {"delta": {"content": token_text}, ...}

# NEW:
filtered_output, should_yield = stream_processor.feed(token_text)
if should_yield and filtered_output:
    yield {
        "id": completion_id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": model_id,
        "choices": [{
            "index": 0,
            "delta": {"content": filtered_output},
            "finish_reason": None,
        }]
    }
```

4. After generation loop, finalize:
```python
result = stream_processor.finalize()

# Build final chunk with tool_calls and finish_reason
tool_calls = [tc.model_dump() for tc in result.tool_calls] if result.tool_calls else None
finish_reason = "tool_calls" if tool_calls else finish_reason

# Include reasoning in final metadata if present
final_delta = {}
if tool_calls:
    final_delta["tool_calls"] = [...]
# Note: reasoning_content typically not streamed, but available in result
```

5. Ensure accumulated text is available for:
   - Token counting
   - Logging
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
import ast
import inspect
from mlx_manager.mlx_server.services import inference

source = inspect.getsource(inference)
assert 'StreamingProcessor' in source
print('StreamingProcessor integrated into inference: OK')
"
```
  </verify>
  <done>
Streaming inference now uses StreamingProcessor to filter patterns during generation. Tool call markers and thinking tags never reach the client.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add Streaming Tests</name>
  <files>backend/tests/mlx_server/test_response_processor.py</files>
  <action>
Add tests for StreamingProcessor to existing test file:

1. Test basic filtering:
   - Think tags not yielded
   - Tool call tags not yielded
   - Clean content yielded

2. Test partial marker handling:
   - Token ends with "<tool" (partial)
   - Next token completes "_call>"
   - Buffering works correctly

3. Test multiple patterns:
   - Both thinking and tool call in stream
   - Neither leaked to output

4. Test finalize():
   - Returns complete ParseResult
   - tool_calls populated
   - reasoning populated
   - content is clean

5. Test edge cases:
   - Empty tokens
   - Pattern split across many tokens
   - No patterns (passthrough)
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_response_processor.py -v -k streaming --tb=short
```
  </verify>
  <done>
Streaming tests added covering filtering, partial markers, multiple patterns, and finalization.
  </done>
</task>

<task type="human">
  <name>Task 4: Verification Checkpoint</name>
  <action>
Verify streaming filtering works:

1. Run tests:
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_response_processor.py -v --tb=short
```

2. Quality checks:
```bash
ruff check . && ruff format --check . && mypy mlx_manager
```

3. If possible, test with actual streaming:
   - Use chat UI with streaming enabled
   - Confirm no raw XML tags visible
   - Confirm tool calls appear in final response
  </action>
</task>

</tasks>

<verification>
- [ ] StreamingProcessor class created
- [ ] Pattern start detection works
- [ ] Buffering continues until pattern end
- [ ] Partial marker detection (cross-token)
- [ ] finalize() returns correct ParseResult
- [ ] Inference service uses StreamingProcessor
- [ ] Tests pass for streaming scenarios
- [ ] Quality gates pass
</verification>

<success_criteria>
1. Tool call markers never visible during streaming
2. Thinking tags never visible during streaming
3. Reasoning content extracted in streaming mode
4. Feature parity between streaming and non-streaming
5. All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-model-adapter-enhancements/14-08-SUMMARY.md`
</output>
