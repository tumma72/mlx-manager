---
phase: 14-model-adapter-enhancements
plan: 03
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - backend/mlx_manager/mlx_server/services/reasoning.py
  - backend/mlx_manager/mlx_server/models/adapters/llama.py
  - backend/mlx_manager/mlx_server/models/adapters/qwen.py
  - backend/mlx_manager/mlx_server/schemas/openai.py
autonomous: true

must_haves:
  truths:
    - "Reasoning content in <think> tags is extracted separately from final content"
    - "Reasoning content in <thinking> tags is extracted separately"
    - "Reasoning content in <reasoning> tags is extracted separately"
    - "Models that support reasoning return True for supports_reasoning_mode()"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/reasoning.py"
      provides: "ReasoningExtractor service"
      contains: "class ReasoningExtractor"
    - path: "backend/mlx_manager/mlx_server/schemas/openai.py"
      provides: "Extended response with reasoning_content field"
      contains: "reasoning_content"
  key_links:
    - from: "adapters/llama.py"
      to: "services/reasoning.py"
      via: "ReasoningExtractor import"
      pattern: "from.*reasoning.*import.*ReasoningExtractor"
---

<objective>
Implement reasoning/thinking mode extraction to separate chain-of-thought content from final responses.

Purpose: Modern reasoning models (DeepSeek-R1, Qwen3-thinking, Llama-thinking) output thinking content in special tags. Users need access to both the reasoning process and the final answer.

Output: ReasoningExtractor service, updated adapters with reasoning support, and extended schemas for reasoning_content field.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-model-adapter-enhancements/14-RESEARCH.md
@backend/mlx_manager/mlx_server/models/adapters/base.py
@backend/mlx_manager/mlx_server/models/adapters/llama.py
@backend/mlx_manager/mlx_server/models/adapters/qwen.py
@backend/mlx_manager/mlx_server/schemas/openai.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ReasoningExtractor Service</name>
  <files>backend/mlx_manager/mlx_server/services/reasoning.py</files>
  <action>
Create a new service for extracting reasoning content from model output.

Implement ReasoningExtractor class:

1. Define THINKING_PATTERNS - list of (regex_pattern, tag_name) tuples:
   - `<think>(.*?)</think>` - DeepSeek-R1 style
   - `<thinking>(.*?)</thinking>` - Alternative thinking tag
   - `<reasoning>(.*?)</reasoning>` - Explicit reasoning tag
   - `<reflection>(.*?)</reflection>` - Reflection tag (some models)

2. extract(text: str) -> tuple[str | None, str] method:
   - Search for all thinking patterns (use re.DOTALL for multiline)
   - Collect all reasoning content from matched tags
   - Remove matched tags from the content
   - Return (combined_reasoning or None, cleaned_content.strip())
   - Handle nested tags gracefully (outer tags first)

3. has_reasoning_tags(text: str) -> bool method:
   - Quick check if text contains any reasoning tags
   - Used for early exit in adapters

Reference: 14-RESEARCH.md Pattern 3 and Pitfall 4
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.services.reasoning import ReasoningExtractor

extractor = ReasoningExtractor()

# Test <think> tag
text = '<think>Let me analyze this...</think>The answer is 42.'
reasoning, content = extractor.extract(text)
assert reasoning == 'Let me analyze this...'
assert content == 'The answer is 42.'

# Test <thinking> tag
text2 = '<thinking>Step 1: Consider X</thinking>Final answer: Y'
reasoning2, content2 = extractor.extract(text2)
assert reasoning2 == 'Step 1: Consider X'
assert content2 == 'Final answer: Y'

# Test no reasoning
text3 = 'Just a normal response'
reasoning3, content3 = extractor.extract(text3)
assert reasoning3 is None
assert content3 == 'Just a normal response'

print('ReasoningExtractor works correctly')
"
```
  </verify>
  <done>
ReasoningExtractor service extracts thinking content from `<think>`, `<thinking>`, `<reasoning>`, and `<reflection>` tags.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Reasoning Support to Adapters</name>
  <files>
backend/mlx_manager/mlx_server/models/adapters/llama.py
backend/mlx_manager/mlx_server/models/adapters/qwen.py
  </files>
  <action>
Update Llama and Qwen adapters with reasoning mode support:

1. LlamaAdapter:
   - Add supports_reasoning_mode() -> True
   - Add extract_reasoning() using ReasoningExtractor
   - Llama 3.x thinking models use `<think>` tags

2. QwenAdapter:
   - Add supports_reasoning_mode() -> True
   - Add extract_reasoning() using ReasoningExtractor
   - Qwen3-thinking uses `<think>` tags

Both adapters should:
- Import ReasoningExtractor from services.reasoning
- Create a single extractor instance (module-level or in __init__)
- Delegate extract_reasoning() to the extractor

Note: Not all Llama/Qwen models support reasoning - only "thinking" variants. The adapter reports capability, but the endpoint will only use it when the model actually outputs reasoning tags.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.adapters import get_adapter

llama = get_adapter('mlx-community/Llama-3.2-3B-Instruct')
assert llama.supports_reasoning_mode() == True

# Test extraction
text = '<think>Processing...</think>Here is the result.'
reasoning, content = llama.extract_reasoning(text)
assert reasoning == 'Processing...'
assert content == 'Here is the result.'

qwen = get_adapter('Qwen/Qwen2.5-7B-Instruct')
assert qwen.supports_reasoning_mode() == True

print('Adapters support reasoning mode')
"
```
  </verify>
  <done>
LlamaAdapter and QwenAdapter implement supports_reasoning_mode() returning True and delegate extract_reasoning() to ReasoningExtractor.
  </done>
</task>

<task type="auto">
  <name>Task 3: Extend OpenAI Schemas for Reasoning Content</name>
  <files>backend/mlx_manager/mlx_server/schemas/openai.py</files>
  <action>
Add reasoning_content field to response schemas:

1. Update ChatMessage model:
   - Add reasoning_content: str | None = None
   - This holds the extracted thinking/reasoning content

2. Update ChatCompletionChunkDelta model:
   - Add reasoning_content: str | None = None
   - For streaming, reasoning can be sent incrementally

3. Document the field:
   - Add docstring explaining reasoning_content is populated when model outputs thinking tags
   - Note that it's None for non-reasoning models

This follows the pattern used by Anthropic's Claude API for "thinking" content.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.schemas.openai import ChatMessage, ChatCompletionChunkDelta

# Test ChatMessage with reasoning
msg = ChatMessage(role='assistant', content='Answer', reasoning_content='Thinking process')
assert msg.reasoning_content == 'Thinking process'

# Test without reasoning
msg2 = ChatMessage(role='assistant', content='Answer')
assert msg2.reasoning_content is None

# Test streaming delta
delta = ChatCompletionChunkDelta(content='test', reasoning_content='thinking')
assert delta.reasoning_content == 'thinking'

print('Reasoning content fields work correctly')
"
```
  </verify>
  <done>
ChatMessage and ChatCompletionChunkDelta have reasoning_content field for chain-of-thought content.
  </done>
</task>

</tasks>

<verification>
- [ ] ReasoningExtractor service exists in services/reasoning.py
- [ ] Extracts content from `<think>`, `<thinking>`, `<reasoning>`, `<reflection>` tags
- [ ] Returns (None, text) when no tags present
- [ ] LlamaAdapter.supports_reasoning_mode() returns True
- [ ] QwenAdapter.supports_reasoning_mode() returns True
- [ ] ChatMessage has reasoning_content field
- [ ] ChatCompletionChunkDelta has reasoning_content field
- [ ] Quality gates pass (ruff, mypy)
</verification>

<success_criteria>
1. ReasoningExtractor handles all four tag patterns
2. Llama and Qwen adapters support reasoning mode
3. OpenAI schemas include reasoning_content field
4. No reasoning tags leak into final content
5. All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-model-adapter-enhancements/14-03-SUMMARY.md`
</output>
