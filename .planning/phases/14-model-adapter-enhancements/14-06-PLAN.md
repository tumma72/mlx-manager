---
phase: 14-model-adapter-enhancements
plan: 06
type: execute
wave: 3
depends_on: ["14-02", "14-03", "14-04", "14-05"]
files_modified:
  - backend/mlx_manager/mlx_server/services/inference.py
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/tests/mlx_server/test_tool_calling.py
  - backend/tests/mlx_server/test_reasoning.py
  - backend/tests/mlx_server/test_structured_output.py
autonomous: true

must_haves:
  truths:
    - "Chat completions endpoint accepts tools parameter"
    - "Tool calls are parsed and returned in response"
    - "Reasoning content is extracted and returned separately"
    - "Structured output is validated against provided schema"
    - "Finish reason is 'tool_calls' when tool calls detected"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      provides: "Extended chat endpoint with tool/reasoning/structured output support"
      contains: "tools"
    - path: "backend/mlx_manager/mlx_server/services/inference.py"
      provides: "Extended inference with tool call parsing and reasoning extraction"
      contains: "parse_tool_calls"
    - path: "backend/tests/mlx_server/test_tool_calling.py"
      provides: "Tool calling tests"
      contains: "test_parse_tool_calls"
  key_links:
    - from: "api/v1/chat.py"
      to: "services/inference.py"
      via: "generate_chat_completion call"
      pattern: "generate_chat_completion"
    - from: "services/inference.py"
      to: "adapters"
      via: "parse_tool_calls"
      pattern: "adapter\\.parse_tool_calls"
---

<objective>
Integrate tool calling, reasoning extraction, and structured output validation into the chat completions endpoint.

Purpose: Wire all the new adapter capabilities into the actual API endpoint so users can use tool calling, see reasoning content, and get validated structured output.

Output: Extended chat endpoint handling tools, reasoning, and structured output. Test suite for new features.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-model-adapter-enhancements/14-RESEARCH.md
@backend/mlx_manager/mlx_server/services/inference.py
@backend/mlx_manager/mlx_server/api/v1/chat.py
@backend/mlx_manager/mlx_server/schemas/openai.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend Inference Service for Tool Calls and Reasoning</name>
  <files>backend/mlx_manager/mlx_server/services/inference.py</files>
  <action>
Extend generate_chat_completion to handle tools and reasoning:

1. Add tools parameter to generate_chat_completion():
   - tools: list[dict] | None = None
   - When tools provided and adapter supports tool calling:
     - Inject tool definitions into prompt via adapter.format_tools_for_prompt()
     - Add tool call stop tokens via adapter.get_tool_call_stop_tokens()

2. Add response post-processing:
   - After generation completes, check if adapter supports tool calling
   - If so, call adapter.parse_tool_calls(response_text)
   - If tool calls found, set finish_reason="tool_calls"

3. Add reasoning extraction:
   - After generation, check if adapter supports reasoning mode
   - Call adapter.extract_reasoning(response_text)
   - Include reasoning_content in response if present
   - Final content is cleaned (reasoning tags removed)

4. Update response dict to include:
   - tool_calls: list[dict] | None in message
   - reasoning_content: str | None in message
   - finish_reason: "tool_calls" when tool calls present

5. For streaming (_stream_chat_generate):
   - Buffer output to detect tool calls at end
   - For reasoning, stream reasoning_content separately or accumulate
   - Final chunk includes tool_calls if detected

Note: Tool call detection happens AFTER full response is generated. Streaming tool calls requires buffering until stop token.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
import inspect
from mlx_manager.mlx_server.services.inference import generate_chat_completion

# Check tools parameter exists
sig = inspect.signature(generate_chat_completion)
assert 'tools' in sig.parameters
print('tools parameter exists in generate_chat_completion')
"
```
  </verify>
  <done>
Inference service accepts tools parameter, injects tool definitions into prompt, parses tool calls from output, and extracts reasoning content.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend Chat Endpoint for Tools and Structured Output</name>
  <files>backend/mlx_manager/mlx_server/api/v1/chat.py</files>
  <action>
Update chat completions endpoint to handle new features:

1. Extract tools and response_format from request:
   - Pass tools to inference service when provided
   - tool_choice controls whether tools are used (auto/none/required)

2. Handle structured output validation:
   - When response_format.type == "json_schema", validate output
   - Import StructuredOutputValidator
   - After generation (non-streaming), validate against schema
   - If validation fails, return error response (400)

3. Update response building:
   - Include tool_calls in ChatMessage if present
   - Include reasoning_content in ChatMessage if present
   - Set finish_reason based on generation result

4. For streaming responses:
   - Stream content normally
   - Include reasoning_content in delta if present
   - Final chunk includes tool_calls if detected

5. Handle tool_choice:
   - "none": Don't use tools even if provided
   - "auto": Let model decide (default)
   - "required": Model must use a tool

Note: Structured output validation only applies to non-streaming responses. For streaming with json_schema, validate accumulated output in final chunk.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.schemas.openai import ChatCompletionRequest, Tool, FunctionDefinition, ResponseFormat

# Test request with tools
req = ChatCompletionRequest(
    model='test',
    messages=[{'role': 'user', 'content': 'hello'}],
    tools=[Tool(function=FunctionDefinition(name='test', description='test func'))]
)
assert len(req.tools) == 1

# Test request with response_format
req2 = ChatCompletionRequest(
    model='test',
    messages=[{'role': 'user', 'content': 'hello'}],
    response_format=ResponseFormat(type='json_schema', json_schema={'type': 'object'})
)
assert req2.response_format.type == 'json_schema'

print('Request schemas support tools and response_format')
"
```
  </verify>
  <done>
Chat endpoint passes tools to inference, validates structured output, and builds responses with tool_calls and reasoning_content.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add Tests for New Features</name>
  <files>
backend/tests/mlx_server/test_tool_calling.py
backend/tests/mlx_server/test_reasoning.py
backend/tests/mlx_server/test_structured_output.py
  </files>
  <action>
Create test files for the new features:

1. test_tool_calling.py:
   - Test LlamaToolParser.parse() with valid tool calls
   - Test QwenToolParser.parse() with Hermes format
   - Test GLM4ToolParser.parse() with XML format
   - Test adapter.supports_tool_calling() returns correct values
   - Test format_tools_for_prompt() generates valid prompt text
   - Test deduplication in GLM4 parser

2. test_reasoning.py:
   - Test ReasoningExtractor with <think> tags
   - Test ReasoningExtractor with <thinking> tags
   - Test ReasoningExtractor with multiple tags
   - Test extraction returns (None, text) when no tags
   - Test adapter.extract_reasoning() delegation

3. test_structured_output.py:
   - Test StructuredOutputValidator.validate() with valid JSON
   - Test validation failure with missing required field
   - Test validation failure with wrong type
   - Test extract_json() with mixed content
   - Test validate_and_coerce() type coercion

Use pytest and mock where needed (don't require actual models).
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_tool_calling.py tests/mlx_server/test_reasoning.py tests/mlx_server/test_structured_output.py -v --tb=short
```
  </verify>
  <done>
Test files created with comprehensive coverage for tool calling, reasoning extraction, and structured output validation. All tests pass.
  </done>
</task>

</tasks>

<verification>
- [ ] generate_chat_completion accepts tools parameter
- [ ] Tool definitions injected into prompt when tools provided
- [ ] Tool calls parsed from model output
- [ ] Reasoning content extracted and included in response
- [ ] Structured output validated against JSON schema
- [ ] finish_reason="tool_calls" when tool calls present
- [ ] Tests pass for tool calling, reasoning, and structured output
- [ ] Quality gates pass (ruff, mypy)
</verification>

<success_criteria>
1. Chat endpoint accepts tools, tool_choice, response_format
2. Tools are formatted and injected into prompts
3. Tool calls are parsed and returned in responses
4. Reasoning content is extracted and returned
5. Structured output is validated against schema
6. Comprehensive tests pass
7. All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-model-adapter-enhancements/14-06-SUMMARY.md`
</output>
