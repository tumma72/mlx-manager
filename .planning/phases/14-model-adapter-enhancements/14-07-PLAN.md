---
phase: 14-model-adapter-enhancements
plan: 07
type: execute
wave: 4
depends_on: ["14-06"]
files_modified:
  - backend/mlx_manager/mlx_server/services/response_processor.py
  - backend/mlx_manager/mlx_server/services/inference.py
  - backend/mlx_manager/mlx_server/models/adapters/parsers/llama.py
  - backend/mlx_manager/mlx_server/models/adapters/parsers/glm4.py
  - backend/tests/mlx_server/test_response_processor.py
autonomous: true

must_haves:
  truths:
    - "ResponseProcessor extracts tool calls, reasoning, and cleans content in single pass"
    - "All known tool call patterns handled (Hermes, Llama XML, GLM4)"
    - "All thinking tags handled (think, thinking, reasoning, reflection)"
    - "Tool call markers removed from final content"
    - "Pydantic models used for ParseResult and ToolCall"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/response_processor.py"
      provides: "Unified callback-based response processor"
      contains: "ResponseProcessor"
    - path: "backend/tests/mlx_server/test_response_processor.py"
      provides: "Comprehensive processor tests"
      contains: "test_"
---

<objective>
Create a unified ResponseProcessor that replaces per-adapter parsing with callback-based single-pass extraction.

Purpose: Fix the bug where tool call markers remain in content, and improve performance by extracting tool calls, reasoning, and cleaning content in one scan instead of multiple passes.

Output: ResponseProcessor class with registered pattern handlers, Pydantic result models, integration into inference service.
</objective>

<context>
Current issues:
1. Tool calls parsed but markers left in content (clean_response never called)
2. Two parsing passes: tool calls, then reasoning
3. LlamaToolParser missing clean_response override

Solution: Unified processor that:
1. Registers patterns with handlers at startup
2. Single scan finds all matches
3. Handlers return structured data
4. Matched spans removed from content
5. Returns Pydantic ParseResult model
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ResponseProcessor with Pydantic Models</name>
  <files>backend/mlx_manager/mlx_server/services/response_processor.py</files>
  <action>
Create new file with:

1. Pydantic models:
```python
from pydantic import BaseModel, Field

class ToolCallFunction(BaseModel):
    name: str
    arguments: str

class ToolCall(BaseModel):
    id: str
    type: str = "function"
    function: ToolCallFunction

class ParseResult(BaseModel):
    content: str
    tool_calls: list[ToolCall] = Field(default_factory=list)
    reasoning: str | None = None
```

2. ResponseProcessor class:
```python
class ResponseProcessor:
    """Single-pass response processor with registered handlers."""

    def __init__(self):
        self._thinking_patterns: list[re.Pattern] = []
        self._tool_patterns: list[tuple[re.Pattern, Callable]] = []
        self._cleanup_patterns: list[re.Pattern] = []

    def register_thinking_tags(self, tags: list[str]):
        """Register thinking tag names (without brackets)."""

    def register_tool_pattern(self, pattern: str, parser: Callable):
        """Register tool pattern with parser callback."""

    def register_cleanup_patterns(self, patterns: list[str]):
        """Register special tokens to remove."""

    def process(self, text: str) -> ParseResult:
        """Single-pass extraction and cleaning."""
```

3. Default processor factory:
```python
def create_default_processor() -> ResponseProcessor:
    """Create processor with all standard handlers registered."""
    processor = ResponseProcessor()

    # Thinking tags
    processor.register_thinking_tags(["think", "thinking", "reasoning", "reflection"])

    # Tool patterns with parsers
    processor.register_tool_pattern(
        r"<tool_call>\s*(.*?)\s*</tool_call>",  # Hermes/Qwen
        parse_hermes_tool
    )
    processor.register_tool_pattern(
        r"<function=(\w+)>(.*?)</function>",   # Llama
        parse_llama_tool
    )
    processor.register_tool_pattern(
        r"<tool_call>\s*<name>(.*?)</name>\s*<arguments>(.*?)</arguments>\s*</tool_call>",  # GLM4 XML
        parse_glm4_tool
    )

    # Cleanup patterns
    processor.register_cleanup_patterns([
        "<|endoftext|>", "<|im_end|>", "<|im_start|>",
        "<|eot_id|>", "<|end|>", "<|start_header_id|>",
        "<|end_header_id|>",
    ])

    return processor

# Singleton
_processor: ResponseProcessor | None = None

def get_response_processor() -> ResponseProcessor:
    global _processor
    if _processor is None:
        _processor = create_default_processor()
    return _processor
```

4. Parser callback functions:
```python
def parse_hermes_tool(match: re.Match) -> ToolCall | None:
    """Parse Qwen/Hermes: <tool_call>{"name": ..., "arguments": ...}</tool_call>"""

def parse_llama_tool(match: re.Match) -> ToolCall | None:
    """Parse Llama: <function=name>{...}</function>"""

def parse_glm4_tool(match: re.Match) -> ToolCall | None:
    """Parse GLM4: <tool_call><name>...</name><arguments>...</arguments></tool_call>"""
```

Key implementation details:
- process() collects all matches with their spans
- Removes spans in reverse order (preserves indices)
- Merges overlapping spans
- Deduplicates tool calls by (name, arguments) hash for GLM4 bug
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.services.response_processor import (
    get_response_processor, ParseResult, ToolCall
)

processor = get_response_processor()

# Test thinking extraction
text = '<think>Let me analyze</think>The answer is 42.'
result = processor.process(text)
assert result.reasoning == 'Let me analyze'
assert result.content == 'The answer is 42.'
assert len(result.tool_calls) == 0
print('Thinking extraction: OK')

# Test tool call extraction (Hermes)
text2 = 'Here is the result: <tool_call>{\"name\": \"get_weather\", \"arguments\": {\"city\": \"SF\"}}</tool_call>'
result2 = processor.process(text2)
assert len(result2.tool_calls) == 1
assert result2.tool_calls[0].function.name == 'get_weather'
assert 'tool_call' not in result2.content  # Marker removed!
print('Tool call extraction (Hermes): OK')

# Test Llama format
text3 = 'Result: <function=search>{\"query\": \"test\"}</function>'
result3 = processor.process(text3)
assert len(result3.tool_calls) == 1
assert result3.tool_calls[0].function.name == 'search'
assert 'function=' not in result3.content
print('Tool call extraction (Llama): OK')

print('All ResponseProcessor tests passed!')
"
```
  </verify>
  <done>
ResponseProcessor created with Pydantic models, callback-based pattern handlers, and singleton accessor.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update Inference Service to Use ResponseProcessor</name>
  <files>backend/mlx_manager/mlx_server/services/inference.py</files>
  <action>
Replace multi-pass adapter-based parsing with ResponseProcessor:

1. Import ResponseProcessor:
```python
from mlx_manager.mlx_server.services.response_processor import (
    get_response_processor, ParseResult
)
```

2. Update _generate_chat_complete() (non-streaming):
```python
# OLD (multiple passes):
# tool_calls = adapter.parse_tool_calls(response_text)
# reasoning_content, final_content = adapter.extract_reasoning(response_text)

# NEW (single pass):
processor = get_response_processor()
result = processor.process(response_text)

tool_calls = [tc.model_dump() for tc in result.tool_calls] if result.tool_calls else None
reasoning_content = result.reasoning
final_content = result.content

if tool_calls:
    finish_reason = "tool_calls"
```

3. Update _stream_chat_generate() (streaming):
   - After accumulating text, use processor.process()
   - Tool calls and reasoning extracted from accumulated buffer
   - Clean content used for final response metadata

4. Remove direct calls to:
   - adapter.parse_tool_calls()
   - adapter.extract_reasoning()

   Keep adapter for:
   - apply_chat_template()
   - get_stop_tokens()
   - format_tools_for_prompt() (still needed for prompt injection)
   - get_tool_call_stop_tokens()
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
import ast
import inspect
from mlx_manager.mlx_server.services import inference

# Check ResponseProcessor is imported
source = inspect.getsource(inference)
assert 'get_response_processor' in source or 'ResponseProcessor' in source
print('ResponseProcessor imported: OK')

# Check old methods not called
assert 'adapter.parse_tool_calls' not in source or source.count('adapter.parse_tool_calls') == 0
print('Old parsing methods removed: OK')
"
```
  </verify>
  <done>
Inference service now uses ResponseProcessor for single-pass extraction. Tool call markers properly removed from content.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add Comprehensive Tests</name>
  <files>backend/tests/mlx_server/test_response_processor.py</files>
  <action>
Create comprehensive test file:

1. Test ParseResult model:
   - Serialization to dict
   - Default values
   - Tool call list handling

2. Test thinking extraction:
   - Single <think> tag
   - Multiple thinking tags
   - Mixed tag types (<think> and <reasoning>)
   - No tags (returns None)
   - Nested content preserved

3. Test tool call extraction:
   - Hermes/Qwen format
   - Llama XML format
   - GLM4 nested XML format
   - Multiple tool calls
   - Invalid JSON handling
   - Deduplication (GLM4 bug)

4. Test content cleaning:
   - Special tokens removed
   - Whitespace normalized
   - No double/triple newlines

5. Test combined extraction:
   - Both thinking and tool calls in same response
   - Order doesn't matter
   - All markers removed from content

6. Test edge cases:
   - Empty string
   - No patterns match
   - Malformed tags (unclosed)
   - Unicode content
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_response_processor.py -v --tb=short
```
  </verify>
  <done>
Comprehensive tests for ResponseProcessor covering all patterns, edge cases, and combined extraction scenarios.
  </done>
</task>

<task type="human">
  <name>Task 4: Verification Checkpoint</name>
  <action>
Verify the fix works end-to-end:

1. Run full test suite:
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest -v --tb=short
```

2. Run quality checks:
```bash
ruff check . && ruff format --check . && mypy mlx_manager
```

3. Manual verification (if server running):
   - Send chat request with tool-using model
   - Confirm tool call markers NOT in content
   - Confirm tool_calls field populated correctly
  </action>
</task>

</tasks>

<verification>
- [ ] ResponseProcessor class created with Pydantic models
- [ ] All thinking patterns registered (think, thinking, reasoning, reflection)
- [ ] All tool patterns registered (Hermes, Llama, GLM4)
- [ ] Single-pass extraction works correctly
- [ ] Tool call markers removed from content (THE BUG FIX)
- [ ] Inference service uses ResponseProcessor
- [ ] Streaming path updated
- [ ] Tests comprehensive and passing
- [ ] Quality gates pass
</verification>

<success_criteria>
1. Tool call markers no longer appear in response content
2. Reasoning content properly extracted
3. Single-pass parsing (not multi-pass)
4. Pydantic models for type safety
5. All existing tests still pass
6. New processor tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-model-adapter-enhancements/14-07-SUMMARY.md`
</output>
