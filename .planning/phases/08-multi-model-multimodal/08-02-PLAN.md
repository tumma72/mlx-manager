---
phase: 08-multi-model-multimodal
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/mlx_server/models/adapters/qwen.py
  - backend/mlx_manager/mlx_server/models/adapters/mistral.py
  - backend/mlx_manager/mlx_server/models/adapters/gemma.py
  - backend/mlx_manager/mlx_server/models/adapters/registry.py
  - backend/tests/mlx_server/test_adapters.py
autonomous: true

must_haves:
  truths:
    - "Qwen models use <|im_end|> stop token (ChatML format)"
    - "Mistral models use </s> stop token and handle system messages"
    - "Gemma models use <end_of_turn> stop token"
    - "All adapters registered in registry and auto-selected by model ID"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/adapters/qwen.py"
      provides: "QwenAdapter with ChatML stop tokens"
      exports: ["QwenAdapter"]
    - path: "backend/mlx_manager/mlx_server/models/adapters/mistral.py"
      provides: "MistralAdapter with system message handling"
      exports: ["MistralAdapter"]
    - path: "backend/mlx_manager/mlx_server/models/adapters/gemma.py"
      provides: "GemmaAdapter with end_of_turn stop token"
      exports: ["GemmaAdapter"]
  key_links:
    - from: "registry.py"
      to: "qwen.py, mistral.py, gemma.py"
      via: "import and register adapters"
      pattern: "_ADAPTERS.*qwen.*QwenAdapter"
---

<objective>
Create Qwen, Mistral, and Gemma model family adapters

Purpose: Enable proper chat template formatting and stop token handling for Qwen, Mistral, and Gemma model families, preventing runaway generation and ensuring correct output.

Output: Three new adapter files and updated registry with auto-detection
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-multi-model-multimodal/08-RESEARCH.md
@backend/mlx_manager/mlx_server/models/adapters/base.py
@backend/mlx_manager/mlx_server/models/adapters/llama.py
@backend/mlx_manager/mlx_server/models/adapters/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create QwenAdapter for Qwen/Qwen2/Qwen2.5 models</name>
  <files>backend/mlx_manager/mlx_server/models/adapters/qwen.py</files>
  <action>
Create qwen.py following the exact pattern of llama.py:

```python
"""Qwen model family adapter (Qwen, Qwen2, Qwen2.5, Qwen3).

Qwen models use ChatML format with <|im_start|> and <|im_end|> tokens.
"""

from typing import Any, cast

class QwenAdapter:
    """Adapter for Qwen model family."""

    @property
    def family(self) -> str:
        return "qwen"

    def apply_chat_template(
        self,
        tokenizer: Any,
        messages: list[dict[str, Any]],
        add_generation_prompt: bool = True,
    ) -> str:
        """Apply Qwen chat template using tokenizer's built-in template."""
        # Qwen tokenizers have proper chat templates - use them
        result: str = cast(
            str,
            tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=add_generation_prompt,
                tokenize=False,
            ),
        )
        return result

    def get_stop_tokens(self, tokenizer: Any) -> list[int]:
        """Get Qwen stop tokens.

        Qwen uses ChatML format with <|im_end|> as end-of-turn marker.
        Must include both eos_token_id and <|im_end|> to prevent runaway generation.
        """
        stop_tokens = [tokenizer.eos_token_id]

        # Add <|im_end|> token (ChatML end of turn)
        try:
            im_end_id = tokenizer.convert_tokens_to_ids("<|im_end|>")
            # Check it's a valid token (not None or unk)
            if im_end_id is not None and im_end_id != tokenizer.unk_token_id:
                stop_tokens.append(im_end_id)
        except Exception:
            pass

        return stop_tokens
```

Note: Qwen2.5-VL vision models need special handling in the vision inference service (not here) using mlx_vlm.prompt_utils.apply_chat_template(). This adapter is for text-only Qwen models.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.adapters.qwen import QwenAdapter
adapter = QwenAdapter()
print('family:', adapter.family)
"
```
Should output: family: qwen
  </verify>
  <done>QwenAdapter created with ChatML stop token handling</done>
</task>

<task type="auto">
  <name>Task 2: Create MistralAdapter for Mistral/Mixtral models</name>
  <files>backend/mlx_manager/mlx_server/models/adapters/mistral.py</files>
  <action>
Create mistral.py:

```python
"""Mistral model family adapter (Mistral, Mixtral).

Mistral v1/v2 uses [INST] ... [/INST] format without native system message support.
Mistral v3+ has native system message support in tokenizer template.

This adapter prepends system message to first user message for v1/v2 compatibility.
"""

from typing import Any, cast


class MistralAdapter:
    """Adapter for Mistral model family."""

    @property
    def family(self) -> str:
        return "mistral"

    def apply_chat_template(
        self,
        tokenizer: Any,
        messages: list[dict[str, Any]],
        add_generation_prompt: bool = True,
    ) -> str:
        """Apply Mistral chat template with system message handling.

        For Mistral v1/v2 models that don't support system role natively,
        prepend system message content to the first user message.
        Mistral v3+ tokenizers handle system messages correctly.
        """
        processed = list(messages)

        # Handle system message for older Mistral versions
        if processed and processed[0].get("role") == "system":
            system_content = processed[0].get("content", "")
            processed = processed[1:]

            # Prepend to first user message if exists
            if processed and processed[0].get("role") == "user":
                user_content = processed[0].get("content", "")
                processed[0] = {
                    "role": "user",
                    "content": f"{system_content}\n\n{user_content}",
                }

        # Use tokenizer's built-in template (works for v3+, fallback for v1/v2)
        result: str = cast(
            str,
            tokenizer.apply_chat_template(
                processed,
                add_generation_prompt=add_generation_prompt,
                tokenize=False,
            ),
        )
        return result

    def get_stop_tokens(self, tokenizer: Any) -> list[int]:
        """Get Mistral stop tokens.

        Mistral uses </s> as the end-of-sequence token.
        """
        return [tokenizer.eos_token_id]
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.adapters.mistral import MistralAdapter
adapter = MistralAdapter()
print('family:', adapter.family)
# Test system message handling
messages = [
    {'role': 'system', 'content': 'You are helpful.'},
    {'role': 'user', 'content': 'Hello'}
]
# Can't fully test without tokenizer, but verify structure
print('has apply_chat_template:', hasattr(adapter, 'apply_chat_template'))
"
```
Should output: family: mistral, has apply_chat_template: True
  </verify>
  <done>MistralAdapter created with system message prepending for v1/v2 compatibility</done>
</task>

<task type="auto">
  <name>Task 3: Create GemmaAdapter and register all new adapters</name>
  <files>
backend/mlx_manager/mlx_server/models/adapters/gemma.py
backend/mlx_manager/mlx_server/models/adapters/registry.py
  </files>
  <action>
**Part A: Create gemma.py:**

```python
"""Gemma model family adapter (Gemma, Gemma2, Gemma3).

Gemma models use <start_of_turn> and <end_of_turn> tokens for chat formatting.
"""

from typing import Any, cast


class GemmaAdapter:
    """Adapter for Gemma model family."""

    @property
    def family(self) -> str:
        return "gemma"

    def apply_chat_template(
        self,
        tokenizer: Any,
        messages: list[dict[str, Any]],
        add_generation_prompt: bool = True,
    ) -> str:
        """Apply Gemma chat template using tokenizer's built-in template."""
        result: str = cast(
            str,
            tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=add_generation_prompt,
                tokenize=False,
            ),
        )
        return result

    def get_stop_tokens(self, tokenizer: Any) -> list[int]:
        """Get Gemma stop tokens.

        Gemma uses <end_of_turn> as the end-of-turn marker.
        Must include both eos_token_id and <end_of_turn> to prevent runaway generation.
        """
        stop_tokens = [tokenizer.eos_token_id]

        # Add <end_of_turn> token
        try:
            end_turn_id = tokenizer.convert_tokens_to_ids("<end_of_turn>")
            if end_turn_id is not None and end_turn_id != tokenizer.unk_token_id:
                stop_tokens.append(end_turn_id)
        except Exception:
            pass

        return stop_tokens
```

**Part B: Update registry.py:**

Add imports at top of file:
```python
from mlx_manager.mlx_server.models.adapters.qwen import QwenAdapter
from mlx_manager.mlx_server.models.adapters.mistral import MistralAdapter
from mlx_manager.mlx_server.models.adapters.gemma import GemmaAdapter
```

Update _ADAPTERS dict to include new adapters:
```python
_ADAPTERS: dict[str, ModelAdapter] = {
    "llama": LlamaAdapter(),
    "qwen": QwenAdapter(),
    "mistral": MistralAdapter(),
    "gemma": GemmaAdapter(),
    "default": DefaultAdapter(),
}
```

The detect_model_family() function already has the detection patterns for qwen, mistral, and gemma - no changes needed there.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.adapters import get_adapter, get_supported_families
print('Supported families:', get_supported_families())

# Test detection
qwen = get_adapter('mlx-community/Qwen2.5-7B-Instruct-4bit')
mistral = get_adapter('mlx-community/Mistral-7B-Instruct-v0.3-4bit')
gemma = get_adapter('mlx-community/gemma-2-9b-it-4bit')
print('Qwen adapter:', qwen.family)
print('Mistral adapter:', mistral.family)
print('Gemma adapter:', gemma.family)
"
```
Should show all families including qwen, mistral, gemma and correct adapter selection
  </verify>
  <done>
- GemmaAdapter created with <end_of_turn> stop token
- All three adapters (Qwen, Mistral, Gemma) registered in registry
- Auto-detection works for all model families
  </done>
</task>

<task type="auto">
  <name>Task 4: Add unit tests for new adapters</name>
  <files>backend/tests/mlx_server/test_adapters.py</files>
  <action>
Create or extend test_adapters.py with tests for the new adapters:

```python
"""Tests for model family adapters."""

import pytest
from unittest.mock import MagicMock

from mlx_manager.mlx_server.models.adapters import get_adapter, get_supported_families
from mlx_manager.mlx_server.models.adapters.qwen import QwenAdapter
from mlx_manager.mlx_server.models.adapters.mistral import MistralAdapter
from mlx_manager.mlx_server.models.adapters.gemma import GemmaAdapter


class TestAdapterRegistry:
    """Tests for adapter registry."""

    def test_supported_families_includes_all(self):
        """Verify all expected families are registered."""
        families = get_supported_families()
        assert "llama" in families
        assert "qwen" in families
        assert "mistral" in families
        assert "gemma" in families
        assert "default" in families

    def test_get_adapter_qwen(self):
        """Verify Qwen models get QwenAdapter."""
        adapter = get_adapter("mlx-community/Qwen2.5-7B-Instruct-4bit")
        assert adapter.family == "qwen"

    def test_get_adapter_mistral(self):
        """Verify Mistral models get MistralAdapter."""
        adapter = get_adapter("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
        assert adapter.family == "mistral"

    def test_get_adapter_mixtral(self):
        """Verify Mixtral models also get MistralAdapter."""
        adapter = get_adapter("mlx-community/Mixtral-8x7B-Instruct-4bit")
        assert adapter.family == "mistral"

    def test_get_adapter_gemma(self):
        """Verify Gemma models get GemmaAdapter."""
        adapter = get_adapter("mlx-community/gemma-2-9b-it-4bit")
        assert adapter.family == "gemma"


class TestQwenAdapter:
    """Tests for QwenAdapter."""

    def test_family(self):
        adapter = QwenAdapter()
        assert adapter.family == "qwen"

    def test_get_stop_tokens_includes_im_end(self):
        """Verify <|im_end|> is included in stop tokens."""
        adapter = QwenAdapter()
        tokenizer = MagicMock()
        tokenizer.eos_token_id = 100
        tokenizer.unk_token_id = 0
        tokenizer.convert_tokens_to_ids.return_value = 200  # <|im_end|>

        stop_tokens = adapter.get_stop_tokens(tokenizer)

        assert 100 in stop_tokens  # eos
        assert 200 in stop_tokens  # <|im_end|>
        tokenizer.convert_tokens_to_ids.assert_called_with("<|im_end|>")


class TestMistralAdapter:
    """Tests for MistralAdapter."""

    def test_family(self):
        adapter = MistralAdapter()
        assert adapter.family == "mistral"

    def test_apply_chat_template_prepends_system_message(self):
        """Verify system message is prepended to first user message."""
        adapter = MistralAdapter()
        tokenizer = MagicMock()
        tokenizer.apply_chat_template.return_value = "formatted"

        messages = [
            {"role": "system", "content": "You are helpful."},
            {"role": "user", "content": "Hello"},
        ]

        adapter.apply_chat_template(tokenizer, messages)

        # Check that system was merged into user message
        call_args = tokenizer.apply_chat_template.call_args
        processed_messages = call_args[0][0]
        assert len(processed_messages) == 1
        assert processed_messages[0]["role"] == "user"
        assert "You are helpful." in processed_messages[0]["content"]
        assert "Hello" in processed_messages[0]["content"]

    def test_get_stop_tokens(self):
        """Verify only eos_token_id is returned."""
        adapter = MistralAdapter()
        tokenizer = MagicMock()
        tokenizer.eos_token_id = 100

        stop_tokens = adapter.get_stop_tokens(tokenizer)

        assert stop_tokens == [100]


class TestGemmaAdapter:
    """Tests for GemmaAdapter."""

    def test_family(self):
        adapter = GemmaAdapter()
        assert adapter.family == "gemma"

    def test_get_stop_tokens_includes_end_of_turn(self):
        """Verify <end_of_turn> is included in stop tokens."""
        adapter = GemmaAdapter()
        tokenizer = MagicMock()
        tokenizer.eos_token_id = 100
        tokenizer.unk_token_id = 0
        tokenizer.convert_tokens_to_ids.return_value = 300  # <end_of_turn>

        stop_tokens = adapter.get_stop_tokens(tokenizer)

        assert 100 in stop_tokens  # eos
        assert 300 in stop_tokens  # <end_of_turn>
        tokenizer.convert_tokens_to_ids.assert_called_with("<end_of_turn>")
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_adapters.py -v
```
All tests should pass
  </verify>
  <done>
- Tests verify all adapters are registered
- Tests verify correct adapter selection by model ID
- Tests verify Qwen <|im_end|> stop token
- Tests verify Mistral system message prepending
- Tests verify Gemma <end_of_turn> stop token
  </done>
</task>

</tasks>

<verification>
After all tasks:
```bash
cd /Users/atomasini/Development/mlx-manager/backend
# Type check
mypy mlx_manager/mlx_server/models/adapters/
# Lint
ruff check mlx_manager/mlx_server/models/adapters/
# Tests
pytest tests/mlx_server/test_adapters.py -v
```
All checks should pass.
</verification>

<success_criteria>
- QwenAdapter handles ChatML format with <|im_end|> stop token
- MistralAdapter prepends system message to first user message for v1/v2 compatibility
- GemmaAdapter handles <end_of_turn> stop token
- All adapters registered in registry
- Auto-detection selects correct adapter based on model ID
- Unit tests pass for all adapters
</success_criteria>

<output>
After completion, create `.planning/phases/08-multi-model-multimodal/08-02-SUMMARY.md`
</output>
