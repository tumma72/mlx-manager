---
phase: 08-multi-model-multimodal
plan: 03
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - backend/mlx_manager/mlx_server/models/detection.py
  - backend/mlx_manager/mlx_server/models/pool.py
  - backend/mlx_manager/mlx_server/services/image_processor.py
  - backend/mlx_manager/mlx_server/schemas/openai.py
  - backend/tests/mlx_server/test_detection.py
  - backend/tests/mlx_server/test_image_processor.py
autonomous: true

must_haves:
  truths:
    - "Model type (text-gen/vision/embeddings) detected from config.json or name patterns"
    - "Vision models load via mlx-vlm with (model, processor) tuple"
    - "Images accepted as base64 data URIs and HTTP URLs"
    - "Large images auto-resized to max dimension with warning"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/detection.py"
      provides: "detect_model_type() function"
      exports: ["detect_model_type"]
    - path: "backend/mlx_manager/mlx_server/services/image_processor.py"
      provides: "Image preprocessing service"
      exports: ["preprocess_image", "preprocess_images"]
  key_links:
    - from: "pool.py"
      to: "detection.py"
      via: "detect_model_type call in _load_model"
      pattern: "detect_model_type"
    - from: "pool.py"
      to: "mlx_vlm"
      via: "conditional import for vision models"
      pattern: "mlx_vlm.*load"
---

<objective>
Add model type detection and vision model infrastructure

Purpose: Enable the server to automatically detect model type (text-gen, vision, embeddings) and load vision models correctly using mlx-vlm, with image preprocessing for URLs and base64 data.

Output: Model type detection, vision model loading in pool, image preprocessing service
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-multi-model-multimodal/08-RESEARCH.md
@.planning/phases/08-multi-model-multimodal/08-CONTEXT.md
@backend/mlx_manager/mlx_server/models/pool.py
@backend/mlx_manager/mlx_server/models/types.py
@backend/mlx_manager/utils/model_detection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create model type detection module</name>
  <files>backend/mlx_manager/mlx_server/models/detection.py</files>
  <action>
Create detection.py that provides model TYPE detection (text-gen vs vision vs embeddings):

```python
"""Model type detection for MLX server.

This module determines the model TYPE (text-gen, vision, embeddings) to select
the correct loading strategy. It reuses config reading utilities from the
existing model_detection module but applies MLX server-specific logic.
"""

import logging
from typing import Any

from mlx_manager.mlx_server.models.types import ModelType

logger = logging.getLogger(__name__)


def detect_model_type(model_id: str, config: dict[str, Any] | None = None) -> ModelType:
    """Detect model type using the decision chain:
    1. config.json fields (most reliable)
    2. Model name patterns (fallback)
    3. Default to TEXT_GEN

    Args:
        model_id: HuggingFace model ID (e.g., "mlx-community/Qwen2-VL-2B-Instruct-4bit")
        config: Optional pre-loaded config.json dict (loads from cache if not provided)

    Returns:
        ModelType enum value
    """
    # Try to load config if not provided
    if config is None:
        try:
            from mlx_manager.utils.model_detection import read_model_config
            config = read_model_config(model_id)
        except Exception:
            config = None

    if config:
        # Vision: has vision_config or image_token_id
        if "vision_config" in config or "image_token_id" in config:
            logger.debug(f"Detected VISION model from config: {model_id}")
            return ModelType.VISION

        # Check model_type for vision indicators
        model_type = config.get("model_type", "").lower()
        if any(ind in model_type for ind in ("vl", "vision", "multimodal")):
            logger.debug(f"Detected VISION model from model_type: {model_id}")
            return ModelType.VISION

        # Embeddings: specific architectures
        arch_list = config.get("architectures", [])
        if arch_list:
            arch = arch_list[0].lower() if arch_list else ""
            embedding_indicators = ("embedding", "sentence", "bert", "roberta", "e5", "bge")
            if any(ind in arch for ind in embedding_indicators):
                logger.debug(f"Detected EMBEDDINGS model from architecture: {model_id}")
                return ModelType.EMBEDDINGS

        # Check model_type for embeddings indicators
        if any(ind in model_type for ind in ("embedding", "sentence", "bert")):
            logger.debug(f"Detected EMBEDDINGS model from model_type: {model_id}")
            return ModelType.EMBEDDINGS

    # Name-based fallback
    name_lower = model_id.lower()

    # Vision patterns
    vision_patterns = ("-vl", "vlm", "vision", "qwen2-vl", "qwen2.5-vl", "llava", "pixtral")
    if any(pattern in name_lower for pattern in vision_patterns):
        logger.debug(f"Detected VISION model from name pattern: {model_id}")
        return ModelType.VISION

    # Embeddings patterns
    embed_patterns = ("embed", "minilm", "sentence", "e5-", "bge-", "gte-")
    if any(pattern in name_lower for pattern in embed_patterns):
        logger.debug(f"Detected EMBEDDINGS model from name pattern: {model_id}")
        return ModelType.EMBEDDINGS

    # Default to text generation
    logger.debug(f"Defaulting to TEXT_GEN model: {model_id}")
    return ModelType.TEXT_GEN
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.detection import detect_model_type
from mlx_manager.mlx_server.models.types import ModelType

# Vision
assert detect_model_type('mlx-community/Qwen2-VL-2B-Instruct-4bit') == ModelType.VISION
assert detect_model_type('mlx-community/LLaVA-1.5-7B-4bit') == ModelType.VISION

# Embeddings
assert detect_model_type('mlx-community/all-MiniLM-L6-v2-4bit') == ModelType.EMBEDDINGS
assert detect_model_type('mlx-community/bge-small-en-v1.5-4bit') == ModelType.EMBEDDINGS

# Text-gen
assert detect_model_type('mlx-community/Llama-3.2-3B-Instruct-4bit') == ModelType.TEXT_GEN
assert detect_model_type('mlx-community/Qwen2.5-7B-Instruct-4bit') == ModelType.TEXT_GEN

print('All detection tests passed')
"
```
  </verify>
  <done>detect_model_type() correctly identifies vision, embeddings, and text-gen models</done>
</task>

<task type="auto">
  <name>Task 2: Update pool.py to load vision models via mlx-vlm</name>
  <files>backend/mlx_manager/mlx_server/models/pool.py</files>
  <action>
Modify the _load_model() method in pool.py to handle different model types:

1. **Add import at top:**
```python
from mlx_manager.mlx_server.models.detection import detect_model_type
from mlx_manager.mlx_server.models.types import ModelType
```

2. **Modify _load_model() to detect model type and use appropriate loader:**

Replace the current loading logic with:

```python
async def _load_model(self, model_id: str) -> LoadedModel:
    """Load a model from HuggingFace.

    Detects model type and uses appropriate loader:
    - TEXT_GEN: mlx_lm.load()
    - VISION: mlx_vlm.load()
    - EMBEDDINGS: mlx_embeddings.utils.load()
    """
    async with self._lock:
        # Double-check after acquiring lock
        if model_id in self._models:
            return self._models[model_id]

        # Mark as loading
        self._loading[model_id] = asyncio.Event()

    logger.info(f"Loading model: {model_id}")
    start_time = time.time()

    try:
        # Detect model type
        model_type = detect_model_type(model_id)
        logger.info(f"Detected model type: {model_type.value} for {model_id}")

        # Load based on type
        if model_type == ModelType.VISION:
            # Vision models use mlx-vlm (returns model, processor)
            from mlx_vlm import load as load_vlm

            result = await asyncio.to_thread(load_vlm, model_id)
            model, tokenizer = result[0], result[1]  # processor stored as tokenizer
        elif model_type == ModelType.EMBEDDINGS:
            # Embedding models use mlx-embeddings
            from mlx_embeddings.utils import load as load_embeddings

            result = await asyncio.to_thread(load_embeddings, model_id)
            model, tokenizer = result[0], result[1]
        else:
            # Text-gen models use mlx-lm
            from mlx_lm import load

            result = await asyncio.to_thread(load, model_id)
            model, tokenizer = result[0], result[1]

        # Get memory after loading
        from mlx_manager.mlx_server.utils.memory import get_memory_usage

        memory = get_memory_usage()

        loaded = LoadedModel(
            model_id=model_id,
            model=model,
            tokenizer=tokenizer,
            model_type=model_type.value,
            size_gb=memory["active_gb"],
        )

        async with self._lock:
            self._models[model_id] = loaded
            self._loading[model_id].set()
            del self._loading[model_id]

        elapsed = time.time() - start_time
        logger.info(
            f"Model loaded: {model_id} (type={model_type.value}, "
            f"{elapsed:.1f}s, {loaded.size_gb:.1f}GB)"
        )
        return loaded

    except Exception as e:
        async with self._lock:
            if model_id in self._loading:
                self._loading[model_id].set()
                del self._loading[model_id]
        logger.error(f"Failed to load model {model_id}: {e}")
        raise RuntimeError(f"Failed to load model: {e}") from e
```

IMPORTANT: The loaded.tokenizer field will contain:
- For TEXT_GEN: HuggingFace tokenizer
- For VISION: mlx-vlm processor (used differently in vision inference)
- For EMBEDDINGS: mlx-embeddings tokenizer
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.models.pool import ModelPoolManager
from mlx_manager.mlx_server.models.detection import detect_model_type
from mlx_manager.mlx_server.models.types import ModelType

# Just verify imports work and detection is wired up
print('ModelPoolManager imports detect_model_type:', 'detect_model_type' in dir())
print('Detection import works')

# Verify the code structure (don't actually load models)
import inspect
source = inspect.getsource(ModelPoolManager._load_model)
assert 'detect_model_type' in source, 'detect_model_type not called in _load_model'
assert 'ModelType.VISION' in source, 'VISION handling not in _load_model'
print('_load_model correctly handles model types')
"
```
  </verify>
  <done>pool.py _load_model() detects model type and uses mlx-vlm for vision, mlx-embeddings for embeddings</done>
</task>

<task type="auto">
  <name>Task 3: Create image preprocessing service</name>
  <files>backend/mlx_manager/mlx_server/services/image_processor.py</files>
  <action>
Create image_processor.py for fetching and preprocessing images:

```python
"""Image preprocessing service for vision models.

Handles:
- Base64 data URI decoding
- URL fetching with retry
- Image resizing to max dimension
- Batch image processing
"""

import asyncio
import base64
import logging
from io import BytesIO

import httpx
from PIL import Image

logger = logging.getLogger(__name__)

# Configuration
MAX_IMAGE_DIMENSION = 2048  # Max width or height in pixels
URL_FETCH_TIMEOUT = httpx.Timeout(connect=5.0, read=30.0, write=5.0, pool=5.0)
MAX_URL_RETRIES = 3


async def preprocess_image(
    image_input: str,
    client: httpx.AsyncClient | None = None,
    max_dimension: int = MAX_IMAGE_DIMENSION,
) -> Image.Image:
    """Fetch, decode, and resize an image.

    Args:
        image_input: One of:
            - Base64 data URI: "data:image/png;base64,<data>"
            - HTTP(S) URL: "https://example.com/image.jpg"
            - Local file path: "/path/to/image.jpg"
        client: Optional httpx.AsyncClient for URL fetching (created if not provided)
        max_dimension: Maximum width or height (default 2048px)

    Returns:
        PIL Image object, resized if necessary

    Raises:
        ValueError: If image cannot be decoded or fetched
    """
    if image_input.startswith("data:"):
        # Base64 data URI: "data:image/png;base64,<data>"
        try:
            # Extract base64 data after the comma
            _, data = image_input.split(",", 1)
            img_bytes = base64.b64decode(data)
            img = Image.open(BytesIO(img_bytes))
        except Exception as e:
            raise ValueError(f"Failed to decode base64 image: {e}") from e

    elif image_input.startswith(("http://", "https://")):
        # URL - fetch with retry
        img = await _fetch_image_from_url(image_input, client)

    else:
        # Assume local file path
        try:
            img = Image.open(image_input)
        except Exception as e:
            raise ValueError(f"Failed to open image file {image_input}: {e}") from e

    # Convert to RGB if needed (removes alpha channel, handles palette images)
    if img.mode not in ("RGB", "L"):
        img = img.convert("RGB")

    # Auto-resize if exceeds max dimension
    max_dim = max(img.size)
    if max_dim > max_dimension:
        scale = max_dimension / max_dim
        new_size = (int(img.size[0] * scale), int(img.size[1] * scale))
        original_size = img.size
        img = img.resize(new_size, Image.LANCZOS)
        logger.warning(
            f"Image resized from {original_size[0]}x{original_size[1]} to "
            f"{new_size[0]}x{new_size[1]} (max dimension: {max_dimension}px)"
        )

    return img


async def _fetch_image_from_url(
    url: str,
    client: httpx.AsyncClient | None = None,
) -> Image.Image:
    """Fetch image from URL with retry and timeout.

    Args:
        url: HTTP(S) URL to fetch
        client: Optional httpx.AsyncClient (created if not provided)

    Returns:
        PIL Image object

    Raises:
        ValueError: If image cannot be fetched after retries
    """
    should_close = False
    if client is None:
        client = httpx.AsyncClient()
        should_close = True

    try:
        last_error: Exception | None = None
        for attempt in range(MAX_URL_RETRIES):
            try:
                response = await client.get(url, timeout=URL_FETCH_TIMEOUT, follow_redirects=True)
                response.raise_for_status()
                return Image.open(BytesIO(response.content))
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                last_error = e
                if attempt < MAX_URL_RETRIES - 1:
                    wait_time = 2**attempt  # Exponential backoff: 1, 2, 4 seconds
                    logger.warning(
                        f"Failed to fetch {url} (attempt {attempt + 1}/{MAX_URL_RETRIES}), "
                        f"retrying in {wait_time}s: {e}"
                    )
                    await asyncio.sleep(wait_time)

        raise ValueError(
            f"Failed to fetch image from {url} after {MAX_URL_RETRIES} attempts: {last_error}"
        )
    finally:
        if should_close:
            await client.aclose()


async def preprocess_images(
    image_inputs: list[str],
    client: httpx.AsyncClient | None = None,
    max_dimension: int = MAX_IMAGE_DIMENSION,
) -> list[Image.Image]:
    """Process multiple images concurrently.

    Args:
        image_inputs: List of image sources (base64, URLs, or paths)
        client: Optional shared httpx.AsyncClient
        max_dimension: Maximum width or height

    Returns:
        List of PIL Image objects in same order as inputs
    """
    should_close = False
    if client is None:
        client = httpx.AsyncClient()
        should_close = True

    try:
        tasks = [
            preprocess_image(img_input, client, max_dimension)
            for img_input in image_inputs
        ]
        return await asyncio.gather(*tasks)
    finally:
        if should_close:
            await client.aclose()
```

Also add Pillow and httpx to backend dependencies if not already present:
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pip install Pillow httpx
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.services.image_processor import (
    preprocess_image,
    preprocess_images,
    MAX_IMAGE_DIMENSION,
)
print('MAX_IMAGE_DIMENSION:', MAX_IMAGE_DIMENSION)
print('preprocess_image function exists')
print('preprocess_images function exists')
"
```
  </verify>
  <done>Image processor service created with base64/URL/file support and auto-resize</done>
</task>

<task type="auto">
  <name>Task 4: Add vision content blocks to OpenAI schemas</name>
  <files>backend/mlx_manager/mlx_server/schemas/openai.py</files>
  <action>
Extend openai.py to support vision content blocks in chat messages:

1. **Add new models for content blocks** (add after existing imports):

```python
from typing import Literal, Union

# ... existing imports ...

# --- Vision Content Blocks ---

class ImageURL(BaseModel):
    """Image URL reference for vision content."""
    url: str  # Can be data:image/... base64 URI or http(s) URL
    detail: Literal["auto", "low", "high"] = "auto"


class ImageContentBlock(BaseModel):
    """Image content block in a message."""
    type: Literal["image_url"] = "image_url"
    image_url: ImageURL


class TextContentBlock(BaseModel):
    """Text content block in a message."""
    type: Literal["text"] = "text"
    text: str


# Union type for content blocks
ContentBlock = Union[TextContentBlock, ImageContentBlock]
```

2. **Modify ChatMessage to support content blocks:**

Change the existing ChatMessage class to:

```python
class ChatMessage(BaseModel):
    """A single message in the conversation.

    Content can be:
    - A simple string (text-only message)
    - A list of content blocks (for multimodal messages with images)
    """

    role: Literal["system", "user", "assistant"]
    content: str | list[ContentBlock]
```

3. **Add helper function to extract text and images from messages:**

```python
def extract_content_parts(
    content: str | list[ContentBlock],
) -> tuple[str, list[str]]:
    """Extract text and image URLs from message content.

    Args:
        content: Either a string or list of content blocks

    Returns:
        Tuple of (text_content, list_of_image_urls)
    """
    if isinstance(content, str):
        return content, []

    text_parts: list[str] = []
    image_urls: list[str] = []

    for block in content:
        if isinstance(block, dict):
            # Handle dict form (from JSON parsing)
            if block.get("type") == "text":
                text_parts.append(block.get("text", ""))
            elif block.get("type") == "image_url":
                img_url = block.get("image_url", {})
                if isinstance(img_url, dict):
                    image_urls.append(img_url.get("url", ""))
                elif isinstance(img_url, str):
                    image_urls.append(img_url)
        elif hasattr(block, "type"):
            # Handle Pydantic model form
            if block.type == "text":
                text_parts.append(block.text)
            elif block.type == "image_url":
                image_urls.append(block.image_url.url)

    return " ".join(text_parts), image_urls
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.schemas.openai import (
    ChatMessage,
    ImageURL,
    ImageContentBlock,
    TextContentBlock,
    extract_content_parts,
)

# Test simple string content
msg1 = ChatMessage(role='user', content='Hello')
text, images = extract_content_parts(msg1.content)
assert text == 'Hello'
assert images == []

# Test content blocks (as dicts, simulating JSON parsing)
msg2_content = [
    {'type': 'text', 'text': 'What is in this image?'},
    {'type': 'image_url', 'image_url': {'url': 'https://example.com/img.jpg'}}
]
msg2 = ChatMessage(role='user', content=msg2_content)
text, images = extract_content_parts(msg2.content)
assert 'What is in this image?' in text
assert 'https://example.com/img.jpg' in images

print('Vision content block schemas work correctly')
"
```
  </verify>
  <done>
- ImageURL, ImageContentBlock, TextContentBlock schemas added
- ChatMessage.content supports string or list of content blocks
- extract_content_parts() helper extracts text and images
  </done>
</task>

<task type="auto">
  <name>Task 5: Add unit tests for detection and image processor</name>
  <files>
backend/tests/mlx_server/test_detection.py
backend/tests/mlx_server/test_image_processor.py
  </files>
  <action>
**Part A: Create test_detection.py:**

```python
"""Tests for model type detection."""

import pytest
from unittest.mock import patch, MagicMock

from mlx_manager.mlx_server.models.detection import detect_model_type
from mlx_manager.mlx_server.models.types import ModelType


class TestDetectModelType:
    """Tests for detect_model_type function."""

    def test_vision_from_config_vision_config(self):
        """Detect vision from vision_config in config."""
        config = {"vision_config": {"hidden_size": 1024}}
        result = detect_model_type("test-model", config=config)
        assert result == ModelType.VISION

    def test_vision_from_config_image_token_id(self):
        """Detect vision from image_token_id in config."""
        config = {"image_token_id": 12345}
        result = detect_model_type("test-model", config=config)
        assert result == ModelType.VISION

    def test_vision_from_config_model_type(self):
        """Detect vision from 'vl' in model_type."""
        config = {"model_type": "qwen2_vl"}
        result = detect_model_type("test-model", config=config)
        assert result == ModelType.VISION

    def test_embeddings_from_config_architecture(self):
        """Detect embeddings from architecture name."""
        config = {"architectures": ["BertForSentenceEmbedding"]}
        result = detect_model_type("test-model", config=config)
        assert result == ModelType.EMBEDDINGS

    def test_vision_from_name_pattern(self):
        """Detect vision from model name pattern."""
        assert detect_model_type("mlx-community/Qwen2-VL-2B-4bit") == ModelType.VISION
        assert detect_model_type("mlx-community/llava-v1.5-7b-4bit") == ModelType.VISION
        assert detect_model_type("mlx-community/pixtral-12b-4bit") == ModelType.VISION

    def test_embeddings_from_name_pattern(self):
        """Detect embeddings from model name pattern."""
        assert detect_model_type("mlx-community/all-MiniLM-L6-v2") == ModelType.EMBEDDINGS
        assert detect_model_type("mlx-community/bge-small-en-v1.5") == ModelType.EMBEDDINGS
        assert detect_model_type("mlx-community/gte-small") == ModelType.EMBEDDINGS

    def test_text_gen_default(self):
        """Default to TEXT_GEN for unknown models."""
        assert detect_model_type("mlx-community/Llama-3.2-3B-4bit") == ModelType.TEXT_GEN
        assert detect_model_type("mlx-community/Qwen2.5-7B-Instruct") == ModelType.TEXT_GEN
        assert detect_model_type("unknown-model") == ModelType.TEXT_GEN
```

**Part B: Create test_image_processor.py:**

```python
"""Tests for image preprocessing service."""

import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from io import BytesIO
import base64

import httpx
from PIL import Image

from mlx_manager.mlx_server.services.image_processor import (
    preprocess_image,
    preprocess_images,
    MAX_IMAGE_DIMENSION,
)


def create_test_image(width: int, height: int) -> bytes:
    """Create a test image as PNG bytes."""
    img = Image.new("RGB", (width, height), color="red")
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    return buffer.getvalue()


def create_base64_data_uri(width: int, height: int) -> str:
    """Create a base64 data URI for a test image."""
    img_bytes = create_test_image(width, height)
    b64 = base64.b64encode(img_bytes).decode()
    return f"data:image/png;base64,{b64}"


class TestPreprocessImage:
    """Tests for preprocess_image function."""

    @pytest.mark.asyncio
    async def test_base64_decode(self):
        """Test base64 data URI decoding."""
        data_uri = create_base64_data_uri(100, 100)
        img = await preprocess_image(data_uri)
        assert img.size == (100, 100)

    @pytest.mark.asyncio
    async def test_resize_large_image(self):
        """Test auto-resize of large images."""
        # Create image larger than MAX_IMAGE_DIMENSION
        large_uri = create_base64_data_uri(4000, 3000)
        img = await preprocess_image(large_uri, max_dimension=2048)

        # Should be resized proportionally
        max_dim = max(img.size)
        assert max_dim <= 2048

    @pytest.mark.asyncio
    async def test_small_image_not_resized(self):
        """Test that small images are not resized."""
        small_uri = create_base64_data_uri(800, 600)
        img = await preprocess_image(small_uri)
        assert img.size == (800, 600)

    @pytest.mark.asyncio
    async def test_url_fetch(self):
        """Test URL fetching with mocked client."""
        test_img_bytes = create_test_image(200, 200)

        mock_response = MagicMock()
        mock_response.content = test_img_bytes
        mock_response.raise_for_status = MagicMock()

        mock_client = AsyncMock(spec=httpx.AsyncClient)
        mock_client.get = AsyncMock(return_value=mock_response)
        mock_client.aclose = AsyncMock()

        img = await preprocess_image("https://example.com/image.png", client=mock_client)
        assert img.size == (200, 200)
        mock_client.get.assert_called_once()

    @pytest.mark.asyncio
    async def test_invalid_base64_raises(self):
        """Test that invalid base64 raises ValueError."""
        with pytest.raises(ValueError, match="Failed to decode"):
            await preprocess_image("data:image/png;base64,invalid!!!")


class TestPreprocessImages:
    """Tests for batch image preprocessing."""

    @pytest.mark.asyncio
    async def test_multiple_images(self):
        """Test processing multiple images."""
        uri1 = create_base64_data_uri(100, 100)
        uri2 = create_base64_data_uri(200, 200)

        images = await preprocess_images([uri1, uri2])

        assert len(images) == 2
        assert images[0].size == (100, 100)
        assert images[1].size == (200, 200)
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_detection.py tests/mlx_server/test_image_processor.py -v
```
All tests should pass
  </verify>
  <done>
- Detection tests cover config-based and name-based detection
- Image processor tests cover base64, resize, and URL fetch
  </done>
</task>

</tasks>

<verification>
After all tasks:
```bash
cd /Users/atomasini/Development/mlx-manager/backend
# Type check
mypy mlx_manager/mlx_server/models/detection.py mlx_manager/mlx_server/services/image_processor.py
# Lint
ruff check mlx_manager/mlx_server/
# Tests
pytest tests/mlx_server/test_detection.py tests/mlx_server/test_image_processor.py tests/mlx_server/test_pool.py -v
```
All checks should pass.
</verification>

<success_criteria>
- Model type detection works from config.json and name patterns
- Pool loads vision models via mlx-vlm
- Pool loads embedding models via mlx-embeddings
- Image processor handles base64 data URIs
- Image processor fetches from URLs with retry
- Large images auto-resized with warning
- OpenAI schemas support vision content blocks
- All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-multi-model-multimodal/08-03-SUMMARY.md`
</output>
