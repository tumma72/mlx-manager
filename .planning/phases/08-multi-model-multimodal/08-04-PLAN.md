---
phase: 08-multi-model-multimodal
plan: 04
type: execute
wave: 2
depends_on: ["08-01", "08-03"]
files_modified:
  - backend/mlx_manager/mlx_server/services/vision.py
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/tests/mlx_server/test_vision.py
autonomous: true

must_haves:
  truths:
    - "Vision models generate responses from text + images"
    - "Text-only model receiving image request returns 400 error"
    - "Multiple images per message supported"
    - "SSE streaming works for vision model responses"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/vision.py"
      provides: "Vision generation service"
      exports: ["generate_vision_completion"]
    - path: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      provides: "Chat endpoint with vision support"
      contains: ["extract_content_parts", "generate_vision_completion"]
  key_links:
    - from: "chat.py"
      to: "vision.py"
      via: "import and call generate_vision_completion"
      pattern: "from.*vision import.*generate_vision_completion"
    - from: "vision.py"
      to: "mlx_vlm"
      via: "import mlx_vlm.generate"
      pattern: "mlx_vlm.*generate"
---

<objective>
Implement vision model inference with chat API integration

Purpose: Enable the /v1/chat/completions endpoint to handle multimodal requests with images, routing vision requests to mlx-vlm and returning 400 for text-only models receiving images.

Output: Vision generation service and updated chat endpoint with multimodal support
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-multi-model-multimodal/08-RESEARCH.md
@.planning/phases/08-multi-model-multimodal/08-CONTEXT.md
@backend/mlx_manager/mlx_server/services/inference.py
@backend/mlx_manager/mlx_server/services/image_processor.py
@backend/mlx_manager/mlx_server/api/v1/chat.py
@backend/mlx_manager/mlx_server/schemas/openai.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create vision generation service</name>
  <files>backend/mlx_manager/mlx_server/services/vision.py</files>
  <action>
Create vision.py service for vision model inference. This follows the pattern from inference.py but uses mlx-vlm:

```python
"""Vision model inference service.

Handles vision-language model generation using mlx-vlm.

CRITICAL: This module uses the same queue-based threading pattern as inference.py
to respect MLX Metal thread affinity requirements.
"""

import asyncio
import logging
import threading
import time
import uuid
from collections.abc import AsyncGenerator
from queue import Empty, Queue

from PIL import Image

try:
    import logfire

    LOGFIRE_AVAILABLE = True
except ImportError:
    LOGFIRE_AVAILABLE = False

logger = logging.getLogger(__name__)


async def generate_vision_completion(
    model_id: str,
    text_prompt: str,
    images: list[Image.Image],
    max_tokens: int = 4096,
    temperature: float = 0.7,
    stream: bool = False,
) -> AsyncGenerator[dict, None] | dict:
    """Generate a vision completion from text and images.

    Args:
        model_id: HuggingFace model ID (must be a vision model)
        text_prompt: Text portion of the prompt
        images: List of PIL Image objects (preprocessed)
        max_tokens: Maximum tokens to generate
        temperature: Sampling temperature
        stream: If True, yield chunks; if False, return complete response

    Yields/Returns:
        Streaming: yields chunk dicts
        Non-streaming: returns complete response dict

    Raises:
        RuntimeError: If model loading or generation fails
    """
    from mlx_manager.mlx_server.models.pool import get_model_pool

    # Get model from pool
    pool = get_model_pool()
    loaded = await pool.get_model(model_id)
    model = loaded.model
    processor = loaded.tokenizer  # VLM stores processor in tokenizer field

    # Generate unique ID for this completion
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:12]}"
    created = int(time.time())

    logger.info(
        f"Starting vision generation: {completion_id}, model={model_id}, "
        f"images={len(images)}, max_tokens={max_tokens}"
    )

    # LogFire span
    span_context = None
    if LOGFIRE_AVAILABLE:
        span_context = logfire.span(
            "vision_completion",
            model=model_id,
            num_images=len(images),
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
        )
        span_context.__enter__()

    try:
        if stream:
            return _stream_vision_generate(
                model=model,
                processor=processor,
                text_prompt=text_prompt,
                images=images,
                max_tokens=max_tokens,
                temperature=temperature,
                completion_id=completion_id,
                created=created,
                model_id=model_id,
            )
        else:
            return await _generate_vision_complete(
                model=model,
                processor=processor,
                text_prompt=text_prompt,
                images=images,
                max_tokens=max_tokens,
                temperature=temperature,
                completion_id=completion_id,
                created=created,
                model_id=model_id,
            )
    finally:
        if span_context:
            span_context.__exit__(None, None, None)


async def _stream_vision_generate(
    model,
    processor,
    text_prompt: str,
    images: list[Image.Image],
    max_tokens: int,
    temperature: float,
    completion_id: str,
    created: int,
    model_id: str,
) -> AsyncGenerator[dict, None]:
    """Generate vision completion with streaming.

    Note: mlx-vlm's generate() is non-streaming. We simulate streaming by
    running generation in a thread and yielding the complete response as
    a single chunk, then sending the finish chunk.

    TODO: Investigate mlx-vlm internals for true token-by-token streaming.
    """
    from mlx_manager.mlx_server.utils.memory import clear_cache

    # Queue for passing result from generation thread
    result_queue: Queue[str | Exception] = Queue()

    def run_generation() -> None:
        """Run vision generation in dedicated thread (owns Metal context)."""
        try:
            from mlx_vlm import generate as vlm_generate
            from mlx_vlm.prompt_utils import apply_chat_template
            from mlx_vlm.utils import load_config

            # Load config for chat template
            config = load_config(model_id)

            # Apply chat template with image count
            formatted_prompt = apply_chat_template(
                processor, config, text_prompt, num_images=len(images)
            )

            # Generate response
            response = vlm_generate(
                model,
                processor,
                formatted_prompt,
                images,
                max_tokens=max_tokens,
                temp=temperature,
                verbose=False,
            )

            result_queue.put(response)
        except Exception as e:
            result_queue.put(e)

    try:
        # First chunk with role
        yield {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model_id,
            "choices": [
                {
                    "index": 0,
                    "delta": {"role": "assistant", "content": ""},
                    "finish_reason": None,
                }
            ],
        }

        # Start generation thread
        gen_thread = threading.Thread(target=run_generation, daemon=True)
        gen_thread.start()

        # Wait for result (with 10 minute timeout for vision models)
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(None, lambda: result_queue.get(timeout=600))

        gen_thread.join(timeout=1.0)

        # Check for exception
        if isinstance(result, Exception):
            raise result

        response_text = result

        # Yield content chunk
        yield {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model_id,
            "choices": [
                {
                    "index": 0,
                    "delta": {"content": response_text},
                    "finish_reason": None,
                }
            ],
        }

        # Final chunk with finish_reason
        yield {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model_id,
            "choices": [
                {
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop",
                }
            ],
        }

        logger.info(f"Vision stream complete: {completion_id}")

        if LOGFIRE_AVAILABLE:
            logfire.info(
                "vision_stream_finished",
                completion_id=completion_id,
                response_length=len(response_text),
            )

    finally:
        clear_cache()


async def _generate_vision_complete(
    model,
    processor,
    text_prompt: str,
    images: list[Image.Image],
    max_tokens: int,
    temperature: float,
    completion_id: str,
    created: int,
    model_id: str,
) -> dict:
    """Generate complete vision response (non-streaming)."""
    from mlx_manager.mlx_server.utils.memory import clear_cache

    # Queue for passing result from generation thread
    result_queue: Queue[str | Exception] = Queue()

    def run_generation() -> None:
        """Run vision generation in dedicated thread (owns Metal context)."""
        try:
            from mlx_vlm import generate as vlm_generate
            from mlx_vlm.prompt_utils import apply_chat_template
            from mlx_vlm.utils import load_config

            # Load config for chat template
            config = load_config(model_id)

            # Apply chat template with image count
            formatted_prompt = apply_chat_template(
                processor, config, text_prompt, num_images=len(images)
            )

            # Generate response
            response = vlm_generate(
                model,
                processor,
                formatted_prompt,
                images,
                max_tokens=max_tokens,
                temp=temperature,
                verbose=False,
            )

            result_queue.put(response)
        except Exception as e:
            result_queue.put(e)

    try:
        # Start generation thread
        gen_thread = threading.Thread(target=run_generation, daemon=True)
        gen_thread.start()

        # Wait for result (with 10 minute timeout)
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(None, lambda: result_queue.get(timeout=600))

        gen_thread.join(timeout=1.0)

        # Check for exception
        if isinstance(result, Exception):
            raise result

        response_text = result

        # Estimate tokens (rough approximation)
        completion_tokens = len(response_text.split())
        prompt_tokens = len(text_prompt.split()) + (len(images) * 256)  # ~256 tokens per image

        logger.info(f"Vision complete: {completion_id}, response_length={len(response_text)}")

        if LOGFIRE_AVAILABLE:
            logfire.info(
                "vision_completion_finished",
                completion_id=completion_id,
                response_length=len(response_text),
            )

        return {
            "id": completion_id,
            "object": "chat.completion",
            "created": created,
            "model": model_id,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text,
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
            },
        }

    finally:
        clear_cache()
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.services.vision import (
    generate_vision_completion,
    _stream_vision_generate,
    _generate_vision_complete,
)
print('Vision service imports successfully')
print('generate_vision_completion exists')
"
```
  </verify>
  <done>Vision generation service created with streaming and non-streaming support</done>
</task>

<task type="auto">
  <name>Task 2: Update chat endpoint to handle vision requests</name>
  <files>backend/mlx_manager/mlx_server/api/v1/chat.py</files>
  <action>
Update chat.py to detect multimodal requests and route appropriately:

1. **Add imports at top:**
```python
from fastapi import HTTPException

from mlx_manager.mlx_server.models.pool import get_model_pool
from mlx_manager.mlx_server.schemas.openai import extract_content_parts
from mlx_manager.mlx_server.services.image_processor import preprocess_images
from mlx_manager.mlx_server.services.vision import generate_vision_completion
```

2. **Modify the chat_completions endpoint** to:
   - Extract text and images from all messages
   - If any images present:
     - Check if model is vision type, return 400 if not
     - Preprocess images
     - Call generate_vision_completion
   - If no images:
     - Call existing generate_chat_completion

Here's the updated endpoint logic (preserve existing endpoint decorator and signature):

```python
@router.post("/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
) -> ChatCompletionResponse | EventSourceResponse:
    """Create a chat completion.

    Supports both text-only and multimodal (vision) requests.
    """
    # Extract text and images from all user messages
    all_text_parts: list[str] = []
    all_image_urls: list[str] = []

    for message in request.messages:
        text, images = extract_content_parts(message.content)
        if message.role == "user":
            all_text_parts.append(text)
            all_image_urls.extend(images)

    has_images = len(all_image_urls) > 0

    if has_images:
        # Multimodal request - need vision model
        pool = get_model_pool()

        # Check model type before loading
        from mlx_manager.mlx_server.models.detection import detect_model_type
        from mlx_manager.mlx_server.models.types import ModelType

        model_type = detect_model_type(request.model)
        if model_type != ModelType.VISION:
            raise HTTPException(
                status_code=400,
                detail=f"Model '{request.model}' is type '{model_type.value}', "
                       f"but request contains images. Use a vision model (e.g., Qwen2-VL).",
            )

        # Preprocess images
        images = await preprocess_images(all_image_urls)

        # Build text prompt from messages
        # For vision, combine system + user messages into single prompt
        prompt_parts = []
        for message in request.messages:
            text, _ = extract_content_parts(message.content)
            if message.role == "system":
                prompt_parts.append(f"System: {text}")
            elif message.role == "user":
                prompt_parts.append(f"User: {text}")
            elif message.role == "assistant":
                prompt_parts.append(f"Assistant: {text}")
        text_prompt = "\n".join(prompt_parts)

        # Generate vision completion
        result = await generate_vision_completion(
            model_id=request.model,
            text_prompt=text_prompt,
            images=images,
            max_tokens=request.max_tokens or 4096,
            temperature=request.temperature,
            stream=request.stream,
        )

        if request.stream:
            # result is an async generator
            async def event_generator():
                async for chunk in result:
                    yield {"data": json.dumps(chunk)}
                yield {"data": "[DONE]"}

            return EventSourceResponse(event_generator())
        else:
            # result is a dict
            return ChatCompletionResponse(**result)

    else:
        # Text-only request - use existing logic
        # Convert messages to dict format for generate_chat_completion
        messages = [
            {"role": msg.role, "content": msg.content if isinstance(msg.content, str) else extract_content_parts(msg.content)[0]}
            for msg in request.messages
        ]

        result = await generate_chat_completion(
            model_id=request.model,
            messages=messages,
            max_tokens=request.max_tokens or 4096,
            temperature=request.temperature,
            top_p=request.top_p,
            stop=request.stop if isinstance(request.stop, list) else ([request.stop] if request.stop else None),
            stream=request.stream,
        )

        if request.stream:
            async def event_generator():
                async for chunk in result:
                    yield {"data": json.dumps(chunk)}
                yield {"data": "[DONE]"}

            return EventSourceResponse(event_generator())
        else:
            return ChatCompletionResponse(**result)
```

Make sure to add `import json` at the top if not already present.

IMPORTANT: Preserve the existing `generate_chat_completion` import and functionality for text-only requests.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.api.v1.chat import router, chat_completions
import inspect
source = inspect.getsource(chat_completions)
assert 'extract_content_parts' in source, 'extract_content_parts not used'
assert 'generate_vision_completion' in source, 'generate_vision_completion not used'
assert 'preprocess_images' in source, 'preprocess_images not used'
assert 'Model.*is type' in source or 'model_type.value' in source, '400 error logic not present'
print('chat.py correctly handles vision requests')
"
```
  </verify>
  <done>
- Chat endpoint extracts images from content blocks
- Routes to vision service when images present
- Returns 400 if non-vision model receives images
- Preserves text-only functionality
  </done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for vision service and chat integration</name>
  <files>backend/tests/mlx_server/test_vision.py</files>
  <action>
Create test_vision.py with tests for vision functionality:

```python
"""Tests for vision model inference."""

import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from PIL import Image

from mlx_manager.mlx_server.services.vision import (
    generate_vision_completion,
)


def create_test_image(width: int = 100, height: int = 100) -> Image.Image:
    """Create a test PIL Image."""
    return Image.new("RGB", (width, height), color="blue")


class TestVisionService:
    """Tests for vision generation service."""

    @pytest.mark.asyncio
    @patch("mlx_manager.mlx_server.services.vision.get_model_pool")
    async def test_generate_vision_completion_calls_pool(self, mock_get_pool):
        """Verify vision completion fetches model from pool."""
        # Setup mock
        mock_pool = MagicMock()
        mock_loaded = MagicMock()
        mock_loaded.model = MagicMock()
        mock_loaded.tokenizer = MagicMock()  # processor
        mock_pool.get_model = AsyncMock(return_value=mock_loaded)
        mock_get_pool.return_value = mock_pool

        images = [create_test_image()]

        # Mock the actual generation (we're testing integration, not mlx-vlm)
        with patch(
            "mlx_manager.mlx_server.services.vision._generate_vision_complete"
        ) as mock_gen:
            mock_gen.return_value = {
                "id": "test",
                "choices": [{"message": {"content": "Test response"}}],
            }

            result = await generate_vision_completion(
                model_id="test-vision-model",
                text_prompt="What is in this image?",
                images=images,
                stream=False,
            )

            mock_pool.get_model.assert_called_once_with("test-vision-model")


class TestChatVisionIntegration:
    """Tests for chat endpoint vision handling."""

    @pytest.mark.asyncio
    async def test_text_model_with_images_returns_400(self):
        """Verify 400 error when sending images to text-only model."""
        from fastapi import HTTPException

        from mlx_manager.mlx_server.schemas.openai import (
            ChatCompletionRequest,
            ChatMessage,
        )
        from mlx_manager.mlx_server.api.v1.chat import chat_completions

        # Request with images to a text model
        request = ChatCompletionRequest(
            model="mlx-community/Llama-3.2-3B-Instruct-4bit",  # Text model
            messages=[
                ChatMessage(
                    role="user",
                    content=[
                        {"type": "text", "text": "What is this?"},
                        {
                            "type": "image_url",
                            "image_url": {"url": "data:image/png;base64,abc123"},
                        },
                    ],
                )
            ],
        )

        # Mock pool and detection
        with patch(
            "mlx_manager.mlx_server.api.v1.chat.detect_model_type"
        ) as mock_detect:
            from mlx_manager.mlx_server.models.types import ModelType

            mock_detect.return_value = ModelType.TEXT_GEN

            with pytest.raises(HTTPException) as exc_info:
                await chat_completions(request)

            assert exc_info.value.status_code == 400
            assert "vision model" in exc_info.value.detail.lower()

    @pytest.mark.asyncio
    async def test_text_only_request_uses_inference_service(self):
        """Verify text-only requests use generate_chat_completion."""
        from mlx_manager.mlx_server.schemas.openai import (
            ChatCompletionRequest,
            ChatMessage,
        )
        from mlx_manager.mlx_server.api.v1.chat import chat_completions

        request = ChatCompletionRequest(
            model="mlx-community/Llama-3.2-3B-Instruct-4bit",
            messages=[
                ChatMessage(role="user", content="Hello, how are you?")
            ],
        )

        with patch(
            "mlx_manager.mlx_server.api.v1.chat.generate_chat_completion"
        ) as mock_gen:
            mock_gen.return_value = {
                "id": "test",
                "object": "chat.completion",
                "created": 1234567890,
                "model": "test",
                "choices": [
                    {
                        "index": 0,
                        "message": {"role": "assistant", "content": "I'm fine!"},
                        "finish_reason": "stop",
                    }
                ],
                "usage": {
                    "prompt_tokens": 10,
                    "completion_tokens": 5,
                    "total_tokens": 15,
                },
            }

            result = await chat_completions(request)

            mock_gen.assert_called_once()
            # Verify vision service was NOT called
            assert "I'm fine!" in str(result)
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_vision.py -v
```
All tests should pass
  </verify>
  <done>
- Tests verify pool is called for vision models
- Tests verify 400 error for text model + images
- Tests verify text-only requests don't use vision service
  </done>
</task>

</tasks>

<verification>
After all tasks:
```bash
cd /Users/atomasini/Development/mlx-manager/backend
# Type check
mypy mlx_manager/mlx_server/services/vision.py mlx_manager/mlx_server/api/v1/chat.py
# Lint
ruff check mlx_manager/mlx_server/
# Tests
pytest tests/mlx_server/test_vision.py -v
```
All checks should pass.
</verification>

<success_criteria>
- Vision service generates responses from text + images
- Chat endpoint detects multimodal requests from content blocks
- Text-only models return 400 when given images
- Vision requests route to generate_vision_completion
- Text-only requests continue to work via generate_chat_completion
- Streaming works for vision responses
- Unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-multi-model-multimodal/08-04-SUMMARY.md`
</output>
