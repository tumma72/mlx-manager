---
phase: 08-multi-model-multimodal
plan: 05
type: execute
wave: 2
depends_on: ["08-01", "08-03"]
files_modified:
  - backend/mlx_manager/mlx_server/schemas/openai.py
  - backend/mlx_manager/mlx_server/services/embeddings.py
  - backend/mlx_manager/mlx_server/api/v1/embeddings.py
  - backend/mlx_manager/mlx_server/main.py
  - backend/tests/mlx_server/test_embeddings.py
autonomous: true

must_haves:
  truths:
    - "/v1/embeddings endpoint accepts single string or array of strings"
    - "Returns OpenAI-compatible response with embeddings array"
    - "Embeddings are L2-normalized (ready for cosine similarity)"
    - "Non-embedding model returns 400 error"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/schemas/openai.py"
      provides: "EmbeddingRequest/Response schemas"
      contains: ["EmbeddingRequest", "EmbeddingResponse", "EmbeddingData"]
    - path: "backend/mlx_manager/mlx_server/services/embeddings.py"
      provides: "Embeddings generation service"
      exports: ["generate_embeddings"]
    - path: "backend/mlx_manager/mlx_server/api/v1/embeddings.py"
      provides: "/v1/embeddings endpoint"
      exports: ["router"]
  key_links:
    - from: "embeddings.py (api)"
      to: "embeddings.py (service)"
      via: "import generate_embeddings"
      pattern: "from.*services.embeddings import.*generate_embeddings"
    - from: "main.py"
      to: "embeddings.py (api)"
      via: "include_router"
      pattern: "include_router.*embeddings"
---

<objective>
Implement /v1/embeddings endpoint with mlx-embeddings integration

Purpose: Enable text embedding generation using mlx-embeddings models, supporting batch requests and returning OpenAI-compatible responses with L2-normalized vectors.

Output: Embeddings schemas, service, and API endpoint
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-multi-model-multimodal/08-RESEARCH.md
@.planning/phases/08-multi-model-multimodal/08-CONTEXT.md
@backend/mlx_manager/mlx_server/schemas/openai.py
@backend/mlx_manager/mlx_server/services/inference.py
@backend/mlx_manager/mlx_server/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add embeddings schemas to openai.py</name>
  <files>backend/mlx_manager/mlx_server/schemas/openai.py</files>
  <action>
Add the following schemas to the existing openai.py file (add after the existing response models):

```python
# --- Embeddings API ---

class EmbeddingRequest(BaseModel):
    """OpenAI Embeddings request.

    Reference: https://platform.openai.com/docs/api-reference/embeddings
    """

    input: str | list[str]  # Single string or batch of strings
    model: str
    # encoding_format: Literal["float", "base64"] = "float"  # Only float supported for now


class EmbeddingData(BaseModel):
    """A single embedding in the response."""

    embedding: list[float]
    index: int
    object: Literal["embedding"] = "embedding"


class EmbeddingUsage(BaseModel):
    """Token usage for embeddings request."""

    prompt_tokens: int
    total_tokens: int


class EmbeddingResponse(BaseModel):
    """OpenAI Embeddings response."""

    data: list[EmbeddingData]
    model: str
    object: Literal["list"] = "list"
    usage: EmbeddingUsage
```

Make sure `Literal` is imported from `typing` at the top of the file (should already be there for existing models).
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.schemas.openai import (
    EmbeddingRequest,
    EmbeddingData,
    EmbeddingUsage,
    EmbeddingResponse,
)

# Test single input
req1 = EmbeddingRequest(input='Hello world', model='test-model')
assert req1.input == 'Hello world'

# Test batch input
req2 = EmbeddingRequest(input=['Hello', 'World'], model='test-model')
assert len(req2.input) == 2

# Test response structure
resp = EmbeddingResponse(
    data=[EmbeddingData(embedding=[0.1, 0.2, 0.3], index=0)],
    model='test-model',
    usage=EmbeddingUsage(prompt_tokens=10, total_tokens=10),
)
assert resp.object == 'list'
assert resp.data[0].object == 'embedding'

print('Embeddings schemas work correctly')
"
```
  </verify>
  <done>EmbeddingRequest, EmbeddingData, EmbeddingUsage, EmbeddingResponse schemas added</done>
</task>

<task type="auto">
  <name>Task 2: Create embeddings generation service</name>
  <files>backend/mlx_manager/mlx_server/services/embeddings.py</files>
  <action>
Create embeddings.py service for embedding generation:

```python
"""Embeddings generation service using mlx-embeddings.

CRITICAL: This module uses the same queue-based threading pattern as inference.py
to respect MLX Metal thread affinity requirements.
"""

import asyncio
import logging
import threading
from queue import Queue

try:
    import logfire

    LOGFIRE_AVAILABLE = True
except ImportError:
    LOGFIRE_AVAILABLE = False

logger = logging.getLogger(__name__)


async def generate_embeddings(
    model_id: str,
    texts: list[str],
) -> tuple[list[list[float]], int]:
    """Generate embeddings for a list of texts.

    Args:
        model_id: HuggingFace model ID (must be an embeddings model)
        texts: List of strings to embed

    Returns:
        Tuple of (list of embedding vectors, total token count)

    Raises:
        RuntimeError: If model loading or generation fails
    """
    from mlx_manager.mlx_server.models.pool import get_model_pool

    # Get model from pool
    pool = get_model_pool()
    loaded = await pool.get_model(model_id)
    model = loaded.model
    tokenizer = loaded.tokenizer

    logger.info(f"Generating embeddings: model={model_id}, texts={len(texts)}")

    # LogFire span
    span_context = None
    if LOGFIRE_AVAILABLE:
        span_context = logfire.span(
            "embeddings",
            model=model_id,
            num_texts=len(texts),
        )
        span_context.__enter__()

    try:
        # Queue for passing result from generation thread
        result_queue: Queue[tuple[list[list[float]], int] | Exception] = Queue()

        def run_embeddings() -> None:
            """Run embedding generation in dedicated thread (owns Metal context)."""
            try:
                import mlx.core as mx

                # Tokenize batch
                # mlx-embeddings tokenizer uses batch_encode_plus
                inputs = tokenizer.batch_encode_plus(
                    texts,
                    return_tensors="mlx",
                    padding=True,
                    truncation=True,
                    max_length=512,
                )

                # Count tokens per input (not padded batch)
                total_tokens = 0
                for text in texts:
                    tokens = tokenizer.encode(text, truncation=True, max_length=512)
                    total_tokens += len(tokens)

                # Forward pass
                outputs = model(
                    inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask"),
                )

                # text_embeds are ALREADY L2-normalized (mean pooled + normalized)
                embeddings = outputs.text_embeds

                # Convert to Python lists
                # Use mx.eval to ensure computation is complete before converting
                mx.eval(embeddings)
                embeddings_list = embeddings.tolist()

                result_queue.put((embeddings_list, total_tokens))

            except Exception as e:
                result_queue.put(e)

        # Start generation thread
        gen_thread = threading.Thread(target=run_embeddings, daemon=True)
        gen_thread.start()

        # Wait for result (with 5 minute timeout)
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(None, lambda: result_queue.get(timeout=300))

        gen_thread.join(timeout=1.0)

        # Check for exception
        if isinstance(result, Exception):
            raise RuntimeError(f"Embeddings generation failed: {result}") from result

        embeddings_list, total_tokens = result

        logger.info(
            f"Embeddings complete: model={model_id}, "
            f"vectors={len(embeddings_list)}, tokens={total_tokens}"
        )

        if LOGFIRE_AVAILABLE:
            logfire.info(
                "embeddings_finished",
                model=model_id,
                num_embeddings=len(embeddings_list),
                total_tokens=total_tokens,
            )

        return embeddings_list, total_tokens

    finally:
        if span_context:
            span_context.__exit__(None, None, None)

        # Clear cache after embeddings
        from mlx_manager.mlx_server.utils.memory import clear_cache

        clear_cache()
```

NOTE: The `mx.eval()` call is an MLX framework function that ensures tensor computation is complete before converting to Python lists. This is NOT the dangerous Python eval() function.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.services.embeddings import generate_embeddings
import inspect
source = inspect.getsource(generate_embeddings)
assert 'get_model_pool' in source
assert 'text_embeds' in source
assert 'batch_encode_plus' in source
print('Embeddings service imports and structure correct')
"
```
  </verify>
  <done>Embeddings service created with batch support and L2-normalized output</done>
</task>

<task type="auto">
  <name>Task 3: Create /v1/embeddings endpoint</name>
  <files>backend/mlx_manager/mlx_server/api/v1/embeddings.py</files>
  <action>
Create embeddings.py API endpoint:

```python
"""Embeddings API endpoint.

OpenAI-compatible endpoint for generating text embeddings.
Reference: https://platform.openai.com/docs/api-reference/embeddings
"""

import logging

from fastapi import APIRouter, HTTPException

from mlx_manager.mlx_server.models.detection import detect_model_type
from mlx_manager.mlx_server.models.types import ModelType
from mlx_manager.mlx_server.schemas.openai import (
    EmbeddingData,
    EmbeddingRequest,
    EmbeddingResponse,
    EmbeddingUsage,
)
from mlx_manager.mlx_server.services.embeddings import generate_embeddings

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/v1", tags=["embeddings"])


@router.post("/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest) -> EmbeddingResponse:
    """Create embeddings for the input text(s).

    Accepts a single string or array of strings and returns embeddings
    in OpenAI-compatible format. Embeddings are L2-normalized.
    """
    # Normalize input to list
    texts = [request.input] if isinstance(request.input, str) else list(request.input)

    if not texts:
        raise HTTPException(status_code=400, detail="Input cannot be empty")

    # Validate model type
    model_type = detect_model_type(request.model)
    if model_type != ModelType.EMBEDDINGS:
        raise HTTPException(
            status_code=400,
            detail=f"Model '{request.model}' is type '{model_type.value}', not 'embeddings'. "
            f"Use an embedding model (e.g., all-MiniLM-L6-v2).",
        )

    try:
        # Generate embeddings
        embeddings_list, total_tokens = await generate_embeddings(
            model_id=request.model,
            texts=texts,
        )

        # Build response
        return EmbeddingResponse(
            data=[
                EmbeddingData(embedding=emb, index=i)
                for i, emb in enumerate(embeddings_list)
            ],
            model=request.model,
            usage=EmbeddingUsage(
                prompt_tokens=total_tokens,
                total_tokens=total_tokens,
            ),
        )

    except Exception as e:
        logger.error(f"Embeddings generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.api.v1.embeddings import router, create_embeddings
print('Embeddings router:', router.prefix)
print('Routes:', [r.path for r in router.routes])
"
```
Should show prefix /v1 and route /embeddings
  </verify>
  <done>/v1/embeddings endpoint created with model type validation</done>
</task>

<task type="auto">
  <name>Task 4: Register embeddings router in main.py</name>
  <files>backend/mlx_manager/mlx_server/main.py</files>
  <action>
Update main.py to include the embeddings router:

1. **Add import near the top** (with other router imports):
```python
from mlx_manager.mlx_server.api.v1.embeddings import router as embeddings_router
```

2. **Add router registration** (after existing router registrations):
```python
app.include_router(embeddings_router)
```

Find where other routers are registered (likely `chat_router`, `completions_router`, `models_router`) and add the embeddings router in the same section.
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
python -c "
from mlx_manager.mlx_server.main import app

# Check that embeddings route is registered
routes = [r.path for r in app.routes if hasattr(r, 'path')]
assert '/v1/embeddings' in routes, f'Embeddings route not found. Routes: {routes}'
print('Embeddings route registered:', '/v1/embeddings' in routes)
"
```
  </verify>
  <done>Embeddings router registered in FastAPI app</done>
</task>

<task type="auto">
  <name>Task 5: Add unit tests for embeddings</name>
  <files>backend/tests/mlx_server/test_embeddings.py</files>
  <action>
Create test_embeddings.py:

```python
"""Tests for embeddings endpoint and service."""

import pytest
from unittest.mock import AsyncMock, MagicMock, patch

from mlx_manager.mlx_server.schemas.openai import (
    EmbeddingRequest,
    EmbeddingResponse,
)
from mlx_manager.mlx_server.api.v1.embeddings import create_embeddings


class TestEmbeddingsSchemas:
    """Tests for embeddings schemas."""

    def test_embedding_request_single_input(self):
        """Test EmbeddingRequest with single string."""
        req = EmbeddingRequest(input="Hello world", model="test-model")
        assert req.input == "Hello world"

    def test_embedding_request_batch_input(self):
        """Test EmbeddingRequest with list of strings."""
        req = EmbeddingRequest(
            input=["Hello", "World"],
            model="test-model",
        )
        assert isinstance(req.input, list)
        assert len(req.input) == 2

    def test_embedding_response_structure(self):
        """Test EmbeddingResponse structure."""
        from mlx_manager.mlx_server.schemas.openai import (
            EmbeddingData,
            EmbeddingUsage,
        )

        resp = EmbeddingResponse(
            data=[
                EmbeddingData(embedding=[0.1, 0.2, 0.3], index=0),
                EmbeddingData(embedding=[0.4, 0.5, 0.6], index=1),
            ],
            model="test-model",
            usage=EmbeddingUsage(prompt_tokens=10, total_tokens=10),
        )

        assert resp.object == "list"
        assert len(resp.data) == 2
        assert resp.data[0].object == "embedding"
        assert len(resp.data[0].embedding) == 3


class TestEmbeddingsEndpoint:
    """Tests for /v1/embeddings endpoint."""

    @pytest.mark.asyncio
    async def test_non_embedding_model_returns_400(self):
        """Verify 400 error when using non-embedding model."""
        from fastapi import HTTPException

        request = EmbeddingRequest(
            input="Hello world",
            model="mlx-community/Llama-3.2-3B-Instruct-4bit",  # Text model
        )

        with patch(
            "mlx_manager.mlx_server.api.v1.embeddings.detect_model_type"
        ) as mock_detect:
            from mlx_manager.mlx_server.models.types import ModelType

            mock_detect.return_value = ModelType.TEXT_GEN

            with pytest.raises(HTTPException) as exc_info:
                await create_embeddings(request)

            assert exc_info.value.status_code == 400
            assert "embedding" in exc_info.value.detail.lower()

    @pytest.mark.asyncio
    async def test_empty_input_returns_400(self):
        """Verify 400 error when input is empty list."""
        from fastapi import HTTPException

        request = EmbeddingRequest(
            input=[],
            model="mlx-community/all-MiniLM-L6-v2-4bit",
        )

        with patch(
            "mlx_manager.mlx_server.api.v1.embeddings.detect_model_type"
        ) as mock_detect:
            from mlx_manager.mlx_server.models.types import ModelType

            mock_detect.return_value = ModelType.EMBEDDINGS

            with pytest.raises(HTTPException) as exc_info:
                await create_embeddings(request)

            assert exc_info.value.status_code == 400
            assert "empty" in exc_info.value.detail.lower()

    @pytest.mark.asyncio
    async def test_successful_embedding_generation(self):
        """Test successful embedding generation."""
        request = EmbeddingRequest(
            input=["Hello", "World"],
            model="mlx-community/all-MiniLM-L6-v2-4bit",
        )

        with patch(
            "mlx_manager.mlx_server.api.v1.embeddings.detect_model_type"
        ) as mock_detect:
            from mlx_manager.mlx_server.models.types import ModelType

            mock_detect.return_value = ModelType.EMBEDDINGS

            with patch(
                "mlx_manager.mlx_server.api.v1.embeddings.generate_embeddings"
            ) as mock_gen:
                # Return mock embeddings
                mock_gen.return_value = (
                    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],  # embeddings
                    10,  # total_tokens
                )

                response = await create_embeddings(request)

                assert isinstance(response, EmbeddingResponse)
                assert len(response.data) == 2
                assert response.data[0].index == 0
                assert response.data[1].index == 1
                assert response.usage.total_tokens == 10


class TestEmbeddingsService:
    """Tests for embeddings service."""

    @pytest.mark.asyncio
    @patch("mlx_manager.mlx_server.services.embeddings.get_model_pool")
    async def test_generate_embeddings_calls_pool(self, mock_get_pool):
        """Verify embeddings generation fetches model from pool."""
        from mlx_manager.mlx_server.services.embeddings import generate_embeddings

        # Setup mock
        mock_pool = MagicMock()
        mock_loaded = MagicMock()
        mock_loaded.model = MagicMock()
        mock_loaded.tokenizer = MagicMock()
        mock_pool.get_model = AsyncMock(return_value=mock_loaded)
        mock_get_pool.return_value = mock_pool

        # We can't fully test without mlx, so just verify pool is called
        # The actual generation is mocked
        with patch(
            "mlx_manager.mlx_server.services.embeddings.threading.Thread"
        ) as mock_thread:
            # Make thread complete immediately
            mock_thread_instance = MagicMock()
            mock_thread.return_value = mock_thread_instance

            # Skip actual execution by mocking queue
            with patch(
                "mlx_manager.mlx_server.services.embeddings.Queue"
            ) as mock_queue_class:
                mock_queue = MagicMock()
                mock_queue.get.return_value = ([[0.1, 0.2]], 5)
                mock_queue_class.return_value = mock_queue

                result = await generate_embeddings(
                    model_id="test-embedding-model",
                    texts=["Hello"],
                )

                mock_pool.get_model.assert_called_once_with("test-embedding-model")
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
pytest tests/mlx_server/test_embeddings.py -v
```
All tests should pass
  </verify>
  <done>
- Tests cover schema validation
- Tests cover 400 errors for wrong model type and empty input
- Tests cover successful embedding generation
  </done>
</task>

</tasks>

<verification>
After all tasks:
```bash
cd /Users/atomasini/Development/mlx-manager/backend
# Type check
mypy mlx_manager/mlx_server/schemas/openai.py mlx_manager/mlx_server/services/embeddings.py mlx_manager/mlx_server/api/v1/embeddings.py
# Lint
ruff check mlx_manager/mlx_server/
# Tests
pytest tests/mlx_server/test_embeddings.py -v
```
All checks should pass.
</verification>

<success_criteria>
- EmbeddingRequest/Response schemas follow OpenAI spec
- /v1/embeddings accepts single string or array
- Returns embeddings array with correct indices
- Non-embedding model returns 400 error
- Empty input returns 400 error
- Embeddings router registered in main.py
- Unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-multi-model-multimodal/08-05-SUMMARY.md`
</output>
