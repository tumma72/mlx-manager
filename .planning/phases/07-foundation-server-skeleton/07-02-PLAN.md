---
phase: 07-foundation-server-skeleton
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/mlx_server/schemas/__init__.py
  - backend/mlx_manager/mlx_server/schemas/openai.py
  - backend/mlx_manager/mlx_server/api/__init__.py
  - backend/mlx_manager/mlx_server/api/v1/__init__.py
  - backend/mlx_manager/mlx_server/api/v1/models.py
autonomous: true

must_haves:
  truths:
    - "OpenAI-compatible Pydantic schemas exist for chat completions"
    - "Schemas use Pydantic v2 with proper Field constraints"
    - "/v1/models endpoint returns list of available models"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/schemas/openai.py"
      provides: "OpenAI request/response schemas"
      contains: "class ChatCompletionRequest"
    - path: "backend/mlx_manager/mlx_server/api/v1/models.py"
      provides: "Models listing endpoint"
      exports: ["router"]
  key_links:
    - from: "backend/mlx_manager/mlx_server/api/v1/models.py"
      to: "backend/mlx_manager/mlx_server/schemas/openai.py"
      via: "import schemas"
      pattern: "from.*schemas.*import"
---

<objective>
Create OpenAI-compatible Pydantic v2 schemas and the /v1/models endpoint.

Purpose: Define the data contracts for OpenAI API compatibility. These schemas will be used by chat completions and completions endpoints. The /v1/models endpoint allows clients to discover available models.

Output: Complete Pydantic schemas for OpenAI API + working /v1/models endpoint.
</objective>

<execution_context>
@.claude/agents/gsd-executor.md
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-foundation-server-skeleton/07-RESEARCH.md

# Existing patterns
@backend/mlx_manager/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAI-compatible Pydantic schemas</name>
  <files>
    backend/mlx_manager/mlx_server/schemas/__init__.py
    backend/mlx_manager/mlx_server/schemas/openai.py
  </files>
  <action>
Create schemas package with OpenAI API-compatible request/response models.

1. Create `backend/mlx_manager/mlx_server/schemas/__init__.py`:
   - Export all public schemas from openai.py

2. Create `backend/mlx_manager/mlx_server/schemas/openai.py`:

   Use Pydantic BaseModel with Field constraints. Reference: https://platform.openai.com/docs/api-reference/chat

   ```python
   from pydantic import BaseModel, Field
   from typing import Literal
   import time

   # --- Request Models ---

   class ChatMessage(BaseModel):
       """A single message in the conversation."""
       role: Literal["system", "user", "assistant"]
       content: str

   class ChatCompletionRequest(BaseModel):
       """OpenAI Chat Completion request."""
       model: str
       messages: list[ChatMessage]
       max_tokens: int | None = Field(default=None, ge=1, le=128000)
       temperature: float = Field(default=1.0, ge=0.0, le=2.0)
       top_p: float = Field(default=1.0, ge=0.0, le=1.0)
       stream: bool = False
       stop: list[str] | str | None = None
       frequency_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
       presence_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
       # n: int = 1  # Not supported initially (always 1)

   class CompletionRequest(BaseModel):
       """OpenAI Completion request (legacy)."""
       model: str
       prompt: str | list[str]
       max_tokens: int | None = Field(default=16, ge=1, le=128000)
       temperature: float = Field(default=1.0, ge=0.0, le=2.0)
       top_p: float = Field(default=1.0, ge=0.0, le=1.0)
       stream: bool = False
       stop: list[str] | str | None = None
       echo: bool = False

   # --- Response Models ---

   class ChatCompletionChoice(BaseModel):
       """A single choice in chat completion response."""
       index: int
       message: ChatMessage
       finish_reason: Literal["stop", "length", "content_filter"] | None = None

   class CompletionChoice(BaseModel):
       """A single choice in completion response."""
       index: int
       text: str
       finish_reason: Literal["stop", "length"] | None = None

   class Usage(BaseModel):
       """Token usage statistics."""
       prompt_tokens: int
       completion_tokens: int
       total_tokens: int

   class ChatCompletionResponse(BaseModel):
       """OpenAI Chat Completion response."""
       id: str
       object: Literal["chat.completion"] = "chat.completion"
       created: int = Field(default_factory=lambda: int(time.time()))
       model: str
       choices: list[ChatCompletionChoice]
       usage: Usage

   class CompletionResponse(BaseModel):
       """OpenAI Completion response."""
       id: str
       object: Literal["text_completion"] = "text_completion"
       created: int = Field(default_factory=lambda: int(time.time()))
       model: str
       choices: list[CompletionChoice]
       usage: Usage

   # --- Streaming Response Models ---

   class ChatCompletionChunkDelta(BaseModel):
       """Delta content in streaming response."""
       role: str | None = None
       content: str | None = None

   class ChatCompletionChunkChoice(BaseModel):
       """A single choice in streaming chunk."""
       index: int
       delta: ChatCompletionChunkDelta
       finish_reason: Literal["stop", "length", "content_filter"] | None = None

   class ChatCompletionChunk(BaseModel):
       """OpenAI Chat Completion streaming chunk."""
       id: str
       object: Literal["chat.completion.chunk"] = "chat.completion.chunk"
       created: int = Field(default_factory=lambda: int(time.time()))
       model: str
       choices: list[ChatCompletionChunkChoice]

   # --- Models Endpoint ---

   class ModelInfo(BaseModel):
       """Information about a single model."""
       id: str
       object: Literal["model"] = "model"
       created: int = Field(default_factory=lambda: int(time.time()))
       owned_by: str = "mlx-community"

   class ModelListResponse(BaseModel):
       """Response for /v1/models endpoint."""
       object: Literal["list"] = "list"
       data: list[ModelInfo]
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.schemas.openai import ChatCompletionRequest, ChatCompletionResponse; print('OK')"`
    - `cd backend && python -c "from mlx_manager.mlx_server.schemas import ModelListResponse; print('OK')"`
  </verify>
  <done>All schemas import without error, Pydantic validates field constraints</done>
</task>

<task type="auto">
  <name>Task 2: Create /v1/models endpoint</name>
  <files>
    backend/mlx_manager/mlx_server/api/__init__.py
    backend/mlx_manager/mlx_server/api/v1/__init__.py
    backend/mlx_manager/mlx_server/api/v1/models.py
  </files>
  <action>
Create the API router structure and /v1/models endpoint.

1. Create `backend/mlx_manager/mlx_server/api/__init__.py`:
   - Empty file (package marker)

2. Create `backend/mlx_manager/mlx_server/api/v1/__init__.py`:
   - Import and export router from models.py
   - Create combined v1_router that includes all v1 endpoints

3. Create `backend/mlx_manager/mlx_server/api/v1/models.py`:

   ```python
   """Models listing endpoint."""
   from fastapi import APIRouter

   from mlx_manager.mlx_server.schemas.openai import ModelInfo, ModelListResponse

   router = APIRouter(prefix="/v1", tags=["models"])

   # Placeholder for model pool integration (Plan 03)
   # For now, return empty list or mock data
   def get_available_models() -> list[str]:
       """Get list of available model IDs from model pool."""
       # TODO: Integrate with ModelPoolManager in Plan 03
       # For now, return placeholder to enable API testing
       return ["mlx-community/Llama-3.2-3B-Instruct-4bit"]

   @router.get("/models", response_model=ModelListResponse)
   async def list_models() -> ModelListResponse:
       """List all available models.

       Returns models that are:
       - Currently loaded (hot)
       - Available for loading (configured)
       """
       model_ids = get_available_models()

       models = [
           ModelInfo(id=model_id, owned_by="mlx-community")
           for model_id in model_ids
       ]

       return ModelListResponse(data=models)

   @router.get("/models/{model_id}", response_model=ModelInfo)
   async def get_model(model_id: str) -> ModelInfo:
       """Get information about a specific model."""
       # TODO: Check if model exists in pool
       # For now, return info for any requested model
       return ModelInfo(id=model_id, owned_by="mlx-community")
   ```

4. Update `backend/mlx_manager/mlx_server/api/v1/__init__.py`:
   ```python
   """v1 API router."""
   from fastapi import APIRouter
   from mlx_manager.mlx_server.api.v1.models import router as models_router

   v1_router = APIRouter()
   v1_router.include_router(models_router)
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1 import v1_router; print(v1_router.routes)"`
  </verify>
  <done>Router imports successfully, has /v1/models routes</done>
</task>

<task type="auto">
  <name>Task 3: Wire up router to main app</name>
  <files>
    backend/mlx_manager/mlx_server/main.py
  </files>
  <action>
Update main.py to include the v1 router.

Add to `backend/mlx_manager/mlx_server/main.py`:

1. Import the v1 router:
   ```python
   from mlx_manager.mlx_server.api.v1 import v1_router
   ```

2. Include router after app creation:
   ```python
   app.include_router(v1_router)
   ```

This wires up /v1/models to the app.
  </action>
  <verify>
    - Start server and test endpoint:
      ```bash
      cd backend && python -m mlx_manager.mlx_server.main &
      sleep 2
      curl http://127.0.0.1:8000/v1/models
      curl http://127.0.0.1:8000/v1/models/test-model
      kill %1
      ```
  </verify>
  <done>/v1/models returns JSON with model list, /v1/models/{id} returns model info</done>
</task>

</tasks>

<verification>
1. Schema validation works:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.schemas.openai import ChatCompletionRequest

   # Valid request
   req = ChatCompletionRequest(
       model='test',
       messages=[{'role': 'user', 'content': 'Hello'}]
   )
   print(f'Valid: {req.model}')

   # Invalid temperature should raise
   try:
       ChatCompletionRequest(
           model='test',
           messages=[{'role': 'user', 'content': 'Hello'}],
           temperature=3.0  # Invalid: max is 2.0
       )
   except Exception as e:
       print(f'Validation works: {type(e).__name__}')
   "
   ```

2. API endpoint works:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)
   response = client.get('/v1/models')
   print(f'Status: {response.status_code}')
   print(f'Body: {response.json()}')
   "
   ```

3. Quality checks:
   ```bash
   cd backend && ruff check mlx_manager/mlx_server/ && ruff format --check mlx_manager/mlx_server/ && mypy mlx_manager/mlx_server/
   ```
</verification>

<success_criteria>
- [ ] OpenAI schemas created with proper Field constraints
- [ ] ChatCompletionRequest validates temperature 0-2, max_tokens 1-128000
- [ ] /v1/models endpoint returns ModelListResponse
- [ ] /v1/models/{model_id} endpoint returns ModelInfo
- [ ] Router wired to main app
- [ ] All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-02-SUMMARY.md`
</output>
