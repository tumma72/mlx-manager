---
phase: 07-foundation-server-skeleton
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/mlx_server/schemas/__init__.py
  - backend/mlx_manager/mlx_server/schemas/openai.py
  - backend/mlx_manager/mlx_server/api/__init__.py
  - backend/mlx_manager/mlx_server/api/v1/__init__.py
  - backend/mlx_manager/mlx_server/api/v1/models.py
  - backend/mlx_manager/mlx_server/config.py
autonomous: true

must_haves:
  truths:
    - "OpenAI-compatible Pydantic schemas exist for chat completions"
    - "Schemas use Pydantic v2 with proper Field constraints"
    - "/v1/models endpoint returns both loaded AND loadable models"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/schemas/openai.py"
      provides: "OpenAI request/response schemas"
      contains: "class ChatCompletionRequest"
    - path: "backend/mlx_manager/mlx_server/api/v1/models.py"
      provides: "Models listing endpoint"
      exports: ["router"]
    - path: "backend/mlx_manager/mlx_server/config.py"
      provides: "Server configuration with available models"
      contains: "available_models"
  key_links:
    - from: "backend/mlx_manager/mlx_server/api/v1/models.py"
      to: "backend/mlx_manager/mlx_server/schemas/openai.py"
      via: "import schemas"
      pattern: "from.*schemas.*import"
    - from: "backend/mlx_manager/mlx_server/api/v1/models.py"
      to: "backend/mlx_manager/mlx_server/config.py"
      via: "config import for available models"
      pattern: "from.*config.*import|get_settings"
---

<objective>
Create OpenAI-compatible Pydantic v2 schemas and the /v1/models endpoint.

Purpose: Define the data contracts for OpenAI API compatibility. These schemas will be used by chat completions and completions endpoints. The /v1/models endpoint allows clients to discover available models (both loaded and loadable).

Output: Complete Pydantic schemas for OpenAI API + working /v1/models endpoint + config for available models.
</objective>

<execution_context>
@.claude/agents/gsd-executor.md
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-foundation-server-skeleton/07-RESEARCH.md

# Existing patterns
@backend/mlx_manager/models.py
@backend/mlx_manager/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAI-compatible Pydantic schemas</name>
  <files>
    backend/mlx_manager/mlx_server/schemas/__init__.py
    backend/mlx_manager/mlx_server/schemas/openai.py
  </files>
  <action>
Create schemas package with OpenAI API-compatible request/response models.

1. Create `backend/mlx_manager/mlx_server/schemas/__init__.py`:
   - Export all public schemas from openai.py

2. Create `backend/mlx_manager/mlx_server/schemas/openai.py`:

   Use Pydantic BaseModel with Field constraints. Reference: https://platform.openai.com/docs/api-reference/chat

   ```python
   from pydantic import BaseModel, Field
   from typing import Literal
   import time

   # --- Request Models ---

   class ChatMessage(BaseModel):
       """A single message in the conversation."""
       role: Literal["system", "user", "assistant"]
       content: str

   class ChatCompletionRequest(BaseModel):
       """OpenAI Chat Completion request."""
       model: str
       messages: list[ChatMessage]
       max_tokens: int | None = Field(default=None, ge=1, le=128000)
       temperature: float = Field(default=1.0, ge=0.0, le=2.0)
       top_p: float = Field(default=1.0, ge=0.0, le=1.0)
       stream: bool = False
       stop: list[str] | str | None = None
       frequency_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
       presence_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
       # n: int = 1  # Not supported initially (always 1)

   class CompletionRequest(BaseModel):
       """OpenAI Completion request (legacy)."""
       model: str
       prompt: str | list[str]
       max_tokens: int | None = Field(default=16, ge=1, le=128000)
       temperature: float = Field(default=1.0, ge=0.0, le=2.0)
       top_p: float = Field(default=1.0, ge=0.0, le=1.0)
       stream: bool = False
       stop: list[str] | str | None = None
       echo: bool = False

   # --- Response Models ---

   class ChatCompletionChoice(BaseModel):
       """A single choice in chat completion response."""
       index: int
       message: ChatMessage
       finish_reason: Literal["stop", "length", "content_filter"] | None = None

   class CompletionChoice(BaseModel):
       """A single choice in completion response."""
       index: int
       text: str
       finish_reason: Literal["stop", "length"] | None = None

   class Usage(BaseModel):
       """Token usage statistics."""
       prompt_tokens: int
       completion_tokens: int
       total_tokens: int

   class ChatCompletionResponse(BaseModel):
       """OpenAI Chat Completion response."""
       id: str
       object: Literal["chat.completion"] = "chat.completion"
       created: int = Field(default_factory=lambda: int(time.time()))
       model: str
       choices: list[ChatCompletionChoice]
       usage: Usage

   class CompletionResponse(BaseModel):
       """OpenAI Completion response."""
       id: str
       object: Literal["text_completion"] = "text_completion"
       created: int = Field(default_factory=lambda: int(time.time()))
       model: str
       choices: list[CompletionChoice]
       usage: Usage

   # --- Streaming Response Models ---

   class ChatCompletionChunkDelta(BaseModel):
       """Delta content in streaming response."""
       role: str | None = None
       content: str | None = None

   class ChatCompletionChunkChoice(BaseModel):
       """A single choice in streaming chunk."""
       index: int
       delta: ChatCompletionChunkDelta
       finish_reason: Literal["stop", "length", "content_filter"] | None = None

   class ChatCompletionChunk(BaseModel):
       """OpenAI Chat Completion streaming chunk."""
       id: str
       object: Literal["chat.completion.chunk"] = "chat.completion.chunk"
       created: int = Field(default_factory=lambda: int(time.time()))
       model: str
       choices: list[ChatCompletionChunkChoice]

   # --- Models Endpoint ---

   class ModelInfo(BaseModel):
       """Information about a single model."""
       id: str
       object: Literal["model"] = "model"
       created: int = Field(default_factory=lambda: int(time.time()))
       owned_by: str = "mlx-community"

   class ModelListResponse(BaseModel):
       """Response for /v1/models endpoint."""
       object: Literal["list"] = "list"
       data: list[ModelInfo]
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.schemas.openai import ChatCompletionRequest, ChatCompletionResponse; print('OK')"`
    - `cd backend && python -c "from mlx_manager.mlx_server.schemas import ModelListResponse; print('OK')"`
  </verify>
  <done>All schemas import without error, Pydantic validates field constraints</done>
</task>

<task type="auto">
  <name>Task 2: Create MLX server config with available models</name>
  <files>
    backend/mlx_manager/mlx_server/config.py
  </files>
  <action>
Create server configuration with list of available (loadable) models.

This addresses API-04 requirement: "/v1/models returns hot + loadable models".

Create `backend/mlx_manager/mlx_server/config.py`:

```python
"""MLX Server configuration."""
from functools import lru_cache

from pydantic import Field
from pydantic_settings import BaseSettings


class MLXServerSettings(BaseSettings):
    """Settings for the MLX inference server."""

    # Server settings
    host: str = "127.0.0.1"
    port: int = 8000

    # Available models (loadable via /v1/chat/completions)
    # These are model IDs that the server is configured to serve.
    # Models are loaded on-demand when requested.
    # Format: HuggingFace model ID (e.g., "mlx-community/Llama-3.2-3B-Instruct-4bit")
    available_models: list[str] = Field(
        default_factory=lambda: [
            "mlx-community/Llama-3.2-3B-Instruct-4bit",
        ],
        description="List of model IDs available for loading"
    )

    # Default model for inference (if not specified in request)
    default_model: str | None = None

    # Memory settings
    max_cache_size_gb: float = Field(
        default=8.0,
        ge=1.0,
        le=128.0,
        description="Maximum GPU cache size in GB"
    )

    model_config = {
        "env_prefix": "MLX_SERVER_",
        "env_file": ".env",
        "extra": "ignore",
    }


@lru_cache
def get_settings() -> MLXServerSettings:
    """Get cached settings instance."""
    return MLXServerSettings()
```

**Key design decisions:**
- `available_models` is a simple list of HuggingFace model IDs
- Can be configured via `MLX_SERVER_AVAILABLE_MODELS` env var (comma-separated)
- Default includes one small model for testing
- Later phases can extend this with model registry or database lookup
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.config import get_settings; s = get_settings(); print(s.available_models)"`
  </verify>
  <done>Config imports, available_models is accessible</done>
</task>

<task type="auto">
  <name>Task 3: Create /v1/models endpoint with hot + loadable models</name>
  <files>
    backend/mlx_manager/mlx_server/api/__init__.py
    backend/mlx_manager/mlx_server/api/v1/__init__.py
    backend/mlx_manager/mlx_server/api/v1/models.py
    backend/mlx_manager/mlx_server/main.py
  </files>
  <action>
Create the API router structure and /v1/models endpoint that returns both loaded and loadable models.

1. Create `backend/mlx_manager/mlx_server/api/__init__.py`:
   - Empty file (package marker)

2. Create `backend/mlx_manager/mlx_server/api/v1/__init__.py`:
   - Import and export router from models.py
   - Create combined v1_router that includes all v1 endpoints

3. Create `backend/mlx_manager/mlx_server/api/v1/models.py`:

   ```python
   """Models listing endpoint."""
   from fastapi import APIRouter, HTTPException

   from mlx_manager.mlx_server.config import get_settings
   from mlx_manager.mlx_server.schemas.openai import ModelInfo, ModelListResponse

   router = APIRouter(prefix="/v1", tags=["models"])


   def get_available_models() -> list[str]:
       """Get list of available model IDs.

       Returns both:
       - Currently loaded models (hot) - from ModelPool (Plan 03)
       - Configured loadable models - from settings

       For now, returns configured models. Plan 03 will add loaded model detection.
       """
       settings = get_settings()

       # Start with configured available models
       model_ids = set(settings.available_models)

       # TODO: Plan 03 will add loaded models from ModelPoolManager
       # loaded_models = pool.get_loaded_models()
       # model_ids.update(loaded_models)

       return sorted(model_ids)


   @router.get("/models", response_model=ModelListResponse)
   async def list_models() -> ModelListResponse:
       """List all available models.

       Returns models that are:
       - Currently loaded (hot) - ready for immediate inference
       - Available for loading (configured) - will be loaded on first request

       This satisfies API-04: /v1/models returns hot + loadable models.
       """
       model_ids = get_available_models()

       models = [
           ModelInfo(id=model_id, owned_by="mlx-community")
           for model_id in model_ids
       ]

       return ModelListResponse(data=models)


   @router.get("/models/{model_id:path}", response_model=ModelInfo)
   async def get_model(model_id: str) -> ModelInfo:
       """Get information about a specific model.

       Args:
           model_id: HuggingFace model ID (e.g., "mlx-community/Llama-3.2-3B-Instruct-4bit")

       Returns:
           ModelInfo if model is available (loaded or loadable)

       Raises:
           404 if model is not in available_models list
       """
       available = get_available_models()
       if model_id not in available:
           raise HTTPException(
               status_code=404,
               detail=f"Model '{model_id}' not found. Available models: {available}"
           )

       return ModelInfo(id=model_id, owned_by="mlx-community")
   ```

4. Update `backend/mlx_manager/mlx_server/api/v1/__init__.py`:
   ```python
   """v1 API router."""
   from fastapi import APIRouter
   from mlx_manager.mlx_server.api.v1.models import router as models_router

   v1_router = APIRouter()
   v1_router.include_router(models_router)
   ```

5. Update `backend/mlx_manager/mlx_server/main.py`:
   - Import the v1 router:
     ```python
     from mlx_manager.mlx_server.api.v1 import v1_router
     ```
   - Include router after app creation:
     ```python
     app.include_router(v1_router)
     ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1 import v1_router; print(v1_router.routes)"`
    - Start server and test endpoint:
      ```bash
      cd backend && python -m mlx_manager.mlx_server.main &
      sleep 2
      curl http://127.0.0.1:8000/v1/models
      curl http://127.0.0.1:8000/v1/models/mlx-community/Llama-3.2-3B-Instruct-4bit
      curl http://127.0.0.1:8000/v1/models/nonexistent-model  # Should 404
      kill %1
      ```
  </verify>
  <done>Router imports, /v1/models returns configured available_models, /v1/models/{id} returns 404 for unknown models</done>
</task>

</tasks>

<verification>
1. Schema validation works:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.schemas.openai import ChatCompletionRequest

   # Valid request
   req = ChatCompletionRequest(
       model='test',
       messages=[{'role': 'user', 'content': 'Hello'}]
   )
   print(f'Valid: {req.model}')

   # Invalid temperature should raise
   try:
       ChatCompletionRequest(
           model='test',
           messages=[{'role': 'user', 'content': 'Hello'}],
           temperature=3.0  # Invalid: max is 2.0
       )
   except Exception as e:
       print(f'Validation works: {type(e).__name__}')
   "
   ```

2. Config provides available models:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.config import get_settings
   settings = get_settings()
   print(f'Available models: {settings.available_models}')
   assert len(settings.available_models) > 0, 'Should have at least one model'
   "
   ```

3. API endpoint returns available models:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)
   response = client.get('/v1/models')
   print(f'Status: {response.status_code}')
   data = response.json()
   print(f'Models: {[m[\"id\"] for m in data[\"data\"]]}')
   assert response.status_code == 200
   assert len(data['data']) > 0, 'Should return at least one model'
   "
   ```

4. Quality checks:
   ```bash
   cd backend && ruff check mlx_manager/mlx_server/ && ruff format --check mlx_manager/mlx_server/ && mypy mlx_manager/mlx_server/
   ```
</verification>

<success_criteria>
- [ ] OpenAI schemas created with proper Field constraints
- [ ] ChatCompletionRequest validates temperature 0-2, max_tokens 1-128000
- [ ] MLX server config exists with available_models setting
- [ ] /v1/models endpoint returns configured available models (API-04: hot + loadable)
- [ ] /v1/models/{model_id} returns 404 for unconfigured models
- [ ] Router wired to main app
- [ ] All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-02-SUMMARY.md`
</output>
