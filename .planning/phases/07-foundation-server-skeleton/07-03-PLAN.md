---
phase: 07-foundation-server-skeleton
plan: 03
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - backend/mlx_manager/mlx_server/models/__init__.py
  - backend/mlx_manager/mlx_server/models/pool.py
  - backend/mlx_manager/mlx_server/utils/__init__.py
  - backend/mlx_manager/mlx_server/utils/memory.py
  - backend/mlx_manager/mlx_server/main.py
autonomous: true

must_haves:
  truths:
    - "ModelPoolManager loads models via mlx-lm load() function"
    - "Memory tracking uses mlx.core.metal functions"
    - "Models are cached and reused across requests"
    - "Memory cleanup happens after generation"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/pool.py"
      provides: "Model pool manager singleton"
      contains: "class ModelPoolManager"
    - path: "backend/mlx_manager/mlx_server/utils/memory.py"
      provides: "MLX memory utilities"
      contains: "def get_memory_usage"
  key_links:
    - from: "backend/mlx_manager/mlx_server/models/pool.py"
      to: "mlx_lm.load"
      via: "model loading"
      pattern: "from mlx_lm import load"
    - from: "backend/mlx_manager/mlx_server/main.py"
      to: "backend/mlx_manager/mlx_server/models/pool.py"
      via: "lifespan initialization"
      pattern: "model_pool"
---

<objective>
Create the Model Pool Manager for loading and managing MLX models.

Purpose: The model pool manager is the core component that loads models via mlx-lm, tracks memory usage, and provides models for inference. This plan implements single-model support; multi-model LRU eviction comes in Phase 8.

Output: ModelPoolManager that loads one model, tracks memory, and provides cleanup utilities.
</objective>

<execution_context>
@.claude/agents/gsd-executor.md
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-foundation-server-skeleton/07-RESEARCH.md
@.planning/phases/07-foundation-server-skeleton/07-01-SUMMARY.md

# Research patterns for memory management
# Pattern 1: MLX Memory Management from 07-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MLX memory utilities</name>
  <files>
    backend/mlx_manager/mlx_server/utils/__init__.py
    backend/mlx_manager/mlx_server/utils/memory.py
  </files>
  <action>
Create memory management utilities for MLX Metal operations.

1. Create `backend/mlx_manager/mlx_server/utils/__init__.py`:
   - Export memory functions

2. Create `backend/mlx_manager/mlx_server/utils/memory.py`:

   ```python
   """MLX memory management utilities."""
   import logging

   logger = logging.getLogger(__name__)

   # Lazy import to allow testing without MLX
   def _get_mx():
       """Lazy import mlx.core."""
       import mlx.core as mx
       return mx

   def get_memory_usage() -> dict:
       """Get current MLX Metal memory usage.

       Returns:
           dict with keys:
           - active_gb: Currently allocated memory
           - peak_gb: Peak memory usage
           - cache_gb: Cached memory
       """
       try:
           mx = _get_mx()
           return {
               "active_gb": mx.metal.get_active_memory() / (1024 ** 3),
               "peak_gb": mx.metal.get_peak_memory() / (1024 ** 3),
               "cache_gb": mx.metal.get_cache_memory() / (1024 ** 3),
           }
       except Exception as e:
           logger.warning(f"Failed to get memory usage: {e}")
           return {"active_gb": 0.0, "peak_gb": 0.0, "cache_gb": 0.0}

   def clear_cache() -> None:
       """Clear MLX Metal cache.

       Should be called after generation to free memory.
       """
       try:
           mx = _get_mx()
           mx.synchronize()  # Wait for pending operations
           mx.metal.clear_cache()
           logger.debug("MLX cache cleared")
       except Exception as e:
           logger.warning(f"Failed to clear cache: {e}")

   def set_memory_limit(max_gb: float, relaxed: bool = True) -> None:
       """Set MLX memory limit.

       Args:
           max_gb: Maximum memory in gigabytes
           relaxed: If True, allow exceeding limit temporarily
       """
       try:
           mx = _get_mx()
           max_bytes = int(max_gb * 1024 ** 3)
           mx.metal.set_memory_limit(max_bytes, relaxed=relaxed)
           logger.info(f"MLX memory limit set to {max_gb:.1f} GB")
       except Exception as e:
           logger.warning(f"Failed to set memory limit: {e}")

   def reset_peak_memory() -> None:
       """Reset peak memory counter."""
       try:
           mx = _get_mx()
           mx.metal.reset_peak_memory()
       except Exception as e:
           logger.warning(f"Failed to reset peak memory: {e}")
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.utils.memory import get_memory_usage; print(get_memory_usage())"`
  </verify>
  <done>Memory utilities import and return dict (values may be 0 if MLX not available)</done>
</task>

<task type="auto">
  <name>Task 2: Create Model Pool Manager</name>
  <files>
    backend/mlx_manager/mlx_server/models/__init__.py
    backend/mlx_manager/mlx_server/models/pool.py
  </files>
  <action>
Create the ModelPoolManager class that loads and manages MLX models.

1. Create `backend/mlx_manager/mlx_server/models/__init__.py`:
   - Export ModelPoolManager

2. Create `backend/mlx_manager/mlx_server/models/pool.py`:

   ```python
   """Model Pool Manager for loading and managing MLX models."""
   import asyncio
   import logging
   import time
   from dataclasses import dataclass, field

   logger = logging.getLogger(__name__)


   @dataclass
   class LoadedModel:
       """Container for a loaded model and its metadata."""
       model_id: str
       model: object  # mlx_lm model
       tokenizer: object  # HuggingFace tokenizer
       loaded_at: float = field(default_factory=time.time)
       last_used: float = field(default_factory=time.time)
       size_gb: float = 0.0  # Estimated size

       def touch(self) -> None:
           """Update last_used timestamp."""
           self.last_used = time.time()


   class ModelPoolManager:
       """Manages loading and caching of MLX models.

       Phase 7 implementation: Single model support.
       Phase 8 will add: Multi-model with LRU eviction.
       """

       def __init__(self, max_memory_gb: float = 48.0, max_models: int = 1):
           """Initialize the model pool.

           Args:
               max_memory_gb: Maximum memory for model pool
               max_models: Maximum number of hot models (1 for Phase 7)
           """
           self._models: dict[str, LoadedModel] = {}
           self._loading: dict[str, asyncio.Event] = {}
           self._lock = asyncio.Lock()
           self.max_memory_gb = max_memory_gb
           self.max_models = max_models
           logger.info(f"ModelPoolManager initialized (max_memory={max_memory_gb}GB, max_models={max_models})")

       async def get_model(self, model_id: str) -> LoadedModel:
           """Get a model, loading it if necessary.

           Args:
               model_id: HuggingFace model ID (e.g., "mlx-community/Llama-3.2-3B-Instruct-4bit")

           Returns:
               LoadedModel with model and tokenizer

           Raises:
               RuntimeError: If model loading fails
           """
           async with self._lock:
               # Return cached model
               if model_id in self._models:
                   loaded = self._models[model_id]
                   loaded.touch()
                   logger.debug(f"Model cache hit: {model_id}")
                   return loaded

               # Wait if already loading
               if model_id in self._loading:
                   logger.debug(f"Waiting for model load: {model_id}")

           # Wait outside lock if loading
           if model_id in self._loading:
               await self._loading[model_id].wait()
               return self._models[model_id]

           # Start loading
           return await self._load_model(model_id)

       async def _load_model(self, model_id: str) -> LoadedModel:
           """Load a model from HuggingFace.

           Args:
               model_id: Model identifier

           Returns:
               LoadedModel instance
           """
           async with self._lock:
               # Double-check after acquiring lock
               if model_id in self._models:
                   return self._models[model_id]

               # Mark as loading
               self._loading[model_id] = asyncio.Event()

           logger.info(f"Loading model: {model_id}")
           start_time = time.time()

           try:
               # Import mlx_lm lazily
               from mlx_lm import load

               # Load in thread pool (blocking operation)
               model, tokenizer = await asyncio.to_thread(load, model_id)

               # Get memory after loading
               from mlx_manager.mlx_server.utils.memory import get_memory_usage
               memory = get_memory_usage()

               loaded = LoadedModel(
                   model_id=model_id,
                   model=model,
                   tokenizer=tokenizer,
                   size_gb=memory["active_gb"],
               )

               async with self._lock:
                   self._models[model_id] = loaded
                   self._loading[model_id].set()
                   del self._loading[model_id]

               elapsed = time.time() - start_time
               logger.info(f"Model loaded: {model_id} ({elapsed:.1f}s, {loaded.size_gb:.1f}GB)")
               return loaded

           except Exception as e:
               async with self._lock:
                   if model_id in self._loading:
                       self._loading[model_id].set()
                       del self._loading[model_id]
               logger.error(f"Failed to load model {model_id}: {e}")
               raise RuntimeError(f"Failed to load model: {e}") from e

       async def unload_model(self, model_id: str) -> bool:
           """Unload a model from the pool.

           Args:
               model_id: Model to unload

           Returns:
               True if model was unloaded, False if not found
           """
           async with self._lock:
               if model_id not in self._models:
                   return False

               del self._models[model_id]

           # Clear cache after unloading
           from mlx_manager.mlx_server.utils.memory import clear_cache
           clear_cache()

           logger.info(f"Model unloaded: {model_id}")
           return True

       def get_loaded_models(self) -> list[str]:
           """Get list of currently loaded model IDs."""
           return list(self._models.keys())

       def is_loaded(self, model_id: str) -> bool:
           """Check if a model is currently loaded."""
           return model_id in self._models

       async def cleanup(self) -> None:
           """Unload all models and clear cache."""
           async with self._lock:
               model_ids = list(self._models.keys())

           for model_id in model_ids:
               await self.unload_model(model_id)

           logger.info("Model pool cleaned up")


   # Singleton instance (initialized in main.py lifespan)
   model_pool: ModelPoolManager | None = None


   def get_model_pool() -> ModelPoolManager:
       """Get the model pool singleton.

       Raises:
           RuntimeError: If pool not initialized
       """
       if model_pool is None:
           raise RuntimeError("Model pool not initialized")
       return model_pool
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.models.pool import ModelPoolManager; print('OK')"`
    - `cd backend && python -c "from mlx_manager.mlx_server.models import ModelPoolManager; print('OK')"`
  </verify>
  <done>ModelPoolManager class imports successfully</done>
</task>

<task type="auto">
  <name>Task 3: Integrate model pool with main app</name>
  <files>
    backend/mlx_manager/mlx_server/main.py
    backend/mlx_manager/mlx_server/api/v1/models.py
  </files>
  <action>
Wire up ModelPoolManager to the FastAPI app lifespan and update /v1/models endpoint.

1. Update `backend/mlx_manager/mlx_server/main.py`:

   - Import at top:
     ```python
     from mlx_manager.mlx_server.models import pool
     from mlx_manager.mlx_server.models.pool import ModelPoolManager
     from mlx_manager.mlx_server.utils.memory import set_memory_limit, get_memory_usage
     ```

   - Update lifespan to initialize model pool:
     ```python
     @asynccontextmanager
     async def lifespan(app: FastAPI):
         # Startup
         logger.info("MLX Server starting...")

         # Set memory limit
         set_memory_limit(mlx_server_settings.max_memory_gb)

         # Initialize model pool
         pool.model_pool = ModelPoolManager(
             max_memory_gb=mlx_server_settings.max_memory_gb,
             max_models=mlx_server_settings.max_models,
         )
         logger.info(f"Memory usage at startup: {get_memory_usage()}")
         logger.info("MLX Server ready")

         yield

         # Shutdown
         logger.info("MLX Server shutting down...")
         if pool.model_pool:
             await pool.model_pool.cleanup()
         logger.info("MLX Server stopped")
     ```

2. Update `backend/mlx_manager/mlx_server/api/v1/models.py`:

   - Replace placeholder with actual pool integration:
     ```python
     from mlx_manager.mlx_server.models.pool import get_model_pool

     def get_available_models() -> list[str]:
         """Get list of available model IDs from model pool."""
         try:
             pool = get_model_pool()
             return pool.get_loaded_models()
         except RuntimeError:
             # Pool not initialized yet
             return []
     ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.main import app; print('App imports OK')"`
    - Run server briefly and check logs show model pool initialization
  </verify>
  <done>
    - App startup initializes model pool
    - /v1/models returns loaded models from pool
    - Shutdown cleans up model pool
  </done>
</task>

</tasks>

<verification>
1. Memory utilities work:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.utils.memory import get_memory_usage, clear_cache
   print(f'Memory: {get_memory_usage()}')
   clear_cache()
   print('Cache cleared')
   "
   ```

2. Model pool manager works:
   ```bash
   cd backend && python -c "
   import asyncio
   from mlx_manager.mlx_server.models.pool import ModelPoolManager

   async def test():
       pool = ModelPoolManager(max_memory_gb=8.0)
       print(f'Loaded: {pool.get_loaded_models()}')
       # Note: Actual model loading tested in integration
       return True

   asyncio.run(test())
   print('Pool test passed')
   "
   ```

3. App integration works:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)
   response = client.get('/v1/models')
   print(f'Models: {response.json()}')
   "
   ```

4. Quality checks:
   ```bash
   cd backend && ruff check mlx_manager/mlx_server/ && ruff format --check mlx_manager/mlx_server/ && mypy mlx_manager/mlx_server/
   ```
</verification>

<success_criteria>
- [ ] Memory utilities get/set MLX memory stats
- [ ] ModelPoolManager loads models via mlx_lm.load()
- [ ] Models are cached and reused (cache hit logging)
- [ ] Model pool initialized in app lifespan
- [ ] /v1/models returns loaded models from pool
- [ ] Cleanup runs on shutdown
- [ ] All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-03-SUMMARY.md`
</output>
