---
phase: 07-foundation-server-skeleton
plan: 06
type: execute
wave: 3
depends_on: ["07-02", "07-03"]
files_modified:
  - backend/mlx_manager/mlx_server/api/v1/completions.py
  - backend/mlx_manager/mlx_server/api/v1/__init__.py
  - backend/mlx_manager/mlx_server/services/inference.py
autonomous: true

must_haves:
  truths:
    - "/v1/completions endpoint accepts OpenAI legacy format"
    - "Completions work without chat template"
    - "Streaming and non-streaming modes supported"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/api/v1/completions.py"
      provides: "Completions endpoint"
      exports: ["router"]
  key_links:
    - from: "backend/mlx_manager/mlx_server/api/v1/completions.py"
      to: "backend/mlx_manager/mlx_server/services/inference.py"
      via: "inference service"
      pattern: "from.*services.*import"
---

<objective>
Create the /v1/completions endpoint for legacy OpenAI API compatibility.

Purpose: The completions endpoint is the legacy OpenAI API for raw text completion (no chat template). Some clients still use this format. This completes API-02 requirement.

Output: Working /v1/completions endpoint with streaming and non-streaming modes.
</objective>

<execution_context>
@.claude/agents/gsd-executor.md
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-foundation-server-skeleton/07-RESEARCH.md
@.planning/phases/07-foundation-server-skeleton/07-02-SUMMARY.md
@.planning/phases/07-foundation-server-skeleton/07-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add raw completion generation to inference service</name>
  <files>
    backend/mlx_manager/mlx_server/services/inference.py
  </files>
  <action>
Add a function for raw text completion (no chat template).

Add to `backend/mlx_manager/mlx_server/services/inference.py`:

```python
async def generate_completion(
    model_id: str,
    prompt: str | list[str],
    max_tokens: int = 16,
    temperature: float = 1.0,
    top_p: float = 1.0,
    stop: list[str] | None = None,
    stream: bool = False,
    echo: bool = False,
) -> AsyncGenerator[dict, None] | dict:
    """Generate a raw text completion (legacy API).

    Args:
        model_id: HuggingFace model ID
        prompt: Text prompt or list of prompts
        max_tokens: Maximum tokens to generate
        temperature: Sampling temperature
        top_p: Nucleus sampling parameter
        stop: Additional stop strings
        stream: If True, yield chunks; if False, return complete response
        echo: If True, include prompt in response

    Yields/Returns:
        Streaming: yields chunk dicts
        Non-streaming: returns complete response dict
    """
    from mlx_manager.mlx_server.models.pool import get_model_pool
    from mlx_manager.mlx_server.models.adapters import get_adapter
    from mlx_manager.mlx_server.utils.memory import clear_cache

    # Handle list of prompts (use first for now, batch in Phase 9)
    if isinstance(prompt, list):
        prompt = prompt[0] if prompt else ""

    # Get model from pool
    pool = get_model_pool()
    loaded = await pool.get_model(model_id)
    model = loaded.model
    tokenizer = loaded.tokenizer

    # Get stop tokens (still use adapter for this)
    adapter = get_adapter(model_id)
    stop_tokens = set(adapter.get_stop_tokens(tokenizer))

    # Generate unique ID
    completion_id = f"cmpl-{uuid.uuid4().hex[:12]}"
    created = int(time.time())

    logger.info(f"Starting completion: {completion_id}, model={model_id}, max_tokens={max_tokens}")

    if stream:
        return _stream_completion(
            model=model,
            tokenizer=tokenizer,
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stop_tokens=stop_tokens,
            completion_id=completion_id,
            created=created,
            model_id=model_id,
            echo=echo,
        )
    else:
        return await _generate_raw_completion(
            model=model,
            tokenizer=tokenizer,
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stop_tokens=stop_tokens,
            completion_id=completion_id,
            created=created,
            model_id=model_id,
            echo=echo,
        )


async def _stream_completion(
    model,
    tokenizer,
    prompt: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    stop_tokens: set[int],
    completion_id: str,
    created: int,
    model_id: str,
    echo: bool,
) -> AsyncGenerator[dict, None]:
    """Stream raw completion tokens."""
    from mlx_lm import stream_generate
    from mlx_manager.mlx_server.utils.memory import clear_cache

    try:
        # Echo prompt if requested
        if echo:
            yield {
                "id": completion_id,
                "object": "text_completion",
                "created": created,
                "model": model_id,
                "choices": [{
                    "index": 0,
                    "text": prompt,
                    "finish_reason": None,
                }],
            }

        # Generate tokens
        def generate_sync():
            for response in stream_generate(
                model,
                tokenizer,
                prompt,
                max_tokens=max_tokens,
                temp=temperature,
                top_p=top_p,
            ):
                yield response

        for response in await asyncio.to_thread(list, generate_sync()):
            token_text = response.text if hasattr(response, 'text') else str(response)

            yield {
                "id": completion_id,
                "object": "text_completion",
                "created": created,
                "model": model_id,
                "choices": [{
                    "index": 0,
                    "text": token_text,
                    "finish_reason": None,
                }],
            }

        # Final chunk
        yield {
            "id": completion_id,
            "object": "text_completion",
            "created": created,
            "model": model_id,
            "choices": [{
                "index": 0,
                "text": "",
                "finish_reason": "stop",
            }],
        }

    finally:
        clear_cache()


async def _generate_raw_completion(
    model,
    tokenizer,
    prompt: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    stop_tokens: set[int],
    completion_id: str,
    created: int,
    model_id: str,
    echo: bool,
) -> dict:
    """Generate complete raw completion."""
    from mlx_lm import generate
    from mlx_manager.mlx_server.utils.memory import clear_cache

    prompt_tokens = len(tokenizer.encode(prompt))

    try:
        response = await asyncio.to_thread(
            generate,
            model,
            tokenizer,
            prompt,
            max_tokens=max_tokens,
            temp=temperature,
            top_p=top_p,
        )

        # Prepend prompt if echo requested
        text = (prompt + response) if echo else response
        completion_tokens = len(tokenizer.encode(response))

        return {
            "id": completion_id,
            "object": "text_completion",
            "created": created,
            "model": model_id,
            "choices": [{
                "index": 0,
                "text": text,
                "finish_reason": "stop",
            }],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
            },
        }

    finally:
        clear_cache()
```

Don't forget to add the import for `uuid` and `time` if not already present.
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.services.inference import generate_completion; print('OK')"`
  </verify>
  <done>generate_completion function added to inference service</done>
</task>

<task type="auto">
  <name>Task 2: Create /v1/completions endpoint</name>
  <files>
    backend/mlx_manager/mlx_server/api/v1/completions.py
    backend/mlx_manager/mlx_server/api/v1/__init__.py
  </files>
  <action>
Create the completions endpoint for legacy API support.

1. Create `backend/mlx_manager/mlx_server/api/v1/completions.py`:

   ```python
   """Legacy completions endpoint."""
   import json
   import logging

   from fastapi import APIRouter, HTTPException
   from sse_starlette.sse import EventSourceResponse

   from mlx_manager.mlx_server.schemas.openai import (
       CompletionRequest,
       CompletionResponse,
       CompletionChoice,
       Usage,
   )
   from mlx_manager.mlx_server.services.inference import generate_completion

   logger = logging.getLogger(__name__)

   router = APIRouter(prefix="/v1", tags=["completions"])


   @router.post("/completions")
   async def create_completion(
       request: CompletionRequest,
   ):
       """Create a text completion (legacy API).

       Supports both streaming and non-streaming responses.
       Compatible with OpenAI Completions API.
       """
       logger.info(f"Completion request: model={request.model}, stream={request.stream}")

       # Handle stop parameter
       stop = request.stop if isinstance(request.stop, list) else ([request.stop] if request.stop else None)

       try:
           if request.stream:
               return await _handle_streaming(request, stop)
           else:
               return await _handle_non_streaming(request, stop)
       except RuntimeError as e:
           logger.error(f"Generation error: {e}")
           raise HTTPException(status_code=500, detail=str(e))
       except Exception as e:
           logger.exception(f"Unexpected error: {e}")
           raise HTTPException(status_code=500, detail="Internal server error")


   async def _handle_streaming(
       request: CompletionRequest,
       stop: list[str] | None,
   ):
       """Handle streaming response."""

       async def event_generator():
           async for chunk in await generate_completion(
               model_id=request.model,
               prompt=request.prompt,
               max_tokens=request.max_tokens or 16,
               temperature=request.temperature,
               top_p=request.top_p,
               stop=stop,
               stream=True,
               echo=request.echo,
           ):
               yield {"data": json.dumps(chunk)}

           yield {"data": "[DONE]"}

       return EventSourceResponse(event_generator())


   async def _handle_non_streaming(
       request: CompletionRequest,
       stop: list[str] | None,
   ) -> CompletionResponse:
       """Handle non-streaming response."""
       result = await generate_completion(
           model_id=request.model,
           prompt=request.prompt,
           max_tokens=request.max_tokens or 16,
           temperature=request.temperature,
           top_p=request.top_p,
           stop=stop,
           stream=False,
           echo=request.echo,
       )

       choice = result["choices"][0]
       return CompletionResponse(
           id=result["id"],
           created=result["created"],
           model=result["model"],
           choices=[
               CompletionChoice(
                   index=choice["index"],
                   text=choice["text"],
                   finish_reason=choice["finish_reason"],
               )
           ],
           usage=Usage(
               prompt_tokens=result["usage"]["prompt_tokens"],
               completion_tokens=result["usage"]["completion_tokens"],
               total_tokens=result["usage"]["total_tokens"],
           ),
       )
   ```

2. Update `backend/mlx_manager/mlx_server/api/v1/__init__.py`:

   Add the completions router:
   ```python
   """v1 API router."""
   from fastapi import APIRouter
   from mlx_manager.mlx_server.api.v1.models import router as models_router
   from mlx_manager.mlx_server.api.v1.chat import router as chat_router
   from mlx_manager.mlx_server.api.v1.completions import router as completions_router

   v1_router = APIRouter()
   v1_router.include_router(models_router)
   v1_router.include_router(chat_router)
   v1_router.include_router(completions_router)
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1.completions import router; print(router.routes)"`
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1 import v1_router; print([r.path for r in v1_router.routes])"`
  </verify>
  <done>Completions router created and wired up</done>
</task>

<task type="auto">
  <name>Task 3: Update services __init__.py exports</name>
  <files>
    backend/mlx_manager/mlx_server/services/__init__.py
  </files>
  <action>
Update services package to export all inference functions.

Update `backend/mlx_manager/mlx_server/services/__init__.py`:

```python
"""MLX Server services."""
from mlx_manager.mlx_server.services.inference import (
    generate_chat_completion,
    generate_completion,
)

__all__ = [
    "generate_chat_completion",
    "generate_completion",
]
```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.services import generate_completion; print('OK')"`
  </verify>
  <done>Services package exports both generation functions</done>
</task>

</tasks>

<verification>
1. Completions endpoint exists:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)

   response = client.post('/v1/completions', json={
       'model': 'test',
       'prompt': 'Hello, world'
   })
   print(f'Status: {response.status_code}')
   assert response.status_code != 404, 'Endpoint should exist'
   print('Completions endpoint exists OK')
   "
   ```

2. All v1 routes registered:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.api.v1 import v1_router

   paths = [r.path for r in v1_router.routes]
   print(f'Routes: {paths}')

   assert '/models' in paths or any('/models' in p for p in paths), 'Missing /models'
   assert '/chat/completions' in paths or any('chat' in p for p in paths), 'Missing /chat/completions'
   assert '/completions' in paths or any('completions' in p for p in paths), 'Missing /completions'
   print('All routes registered OK')
   "
   ```

3. Quality checks:
   ```bash
   cd backend && ruff check mlx_manager/mlx_server/ && ruff format --check mlx_manager/mlx_server/ && mypy mlx_manager/mlx_server/
   ```
</verification>

<success_criteria>
- [ ] /v1/completions endpoint accepts POST requests
- [ ] CompletionRequest schema validates input
- [ ] Streaming mode returns SSE events
- [ ] Non-streaming returns CompletionResponse
- [ ] Echo parameter prepends prompt to response
- [ ] Services package exports generate_completion
- [ ] All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-06-SUMMARY.md`
</output>
