---
phase: 07-foundation-server-skeleton
plan: 05
type: execute
wave: 3
depends_on: ["07-02", "07-03", "07-04"]
files_modified:
  - backend/mlx_manager/mlx_server/services/__init__.py
  - backend/mlx_manager/mlx_server/services/inference.py
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/mlx_manager/mlx_server/api/v1/__init__.py
autonomous: true

must_haves:
  truths:
    - "/v1/chat/completions endpoint accepts OpenAI-format requests"
    - "SSE streaming delivers tokens as they're generated"
    - "Non-streaming returns complete response in JSON"
    - "Stop tokens handled correctly for Llama models"
    - "Memory cleaned up after each request"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/inference.py"
      provides: "Inference orchestration"
      contains: "async def generate_chat_completion"
    - path: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      provides: "Chat completions endpoint"
      exports: ["router"]
  key_links:
    - from: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      to: "backend/mlx_manager/mlx_server/services/inference.py"
      via: "inference service"
      pattern: "from.*services.inference.*import"
    - from: "backend/mlx_manager/mlx_server/services/inference.py"
      to: "mlx_lm.stream_generate"
      via: "token generation"
      pattern: "stream_generate"
---

<objective>
Create the /v1/chat/completions endpoint with SSE streaming.

Purpose: This is the core endpoint that clients use for chat completion. It integrates the model pool, adapters, and inference logic. SSE streaming provides token-by-token delivery for responsive UX.

Output: Working /v1/chat/completions endpoint with streaming and non-streaming modes.
</objective>

<execution_context>
@.claude/agents/gsd-executor.md
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-foundation-server-skeleton/07-RESEARCH.md
@.planning/phases/07-foundation-server-skeleton/07-02-SUMMARY.md
@.planning/phases/07-foundation-server-skeleton/07-03-SUMMARY.md
@.planning/phases/07-foundation-server-skeleton/07-04-SUMMARY.md

# Critical patterns from research:
# - Pattern 2: SSE Streaming with Proper Format
# - Pattern 1: MLX Memory Management
# - Pitfall 3: Llama 3 Models Don't Stop Generating
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create inference service</name>
  <files>
    backend/mlx_manager/mlx_server/services/__init__.py
    backend/mlx_manager/mlx_server/services/inference.py
  </files>
  <action>
Create the inference service that orchestrates model loading, generation, and cleanup.

1. Create `backend/mlx_manager/mlx_server/services/__init__.py`:
   - Export inference functions

2. Create `backend/mlx_manager/mlx_server/services/inference.py`:

   ```python
   """Inference service for MLX model generation."""
   import asyncio
   import logging
   import time
   import uuid
   from collections.abc import AsyncGenerator

   logger = logging.getLogger(__name__)


   async def generate_chat_completion(
       model_id: str,
       messages: list[dict],
       max_tokens: int = 4096,
       temperature: float = 1.0,
       top_p: float = 1.0,
       stop: list[str] | None = None,
       stream: bool = False,
   ) -> AsyncGenerator[dict, None] | dict:
       """Generate a chat completion.

       Args:
           model_id: HuggingFace model ID
           messages: List of {"role": str, "content": str}
           max_tokens: Maximum tokens to generate
           temperature: Sampling temperature
           top_p: Nucleus sampling parameter
           stop: Additional stop strings
           stream: If True, yield chunks; if False, return complete response

       Yields/Returns:
           Streaming: yields chunk dicts
           Non-streaming: returns complete response dict
       """
       from mlx_manager.mlx_server.models.pool import get_model_pool
       from mlx_manager.mlx_server.models.adapters import get_adapter
       from mlx_manager.mlx_server.utils.memory import clear_cache

       # Get model from pool
       pool = get_model_pool()
       loaded = await pool.get_model(model_id)
       model = loaded.model
       tokenizer = loaded.tokenizer

       # Get adapter for model family
       adapter = get_adapter(model_id)

       # Apply chat template
       prompt = adapter.apply_chat_template(tokenizer, messages, add_generation_prompt=True)

       # Get stop tokens
       stop_tokens = set(adapter.get_stop_tokens(tokenizer))

       # Generate unique ID for this completion
       completion_id = f"chatcmpl-{uuid.uuid4().hex[:12]}"
       created = int(time.time())

       logger.info(f"Starting generation: {completion_id}, model={model_id}, max_tokens={max_tokens}")

       if stream:
           return _stream_generate(
               model=model,
               tokenizer=tokenizer,
               prompt=prompt,
               max_tokens=max_tokens,
               temperature=temperature,
               top_p=top_p,
               stop_tokens=stop_tokens,
               completion_id=completion_id,
               created=created,
               model_id=model_id,
           )
       else:
           return await _generate_complete(
               model=model,
               tokenizer=tokenizer,
               prompt=prompt,
               max_tokens=max_tokens,
               temperature=temperature,
               top_p=top_p,
               stop_tokens=stop_tokens,
               completion_id=completion_id,
               created=created,
               model_id=model_id,
           )


   async def _stream_generate(
       model,
       tokenizer,
       prompt: str,
       max_tokens: int,
       temperature: float,
       top_p: float,
       stop_tokens: set[int],
       completion_id: str,
       created: int,
       model_id: str,
   ) -> AsyncGenerator[dict, None]:
       """Generate tokens with streaming."""
       from mlx_lm import stream_generate
       from mlx_manager.mlx_server.utils.memory import clear_cache

       prompt_tokens = len(tokenizer.encode(prompt))
       completion_tokens = 0

       try:
           # First chunk with role
           yield {
               "id": completion_id,
               "object": "chat.completion.chunk",
               "created": created,
               "model": model_id,
               "choices": [{
                   "index": 0,
                   "delta": {"role": "assistant", "content": ""},
                   "finish_reason": None,
               }],
           }

           # Generate tokens in thread pool (mlx_lm is blocking)
           finish_reason = None

           # Wrap blocking generator in async
           def generate_sync():
               for response in stream_generate(
                   model,
                   tokenizer,
                   prompt,
                   max_tokens=max_tokens,
                   temp=temperature,
                   top_p=top_p,
               ):
                   yield response

           for response in await asyncio.to_thread(list, generate_sync()):
               # Check for stop token
               # Note: stream_generate response may vary by mlx-lm version
               # Adjust based on actual response object structure
               token_text = response.text if hasattr(response, 'text') else str(response)

               completion_tokens += 1

               # Yield content chunk
               yield {
                   "id": completion_id,
                   "object": "chat.completion.chunk",
                   "created": created,
                   "model": model_id,
                   "choices": [{
                       "index": 0,
                       "delta": {"content": token_text},
                       "finish_reason": None,
                   }],
               }

           finish_reason = "stop"

           # Final chunk
           yield {
               "id": completion_id,
               "object": "chat.completion.chunk",
               "created": created,
               "model": model_id,
               "choices": [{
                   "index": 0,
                   "delta": {},
                   "finish_reason": finish_reason,
               }],
           }

           logger.info(f"Generation complete: {completion_id}, tokens={completion_tokens}")

       finally:
           # Always cleanup
           clear_cache()


   async def _generate_complete(
       model,
       tokenizer,
       prompt: str,
       max_tokens: int,
       temperature: float,
       top_p: float,
       stop_tokens: set[int],
       completion_id: str,
       created: int,
       model_id: str,
   ) -> dict:
       """Generate complete response (non-streaming)."""
       from mlx_lm import generate
       from mlx_manager.mlx_server.utils.memory import clear_cache

       prompt_tokens = len(tokenizer.encode(prompt))

       try:
           # Generate in thread pool
           response = await asyncio.to_thread(
               generate,
               model,
               tokenizer,
               prompt,
               max_tokens=max_tokens,
               temp=temperature,
               top_p=top_p,
           )

           completion_tokens = len(tokenizer.encode(response))

           logger.info(f"Generation complete: {completion_id}, tokens={completion_tokens}")

           return {
               "id": completion_id,
               "object": "chat.completion",
               "created": created,
               "model": model_id,
               "choices": [{
                   "index": 0,
                   "message": {
                       "role": "assistant",
                       "content": response,
                   },
                   "finish_reason": "stop",
               }],
               "usage": {
                   "prompt_tokens": prompt_tokens,
                   "completion_tokens": completion_tokens,
                   "total_tokens": prompt_tokens + completion_tokens,
               },
           }

       finally:
           clear_cache()
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.services.inference import generate_chat_completion; print('OK')"`
  </verify>
  <done>Inference service imports successfully</done>
</task>

<task type="auto">
  <name>Task 2: Create /v1/chat/completions endpoint</name>
  <files>
    backend/mlx_manager/mlx_server/api/v1/chat.py
    backend/mlx_manager/mlx_server/api/v1/__init__.py
  </files>
  <action>
Create the chat completions endpoint with SSE streaming support.

1. Create `backend/mlx_manager/mlx_server/api/v1/chat.py`:

   ```python
   """Chat completions endpoint."""
   import json
   import logging

   from fastapi import APIRouter, HTTPException
   from fastapi.responses import StreamingResponse
   from sse_starlette.sse import EventSourceResponse

   from mlx_manager.mlx_server.schemas.openai import (
       ChatCompletionRequest,
       ChatCompletionResponse,
       ChatCompletionChoice,
       ChatMessage,
       Usage,
   )
   from mlx_manager.mlx_server.services.inference import generate_chat_completion

   logger = logging.getLogger(__name__)

   router = APIRouter(prefix="/v1", tags=["chat"])


   @router.post("/chat/completions")
   async def create_chat_completion(
       request: ChatCompletionRequest,
   ):
       """Create a chat completion.

       Supports both streaming and non-streaming responses.
       Compatible with OpenAI Chat Completions API.
       """
       logger.info(f"Chat completion request: model={request.model}, stream={request.stream}")

       # Convert messages to dict format
       messages = [{"role": m.role, "content": m.content} for m in request.messages]

       # Handle stop parameter (can be string or list)
       stop = request.stop if isinstance(request.stop, list) else ([request.stop] if request.stop else None)

       try:
           if request.stream:
               return await _handle_streaming(request, messages, stop)
           else:
               return await _handle_non_streaming(request, messages, stop)
       except RuntimeError as e:
           logger.error(f"Generation error: {e}")
           raise HTTPException(status_code=500, detail=str(e))
       except Exception as e:
           logger.exception(f"Unexpected error: {e}")
           raise HTTPException(status_code=500, detail="Internal server error")


   async def _handle_streaming(
       request: ChatCompletionRequest,
       messages: list[dict],
       stop: list[str] | None,
   ):
       """Handle streaming response."""

       async def event_generator():
           async for chunk in await generate_chat_completion(
               model_id=request.model,
               messages=messages,
               max_tokens=request.max_tokens or 4096,
               temperature=request.temperature,
               top_p=request.top_p,
               stop=stop,
               stream=True,
           ):
               # Format as SSE data
               yield {"data": json.dumps(chunk)}

           # Final [DONE] message
           yield {"data": "[DONE]"}

       return EventSourceResponse(event_generator())


   async def _handle_non_streaming(
       request: ChatCompletionRequest,
       messages: list[dict],
       stop: list[str] | None,
   ) -> ChatCompletionResponse:
       """Handle non-streaming response."""
       result = await generate_chat_completion(
           model_id=request.model,
           messages=messages,
           max_tokens=request.max_tokens or 4096,
           temperature=request.temperature,
           top_p=request.top_p,
           stop=stop,
           stream=False,
       )

       # Convert to Pydantic response model
       choice = result["choices"][0]
       return ChatCompletionResponse(
           id=result["id"],
           created=result["created"],
           model=result["model"],
           choices=[
               ChatCompletionChoice(
                   index=choice["index"],
                   message=ChatMessage(
                       role=choice["message"]["role"],
                       content=choice["message"]["content"],
                   ),
                   finish_reason=choice["finish_reason"],
               )
           ],
           usage=Usage(
               prompt_tokens=result["usage"]["prompt_tokens"],
               completion_tokens=result["usage"]["completion_tokens"],
               total_tokens=result["usage"]["total_tokens"],
           ),
       )
   ```

2. Update `backend/mlx_manager/mlx_server/api/v1/__init__.py`:

   Add the chat router:
   ```python
   """v1 API router."""
   from fastapi import APIRouter
   from mlx_manager.mlx_server.api.v1.models import router as models_router
   from mlx_manager.mlx_server.api.v1.chat import router as chat_router

   v1_router = APIRouter()
   v1_router.include_router(models_router)
   v1_router.include_router(chat_router)
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1.chat import router; print(router.routes)"`
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1 import v1_router; print([r.path for r in v1_router.routes])"`
  </verify>
  <done>Chat router imports and includes /v1/chat/completions route</done>
</task>

<task type="auto">
  <name>Task 3: Add LogFire span for inference</name>
  <files>
    backend/mlx_manager/mlx_server/services/inference.py
  </files>
  <action>
Add LogFire instrumentation to track inference requests.

Update `backend/mlx_manager/mlx_server/services/inference.py`:

1. Add import at top:
   ```python
   try:
       import logfire
       LOGFIRE_AVAILABLE = True
   except ImportError:
       LOGFIRE_AVAILABLE = False
   ```

2. Wrap generation in logfire span in `generate_chat_completion`:
   ```python
   async def generate_chat_completion(...):
       ...
       # After getting adapter, before generation
       span_context = None
       if LOGFIRE_AVAILABLE:
           span_context = logfire.span(
               'chat_completion',
               model=model_id,
               max_tokens=max_tokens,
               temperature=temperature,
               stream=stream,
           )
           span_context.__enter__()

       try:
           # ... existing generation code ...
           if stream:
               result = _stream_generate(...)
           else:
               result = await _generate_complete(...)
           return result
       finally:
           if span_context:
               span_context.__exit__(None, None, None)
   ```

3. Add token count logging in `_generate_complete`:
   ```python
   if LOGFIRE_AVAILABLE:
       logfire.info(
           'completion_finished',
           prompt_tokens=prompt_tokens,
           completion_tokens=completion_tokens,
           total_tokens=prompt_tokens + completion_tokens,
       )
   ```

Note: Keep LogFire optional - should work without it installed.
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.services.inference import generate_chat_completion; print('OK')"`
    - Verify code handles missing logfire gracefully
  </verify>
  <done>LogFire spans added for observability, graceful fallback when LogFire not available</done>
</task>

</tasks>

<verification>
1. Endpoint exists and validates input:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)

   # Check endpoint exists
   response = client.post('/v1/chat/completions', json={
       'model': 'test',
       'messages': [{'role': 'user', 'content': 'Hello'}]
   })
   # Will fail without model, but should not be 404
   print(f'Status: {response.status_code}')
   assert response.status_code != 404, 'Endpoint should exist'
   print('Endpoint exists OK')
   "
   ```

2. Schema validation works:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)

   # Invalid temperature
   response = client.post('/v1/chat/completions', json={
       'model': 'test',
       'messages': [{'role': 'user', 'content': 'Hello'}],
       'temperature': 5.0  # Invalid
   })
   print(f'Invalid temp status: {response.status_code}')
   assert response.status_code == 422, 'Should reject invalid temperature'
   print('Validation works OK')
   "
   ```

3. Quality checks:
   ```bash
   cd backend && ruff check mlx_manager/mlx_server/ && ruff format --check mlx_manager/mlx_server/ && mypy mlx_manager/mlx_server/
   ```
</verification>

<success_criteria>
- [ ] /v1/chat/completions endpoint accepts POST requests
- [ ] Pydantic validation rejects invalid parameters
- [ ] Inference service integrates model pool and adapters
- [ ] SSE streaming format matches OpenAI spec
- [ ] Non-streaming returns ChatCompletionResponse
- [ ] Memory cleanup happens in finally block
- [ ] LogFire spans track inference requests
- [ ] All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-05-SUMMARY.md`
</output>
