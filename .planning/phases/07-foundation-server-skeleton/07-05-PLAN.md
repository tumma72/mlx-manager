---
phase: 07-foundation-server-skeleton
plan: 05
type: execute
wave: 3
depends_on: ["07-02", "07-03", "07-04"]
files_modified:
  - backend/mlx_manager/mlx_server/services/__init__.py
  - backend/mlx_manager/mlx_server/services/inference.py
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/mlx_manager/mlx_server/api/v1/__init__.py
autonomous: true

# SCOPE NOTE: This plan has 3 tasks covering inference service + endpoint + LogFire.
# This is acceptable for an experienced executor but is at the upper limit of complexity.
# If issues arise during execution, consider deferring LogFire to a separate plan.

must_haves:
  truths:
    - "/v1/chat/completions endpoint accepts OpenAI-format requests"
    - "SSE streaming delivers tokens as they're generated"
    - "Non-streaming returns complete response in JSON"
    - "Stop tokens halt generation for Llama models (critical for preventing runaway generation)"
    - "Memory cleaned up after each request"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/inference.py"
      provides: "Inference orchestration with stop token detection"
      contains: "async def generate_chat_completion"
    - path: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      provides: "Chat completions endpoint"
      exports: ["router"]
  key_links:
    - from: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      to: "backend/mlx_manager/mlx_server/services/inference.py"
      via: "inference service"
      pattern: "from.*services.inference.*import"
    - from: "backend/mlx_manager/mlx_server/services/inference.py"
      to: "mlx_lm.stream_generate"
      via: "token generation with stop detection"
      pattern: "stream_generate"
    - from: "backend/mlx_manager/mlx_server/services/inference.py"
      to: "stop_tokens check in generation loop"
      via: "terminator detection"
      pattern: "if.*token.*in.*stop_tokens|terminators"
---

<objective>
Create the /v1/chat/completions endpoint with SSE streaming and proper stop token handling.

Purpose: This is the core endpoint that clients use for chat completion. It integrates the model pool, adapters, and inference logic. SSE streaming provides token-by-token delivery for responsive UX. **CRITICAL:** Stop token detection is essential to prevent Llama 3.x models from generating indefinitely.

Output: Working /v1/chat/completions endpoint with streaming and non-streaming modes, with verified stop token handling.
</objective>

<execution_context>
@.claude/agents/gsd-executor.md
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-foundation-server-skeleton/07-RESEARCH.md
@.planning/phases/07-foundation-server-skeleton/07-02-SUMMARY.md
@.planning/phases/07-foundation-server-skeleton/07-03-SUMMARY.md
@.planning/phases/07-foundation-server-skeleton/07-04-SUMMARY.md

# Critical patterns from research:
# - Pattern 2: SSE Streaming with Proper Format
# - Pattern 1: MLX Memory Management
# - Pattern 4: Dual Stop Token Detection (Llama 3.x) - CRITICAL
# - Pitfall 3: Llama 3 Models Don't Stop Generating - MUST PREVENT
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create inference service with stop token detection</name>
  <files>
    backend/mlx_manager/mlx_server/services/__init__.py
    backend/mlx_manager/mlx_server/services/inference.py
  </files>
  <action>
Create the inference service that orchestrates model loading, generation, and cleanup.

**CRITICAL:** `mlx_lm.stream_generate()` does NOT accept a `stop_tokens` parameter directly.
You MUST check the generated token against terminators in the generation loop.
Without this, Llama 3 models will generate indefinitely past the assistant's response.

1. Create `backend/mlx_manager/mlx_server/services/__init__.py`:
   - Export inference functions

2. Create `backend/mlx_manager/mlx_server/services/inference.py`:

   ```python
   """Inference service for MLX model generation.

   CRITICAL: This module implements stop token detection in the generation loop.
   mlx_lm's stream_generate() doesn't accept stop_tokens directly - we must
   check each generated token against the model's terminators to prevent
   Llama 3.x models from generating indefinitely.
   """
   import asyncio
   import logging
   import time
   import uuid
   from collections.abc import AsyncGenerator

   try:
       import logfire
       LOGFIRE_AVAILABLE = True
   except ImportError:
       LOGFIRE_AVAILABLE = False

   logger = logging.getLogger(__name__)


   async def generate_chat_completion(
       model_id: str,
       messages: list[dict],
       max_tokens: int = 4096,
       temperature: float = 1.0,
       top_p: float = 1.0,
       stop: list[str] | None = None,
       stream: bool = False,
   ) -> AsyncGenerator[dict, None] | dict:
       """Generate a chat completion.

       Args:
           model_id: HuggingFace model ID
           messages: List of {"role": str, "content": str}
           max_tokens: Maximum tokens to generate
           temperature: Sampling temperature
           top_p: Nucleus sampling parameter
           stop: Additional stop strings (user-provided)
           stream: If True, yield chunks; if False, return complete response

       Yields/Returns:
           Streaming: yields chunk dicts
           Non-streaming: returns complete response dict
       """
       from mlx_manager.mlx_server.models.pool import get_model_pool
       from mlx_manager.mlx_server.models.adapters import get_adapter
       from mlx_manager.mlx_server.utils.memory import clear_cache

       # Get model from pool
       pool = get_model_pool()
       loaded = await pool.get_model(model_id)
       model = loaded.model
       tokenizer = loaded.tokenizer

       # Get adapter for model family
       adapter = get_adapter(model_id)

       # Apply chat template
       prompt = adapter.apply_chat_template(tokenizer, messages, add_generation_prompt=True)

       # CRITICAL: Get stop token IDs from adapter
       # Llama 3.x requires BOTH eos_token_id AND <|eot_id|> (end of turn)
       # Without this, models will generate past the assistant's response
       stop_token_ids: set[int] = set(adapter.get_stop_tokens(tokenizer))
       logger.debug(f"Stop token IDs for {model_id}: {stop_token_ids}")

       # Generate unique ID for this completion
       completion_id = f"chatcmpl-{uuid.uuid4().hex[:12]}"
       created = int(time.time())

       logger.info(f"Starting generation: {completion_id}, model={model_id}, max_tokens={max_tokens}")

       # LogFire span for observability
       span_context = None
       if LOGFIRE_AVAILABLE:
           span_context = logfire.span(
               'chat_completion',
               model=model_id,
               max_tokens=max_tokens,
               temperature=temperature,
               stream=stream,
           )
           span_context.__enter__()

       try:
           if stream:
               return _stream_generate(
                   model=model,
                   tokenizer=tokenizer,
                   prompt=prompt,
                   max_tokens=max_tokens,
                   temperature=temperature,
                   top_p=top_p,
                   stop_token_ids=stop_token_ids,
                   completion_id=completion_id,
                   created=created,
                   model_id=model_id,
               )
           else:
               return await _generate_complete(
                   model=model,
                   tokenizer=tokenizer,
                   prompt=prompt,
                   max_tokens=max_tokens,
                   temperature=temperature,
                   top_p=top_p,
                   stop_token_ids=stop_token_ids,
                   completion_id=completion_id,
                   created=created,
                   model_id=model_id,
               )
       finally:
           if span_context:
               span_context.__exit__(None, None, None)


   async def _stream_generate(
       model,
       tokenizer,
       prompt: str,
       max_tokens: int,
       temperature: float,
       top_p: float,
       stop_token_ids: set[int],
       completion_id: str,
       created: int,
       model_id: str,
   ) -> AsyncGenerator[dict, None]:
       """Generate tokens with streaming.

       CRITICAL: This function checks each token against stop_token_ids
       to halt generation when a terminator is encountered.
       """
       from mlx_lm import stream_generate
       from mlx_manager.mlx_server.utils.memory import clear_cache

       prompt_tokens = len(tokenizer.encode(prompt))
       completion_tokens = 0

       try:
           # First chunk with role
           yield {
               "id": completion_id,
               "object": "chat.completion.chunk",
               "created": created,
               "model": model_id,
               "choices": [{
                   "index": 0,
                   "delta": {"role": "assistant", "content": ""},
                   "finish_reason": None,
               }],
           }

           finish_reason = "length"  # Default if we hit max_tokens

           # Stream generate and check tokens
           # mlx_lm stream_generate is blocking - run in thread pool
           def generate_with_stop_detection():
               """Generator that yields tokens until stop token or max_tokens."""
               tokens_generated = 0
               for response in stream_generate(
                   model,
                   tokenizer,
                   prompt,
                   max_tokens=max_tokens,
                   temp=temperature,
                   top_p=top_p,
               ):
                   # Get token ID from response
                   # Note: stream_generate response has .token attribute (int)
                   # and .text attribute (str) for the decoded token
                   token_id = getattr(response, 'token', None)
                   token_text = getattr(response, 'text', str(response))

                   tokens_generated += 1

                   # CRITICAL: Check for stop token BEFORE yielding
                   if token_id is not None and token_id in stop_token_ids:
                       # Found stop token - signal completion
                       yield (token_text, token_id, True)  # is_stop=True
                       return

                   yield (token_text, token_id, False)  # is_stop=False

           # Process tokens from generator
           loop = asyncio.get_event_loop()
           generator = generate_with_stop_detection()

           while True:
               try:
                   # Get next token in thread pool
                   result = await loop.run_in_executor(None, next, generator)
                   token_text, token_id, is_stop = result

                   completion_tokens += 1

                   if is_stop:
                       # Stop token found - don't yield the stop token text
                       finish_reason = "stop"
                       break

                   # Yield content chunk
                   yield {
                       "id": completion_id,
                       "object": "chat.completion.chunk",
                       "created": created,
                       "model": model_id,
                       "choices": [{
                           "index": 0,
                           "delta": {"content": token_text},
                           "finish_reason": None,
                       }],
                   }

               except StopIteration:
                   # Generator exhausted (max_tokens reached)
                   finish_reason = "length"
                   break

           # Final chunk with finish_reason
           yield {
               "id": completion_id,
               "object": "chat.completion.chunk",
               "created": created,
               "model": model_id,
               "choices": [{
                   "index": 0,
                   "delta": {},
                   "finish_reason": finish_reason,
               }],
           }

           logger.info(f"Generation complete: {completion_id}, tokens={completion_tokens}, reason={finish_reason}")

           if LOGFIRE_AVAILABLE:
               logfire.info(
                   'stream_completion_finished',
                   completion_id=completion_id,
                   completion_tokens=completion_tokens,
                   finish_reason=finish_reason,
               )

       finally:
           # Always cleanup
           clear_cache()


   async def _generate_complete(
       model,
       tokenizer,
       prompt: str,
       max_tokens: int,
       temperature: float,
       top_p: float,
       stop_token_ids: set[int],
       completion_id: str,
       created: int,
       model_id: str,
   ) -> dict:
       """Generate complete response (non-streaming).

       CRITICAL: Uses streaming internally to implement stop token detection,
       since mlx_lm.generate() doesn't support stop_tokens parameter.
       """
       from mlx_lm import stream_generate
       from mlx_manager.mlx_server.utils.memory import clear_cache

       prompt_tokens = len(tokenizer.encode(prompt))

       try:
           # Collect all generated text with stop token detection
           response_text = ""
           finish_reason = "length"

           def generate_with_stop_detection():
               """Generate tokens until stop token or max_tokens."""
               for response in stream_generate(
                   model,
                   tokenizer,
                   prompt,
                   max_tokens=max_tokens,
                   temp=temperature,
                   top_p=top_p,
               ):
                   token_id = getattr(response, 'token', None)
                   token_text = getattr(response, 'text', str(response))

                   # CRITICAL: Check for stop token
                   if token_id is not None and token_id in stop_token_ids:
                       yield (token_text, True)  # is_stop=True
                       return

                   yield (token_text, False)

           # Run generation in thread pool
           loop = asyncio.get_event_loop()
           generator = generate_with_stop_detection()

           while True:
               try:
                   result = await loop.run_in_executor(None, next, generator)
                   token_text, is_stop = result

                   if is_stop:
                       finish_reason = "stop"
                       break

                   response_text += token_text

               except StopIteration:
                   finish_reason = "length"
                   break

           completion_tokens = len(tokenizer.encode(response_text))

           logger.info(f"Generation complete: {completion_id}, tokens={completion_tokens}, reason={finish_reason}")

           if LOGFIRE_AVAILABLE:
               logfire.info(
                   'completion_finished',
                   completion_id=completion_id,
                   prompt_tokens=prompt_tokens,
                   completion_tokens=completion_tokens,
                   total_tokens=prompt_tokens + completion_tokens,
                   finish_reason=finish_reason,
               )

           return {
               "id": completion_id,
               "object": "chat.completion",
               "created": created,
               "model": model_id,
               "choices": [{
                   "index": 0,
                   "message": {
                       "role": "assistant",
                       "content": response_text,
                   },
                   "finish_reason": finish_reason,
               }],
               "usage": {
                   "prompt_tokens": prompt_tokens,
                   "completion_tokens": completion_tokens,
                   "total_tokens": prompt_tokens + completion_tokens,
               },
           }

       finally:
           clear_cache()
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.services.inference import generate_chat_completion; print('OK')"`
    - Verify stop token detection logic is present:
      `cd backend && grep -n "stop_token_ids" mlx_manager/mlx_server/services/inference.py`
    - Verify the critical check pattern exists:
      `cd backend && grep -n "if.*token.*in.*stop_token_ids" mlx_manager/mlx_server/services/inference.py`
  </verify>
  <done>Inference service imports, stop token detection logic is present in generation loops</done>
</task>

<task type="auto">
  <name>Task 2: Create /v1/chat/completions endpoint</name>
  <files>
    backend/mlx_manager/mlx_server/api/v1/chat.py
    backend/mlx_manager/mlx_server/api/v1/__init__.py
  </files>
  <action>
Create the chat completions endpoint with SSE streaming support.

1. Create `backend/mlx_manager/mlx_server/api/v1/chat.py`:

   ```python
   """Chat completions endpoint."""
   import json
   import logging

   from fastapi import APIRouter, HTTPException
   from fastapi.responses import StreamingResponse
   from sse_starlette.sse import EventSourceResponse

   from mlx_manager.mlx_server.schemas.openai import (
       ChatCompletionRequest,
       ChatCompletionResponse,
       ChatCompletionChoice,
       ChatMessage,
       Usage,
   )
   from mlx_manager.mlx_server.services.inference import generate_chat_completion

   logger = logging.getLogger(__name__)

   router = APIRouter(prefix="/v1", tags=["chat"])


   @router.post("/chat/completions")
   async def create_chat_completion(
       request: ChatCompletionRequest,
   ):
       """Create a chat completion.

       Supports both streaming and non-streaming responses.
       Compatible with OpenAI Chat Completions API.
       """
       logger.info(f"Chat completion request: model={request.model}, stream={request.stream}")

       # Convert messages to dict format
       messages = [{"role": m.role, "content": m.content} for m in request.messages]

       # Handle stop parameter (can be string or list)
       stop = request.stop if isinstance(request.stop, list) else ([request.stop] if request.stop else None)

       try:
           if request.stream:
               return await _handle_streaming(request, messages, stop)
           else:
               return await _handle_non_streaming(request, messages, stop)
       except RuntimeError as e:
           logger.error(f"Generation error: {e}")
           raise HTTPException(status_code=500, detail=str(e))
       except Exception as e:
           logger.exception(f"Unexpected error: {e}")
           raise HTTPException(status_code=500, detail="Internal server error")


   async def _handle_streaming(
       request: ChatCompletionRequest,
       messages: list[dict],
       stop: list[str] | None,
   ):
       """Handle streaming response."""

       async def event_generator():
           async for chunk in await generate_chat_completion(
               model_id=request.model,
               messages=messages,
               max_tokens=request.max_tokens or 4096,
               temperature=request.temperature,
               top_p=request.top_p,
               stop=stop,
               stream=True,
           ):
               # Format as SSE data
               yield {"data": json.dumps(chunk)}

           # Final [DONE] message
           yield {"data": "[DONE]"}

       return EventSourceResponse(event_generator())


   async def _handle_non_streaming(
       request: ChatCompletionRequest,
       messages: list[dict],
       stop: list[str] | None,
   ) -> ChatCompletionResponse:
       """Handle non-streaming response."""
       result = await generate_chat_completion(
           model_id=request.model,
           messages=messages,
           max_tokens=request.max_tokens or 4096,
           temperature=request.temperature,
           top_p=request.top_p,
           stop=stop,
           stream=False,
       )

       # Convert to Pydantic response model
       choice = result["choices"][0]
       return ChatCompletionResponse(
           id=result["id"],
           created=result["created"],
           model=result["model"],
           choices=[
               ChatCompletionChoice(
                   index=choice["index"],
                   message=ChatMessage(
                       role=choice["message"]["role"],
                       content=choice["message"]["content"],
                   ),
                   finish_reason=choice["finish_reason"],
               )
           ],
           usage=Usage(
               prompt_tokens=result["usage"]["prompt_tokens"],
               completion_tokens=result["usage"]["completion_tokens"],
               total_tokens=result["usage"]["total_tokens"],
           ),
       )
   ```

2. Update `backend/mlx_manager/mlx_server/api/v1/__init__.py`:

   Add the chat router:
   ```python
   """v1 API router."""
   from fastapi import APIRouter
   from mlx_manager.mlx_server.api.v1.models import router as models_router
   from mlx_manager.mlx_server.api.v1.chat import router as chat_router

   v1_router = APIRouter()
   v1_router.include_router(models_router)
   v1_router.include_router(chat_router)
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1.chat import router; print(router.routes)"`
    - `cd backend && python -c "from mlx_manager.mlx_server.api.v1 import v1_router; print([r.path for r in v1_router.routes])"`
  </verify>
  <done>Chat router imports and includes /v1/chat/completions route</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for stop token detection</name>
  <files>
    backend/tests/mlx_server/test_inference.py
  </files>
  <action>
Create unit tests that verify stop token detection logic WITHOUT requiring actual models.

This is critical because:
1. Actual inference testing requires MLX models (~1GB each)
2. CI environments may not have GPU/Apple Silicon
3. The stop token detection logic is the most critical part to verify

```python
"""Unit tests for inference service stop token detection.

NOTE: These tests verify the LOGIC of stop token detection without
requiring actual MLX models. For full inference testing:
- Run manually on Apple Silicon with a downloaded model
- Use `pytest tests/mlx_server/test_inference_integration.py`
"""
import pytest
from unittest.mock import Mock, patch, MagicMock
import asyncio


class TestStopTokenDetection:
    """Test stop token detection logic."""

    def test_stop_tokens_collected_from_adapter(self):
        """Verify stop tokens are retrieved from adapter."""
        # This test verifies the wiring, not the actual values
        from mlx_manager.mlx_server.models.adapters import get_adapter

        # Mock tokenizer with Llama 3 stop tokens
        mock_tokenizer = Mock()
        mock_tokenizer.eos_token_id = 128009  # <|end_of_text|>
        mock_tokenizer.convert_tokens_to_ids = Mock(return_value=128001)  # <|eot_id|>

        adapter = get_adapter("mlx-community/Llama-3.2-3B-Instruct-4bit")
        stop_tokens = adapter.get_stop_tokens(mock_tokenizer)

        assert 128009 in stop_tokens, "Should include eos_token_id"
        assert 128001 in stop_tokens, "Should include <|eot_id|>"

    def test_llama_adapter_returns_dual_stop_tokens(self):
        """Verify Llama adapter returns BOTH stop tokens (critical for Llama 3)."""
        from mlx_manager.mlx_server.models.adapters import LlamaAdapter

        adapter = LlamaAdapter()

        mock_tokenizer = Mock()
        mock_tokenizer.eos_token_id = 128009
        mock_tokenizer.convert_tokens_to_ids = Mock(return_value=128001)

        stop_tokens = adapter.get_stop_tokens(mock_tokenizer)

        assert len(stop_tokens) >= 2, "Llama adapter must return at least 2 stop tokens"
        assert 128009 in stop_tokens, "Must include eos_token_id"
        mock_tokenizer.convert_tokens_to_ids.assert_called_with("<|eot_id|>")

    def test_generation_halts_on_stop_token(self):
        """Verify generation stops when stop token is encountered."""
        # This tests the logic without actual generation
        stop_token_ids = {128009, 128001}

        # Simulate token stream: [token1, token2, STOP_TOKEN, token4_never_reached]
        token_stream = [
            (100, "Hello"),
            (200, " world"),
            (128001, ""),  # <|eot_id|> - stop token
            (300, " should not appear"),
        ]

        # Simulate our stop detection logic
        collected_text = ""
        hit_stop = False

        for token_id, text in token_stream:
            if token_id in stop_token_ids:
                hit_stop = True
                break
            collected_text += text

        assert hit_stop, "Should have detected stop token"
        assert collected_text == "Hello world", "Should stop BEFORE stop token text"
        assert "should not appear" not in collected_text


class TestInferenceServiceImports:
    """Verify inference service can be imported without model dependencies."""

    def test_inference_module_imports(self):
        """Inference service should import without errors."""
        # This verifies the module structure is correct
        from mlx_manager.mlx_server.services import inference
        assert hasattr(inference, 'generate_chat_completion')

    def test_logfire_optional(self):
        """LogFire should be optional - graceful fallback when not installed."""
        # Temporarily hide logfire
        import sys
        logfire_module = sys.modules.get('logfire')

        try:
            # Remove logfire from modules
            if 'logfire' in sys.modules:
                del sys.modules['logfire']

            # Force reimport
            from importlib import reload
            from mlx_manager.mlx_server.services import inference
            reload(inference)

            # Should not crash
            assert True
        finally:
            # Restore logfire if it was present
            if logfire_module:
                sys.modules['logfire'] = logfire_module
```

**Key testing notes:**
- Unit tests verify LOGIC without actual models
- Stop token detection is tested by simulating token streams
- Adapter wiring is tested with mocks
- For full integration testing with real models, create separate test file
  </action>
  <verify>
    - `cd backend && python -c "from tests.mlx_server.test_inference import TestStopTokenDetection; print('OK')"`
    - `cd backend && pytest tests/mlx_server/test_inference.py -v`
  </verify>
  <done>Unit tests exist for stop token detection logic, tests pass without requiring actual models</done>
</task>

</tasks>

<verification>
**NOTE ON INFERENCE TESTING:**
Full inference testing requires:
1. Apple Silicon Mac (M1/M2/M3/M4)
2. Downloaded MLX model (~1GB minimum)
3. Sufficient GPU memory

The verification below tests what CAN be verified without models.
For full inference testing, see manual verification section.

1. Endpoint exists and validates input:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)

   # Check endpoint exists
   response = client.post('/v1/chat/completions', json={
       'model': 'test',
       'messages': [{'role': 'user', 'content': 'Hello'}]
   })
   # Will fail without model, but should not be 404
   print(f'Status: {response.status_code}')
   assert response.status_code != 404, 'Endpoint should exist'
   print('Endpoint exists OK')
   "
   ```

2. Schema validation works:
   ```bash
   cd backend && python -c "
   from fastapi.testclient import TestClient
   from mlx_manager.mlx_server.main import app

   client = TestClient(app)

   # Invalid temperature
   response = client.post('/v1/chat/completions', json={
       'model': 'test',
       'messages': [{'role': 'user', 'content': 'Hello'}],
       'temperature': 5.0  # Invalid
   })
   print(f'Invalid temp status: {response.status_code}')
   assert response.status_code == 422, 'Should reject invalid temperature'
   print('Validation works OK')
   "
   ```

3. Stop token detection logic tests:
   ```bash
   cd backend && pytest tests/mlx_server/test_inference.py -v
   ```

4. Quality checks:
   ```bash
   cd backend && ruff check mlx_manager/mlx_server/ && ruff format --check mlx_manager/mlx_server/ && mypy mlx_manager/mlx_server/
   ```

**Manual verification (requires model):**
After plans 07-01 through 07-05 are complete:
```bash
# Start server
cd backend && python -m mlx_manager.mlx_server.main &

# Test with real model (requires downloaded model)
curl -X POST http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
    "messages": [{"role": "user", "content": "Say hello in one word."}],
    "max_tokens": 10
  }'

# Verify response stops cleanly (no extra content after answer)
# Verify streaming works
curl -X POST http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "mlx-community/Llama-3.2-3B-Instruct-4bit", "messages": [{"role": "user", "content": "Hi"}], "stream": true}'

kill %1
```
</verification>

<success_criteria>
- [ ] /v1/chat/completions endpoint accepts POST requests
- [ ] Pydantic validation rejects invalid parameters
- [ ] Inference service integrates model pool and adapters
- [ ] Stop token IDs retrieved from adapter (CRITICAL)
- [ ] Generation loop checks tokens against stop_token_ids (CRITICAL)
- [ ] SSE streaming format matches OpenAI spec
- [ ] Non-streaming returns ChatCompletionResponse
- [ ] Memory cleanup happens in finally block
- [ ] Unit tests verify stop token detection logic
- [ ] All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-05-SUMMARY.md`
</output>
