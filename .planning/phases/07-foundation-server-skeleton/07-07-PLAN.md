---
phase: 07-foundation-server-skeleton
plan: 07
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/mlx_manager/mlx_server/services/inference.py
  - backend/tests/mlx_server/test_inference.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "SSE streaming returns data: events with generated tokens"
    - "Non-streaming chat completions returns JSON response"
    - "Streaming completions returns SSE events"
    - "Non-streaming completions returns JSON response"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/inference.py"
      provides: "Queue-based threading for MLX Metal compatibility"
      contains: "asyncio.get_running_loop"
    - path: "backend/tests/mlx_server/test_inference.py"
      provides: "Tests for async pattern"
  key_links:
    - from: "inference.py"
      to: "mlx_lm.stream_generate"
      via: "dedicated thread with queue"
      pattern: "queue\\.put|Thread\\(target="
---

<objective>
Fix MLX Metal thread affinity issue in inference service that causes all inference endpoints to hang.

Purpose: UAT revealed that `run_in_executor` doesn't work with MLX Metal GPU operations due to thread affinity requirements. The thread pool workers cannot execute MLX operations, causing the executor to block indefinitely.

Output: Working inference for all 4 affected code paths (streaming/non-streaming for both chat and completions endpoints).
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-foundation-server-skeleton/07-UAT.md
@backend/mlx_manager/mlx_server/services/inference.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement queue-based threading pattern for MLX inference</name>
  <files>backend/mlx_manager/mlx_server/services/inference.py</files>
  <action>
Replace the broken `run_in_executor` pattern with a queue-based threading approach that respects MLX Metal thread affinity.

**Root cause:** MLX Metal GPU operations have thread affinity requirements. When `run_in_executor` dispatches `next(generator)` to the default ThreadPoolExecutor, the MLX operations try to run on a thread pool worker thread where Metal context isn't available, causing indefinite blocking.

**Solution:** Run the entire MLX generation in a dedicated thread that owns the Metal context. Use a queue to pass tokens back to the async generator.

**Implementation pattern for streaming functions (`_stream_chat_generate`, `_stream_completion`):**

```python
import threading
from queue import Queue, Empty

async def _stream_chat_generate(...) -> AsyncGenerator[dict, None]:
    # ... setup code unchanged ...

    # Queue for passing tokens from generation thread to async generator
    token_queue: Queue[tuple[str, int | None, bool] | None] = Queue()

    def run_generation():
        """Run MLX generation in dedicated thread (owns Metal context)."""
        try:
            for response in stream_generate(
                model, tokenizer, prompt,
                max_tokens=max_tokens, temp=temperature, top_p=top_p,
            ):
                token_id = getattr(response, "token", None)
                token_text = getattr(response, "text", str(response))

                # Check stop token
                is_stop = token_id is not None and token_id in stop_token_ids
                token_queue.put((token_text, token_id, is_stop))

                if is_stop:
                    return
        except Exception as e:
            # Put exception marker for async side to handle
            token_queue.put(e)
        finally:
            # Signal completion
            token_queue.put(None)

    # Start generation thread
    gen_thread = threading.Thread(target=run_generation, daemon=True)
    gen_thread.start()

    # Yield initial chunk
    yield { ... first chunk with role ... }

    finish_reason = "length"
    loop = asyncio.get_running_loop()  # Fix: use get_running_loop()

    while True:
        # Poll queue without blocking event loop
        try:
            result = await loop.run_in_executor(None, token_queue.get, True, 0.1)
        except Empty:
            continue

        # Check for completion signal
        if result is None:
            break

        # Check for exception
        if isinstance(result, Exception):
            raise result

        token_text, token_id, is_stop = result
        completion_tokens += 1

        if is_stop:
            finish_reason = "stop"
            break

        # Yield content chunk
        yield { ... chunk with token_text ... }

    # Wait for thread to finish
    gen_thread.join(timeout=1.0)

    # Yield final chunk
    yield { ... final chunk with finish_reason ... }
```

**Implementation pattern for non-streaming functions (`_generate_chat_complete`, `_generate_raw_completion`):**

```python
async def _generate_chat_complete(...) -> dict:
    # ... setup code unchanged ...

    result_queue: Queue[tuple[str, str] | Exception] = Queue()

    def run_generation():
        """Run complete generation in dedicated thread."""
        try:
            response_text = ""
            finish_reason = "length"

            for response in stream_generate(
                model, tokenizer, prompt,
                max_tokens=max_tokens, temp=temperature, top_p=top_p,
            ):
                token_id = getattr(response, "token", None)
                token_text = getattr(response, "text", str(response))

                if token_id is not None and token_id in stop_token_ids:
                    finish_reason = "stop"
                    break

                response_text += token_text

            result_queue.put((response_text, finish_reason))
        except Exception as e:
            result_queue.put(e)

    gen_thread = threading.Thread(target=run_generation, daemon=True)
    gen_thread.start()

    # Wait for result (with timeout to not block forever)
    loop = asyncio.get_running_loop()
    result = await loop.run_in_executor(None, result_queue.get, True, 300)  # 5 min timeout

    gen_thread.join(timeout=1.0)

    if isinstance(result, Exception):
        raise result

    response_text, finish_reason = result
    # ... rest of response building unchanged ...
```

**Critical changes:**
1. Replace `asyncio.get_event_loop()` with `asyncio.get_running_loop()` (deprecated API fix)
2. Run MLX generation in dedicated `threading.Thread` (not executor pool)
3. Use `Queue` to pass tokens/results back to async context
4. Use `run_in_executor` ONLY for the queue.get() call (which doesn't touch MLX)

**Apply this pattern to all 4 functions:**
- `_stream_chat_generate` (lines 120-254)
- `_generate_chat_complete` (lines 257-363)
- `_stream_completion` (lines 448-550)
- `_generate_raw_completion` (lines 553-637)

Add required imports at top of file:
```python
import threading
from queue import Queue, Empty
```
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate && ruff check mlx_manager/mlx_server/services/inference.py && mypy mlx_manager/mlx_server/services/inference.py --ignore-missing-imports
```
Verify no `asyncio.get_event_loop()` remains:
```bash
grep -n "get_event_loop" /Users/atomasini/Development/mlx-manager/backend/mlx_manager/mlx_server/services/inference.py
```
Should return nothing.
  </verify>
  <done>
- All 4 generation functions use queue-based threading pattern
- asyncio.get_running_loop() used instead of deprecated get_event_loop()
- MLX generation runs in dedicated thread (not executor pool)
- Queue used for thread-safe communication
- Code passes ruff and mypy checks
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for async threading pattern</name>
  <files>backend/tests/mlx_server/test_inference.py</files>
  <action>
Add tests to verify the async pattern works correctly. These tests mock MLX to test the threading logic without requiring actual models.

Add a new test class `TestAsyncThreadingPattern`:

```python
import asyncio
import threading
from queue import Queue, Empty
from unittest.mock import Mock, patch

class TestAsyncThreadingPattern:
    """Test the queue-based threading pattern for MLX inference."""

    def test_queue_based_token_passing(self) -> None:
        """Verify tokens can pass through queue from thread to async."""
        token_queue: Queue[tuple[str, int, bool] | None] = Queue()

        # Simulate generation thread
        def producer():
            token_queue.put(("Hello", 100, False))
            token_queue.put((" world", 200, False))
            token_queue.put(("", 128001, True))  # Stop token
            token_queue.put(None)  # Completion signal

        thread = threading.Thread(target=producer)
        thread.start()

        # Consume from queue
        tokens = []
        while True:
            result = token_queue.get(timeout=1.0)
            if result is None:
                break
            token_text, token_id, is_stop = result
            tokens.append((token_text, is_stop))
            if is_stop:
                break

        thread.join()

        assert tokens == [("Hello", False), (" world", False), ("", True)]

    def test_exception_propagation_through_queue(self) -> None:
        """Verify exceptions in generation thread propagate to async side."""
        result_queue: Queue[str | Exception] = Queue()

        def failing_producer():
            result_queue.put(RuntimeError("MLX error"))

        thread = threading.Thread(target=failing_producer)
        thread.start()
        thread.join()

        result = result_queue.get(timeout=1.0)
        assert isinstance(result, RuntimeError)
        assert str(result) == "MLX error"

    def test_empty_queue_timeout(self) -> None:
        """Verify Empty exception on queue timeout."""
        token_queue: Queue[str] = Queue()

        with pytest.raises(Empty):
            token_queue.get(timeout=0.01)

    def test_thread_daemon_flag(self) -> None:
        """Verify generation threads are daemon threads."""
        # Daemon threads don't prevent process exit
        thread = threading.Thread(target=lambda: None, daemon=True)
        assert thread.daemon is True


class TestDeprecatedAPIRemoval:
    """Test that deprecated APIs are not used."""

    def test_no_get_event_loop_usage(self) -> None:
        """Verify asyncio.get_event_loop() is not used (deprecated)."""
        import inspect
        from mlx_manager.mlx_server.services import inference

        source = inspect.getsource(inference)

        # get_event_loop is deprecated in async context
        assert "get_event_loop()" not in source, (
            "Should use asyncio.get_running_loop() instead of get_event_loop()"
        )

    def test_uses_get_running_loop(self) -> None:
        """Verify asyncio.get_running_loop() is used."""
        import inspect
        from mlx_manager.mlx_server.services import inference

        source = inspect.getsource(inference)

        assert "get_running_loop()" in source, (
            "Should use asyncio.get_running_loop() for current event loop"
        )
```

These tests verify:
1. Queue-based token passing works correctly
2. Exceptions propagate through the queue
3. Queue timeout behavior
4. Threads are daemon threads (won't block process exit)
5. Deprecated get_event_loop() is not used
6. Modern get_running_loop() is used
  </action>
  <verify>
```bash
cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate && pytest tests/mlx_server/test_inference.py -v
```
All tests should pass.
  </verify>
  <done>
- New test class TestAsyncThreadingPattern added
- New test class TestDeprecatedAPIRemoval added
- Tests verify queue-based communication pattern
- Tests verify no deprecated API usage
- All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Manual integration test</name>
  <files>backend/mlx_manager/mlx_server/services/inference.py</files>
  <action>
Run the MLX server and test all 4 inference paths manually to verify the fix works.

**Start the server:**
```bash
cd /Users/atomasini/Development/mlx-manager/backend
source .venv/bin/activate
export MLX_SERVER_AVAILABLE_MODELS="mlx-community/Llama-3.2-1B-Instruct-4bit"
uvicorn mlx_manager.mlx_server.main:app --port 8000
```

**Test 1 - Streaming chat completions:**
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "messages": [{"role": "user", "content": "Say hello in 5 words"}],
    "stream": true,
    "max_tokens": 50
  }'
```
Expected: SSE events with `data:` lines containing tokens, ending with `[DONE]`.

**Test 2 - Non-streaming chat completions:**
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "messages": [{"role": "user", "content": "Say hello in 5 words"}],
    "stream": false,
    "max_tokens": 50
  }'
```
Expected: JSON response with `choices[0].message.content`.

**Test 3 - Streaming completions:**
```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "prompt": "The capital of France is",
    "stream": true,
    "max_tokens": 20
  }'
```
Expected: SSE events with `data:` lines containing tokens.

**Test 4 - Non-streaming completions:**
```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "prompt": "The capital of France is",
    "stream": false,
    "max_tokens": 20
  }'
```
Expected: JSON response with `choices[0].text`.

If any test fails, check server logs for errors. The queue-based approach should show tokens being generated and passed through the queue.
  </action>
  <verify>
All 4 curl commands return valid responses:
1. Streaming chat: SSE events with tokens
2. Non-streaming chat: JSON with message content
3. Streaming completions: SSE events with tokens
4. Non-streaming completions: JSON with text
  </verify>
  <done>
- All 4 inference paths return valid responses
- No hanging/blocking behavior
- Stop token detection still works (generation stops at appropriate points)
- SSE streaming delivers tokens incrementally
  </done>
</task>

</tasks>

<verification>
**Code quality:**
```bash
cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate
ruff check mlx_manager/mlx_server/services/inference.py && ruff format --check mlx_manager/mlx_server/services/inference.py && mypy mlx_manager/mlx_server/services/inference.py --ignore-missing-imports
```

**Unit tests:**
```bash
cd /Users/atomasini/Development/mlx-manager/backend && source .venv/bin/activate
pytest tests/mlx_server/test_inference.py -v
```

**No deprecated API:**
```bash
grep -n "get_event_loop()" /Users/atomasini/Development/mlx-manager/backend/mlx_manager/mlx_server/services/inference.py
```
Should return nothing.

**Queue-based pattern in place:**
```bash
grep -n "Queue\|threading.Thread" /Users/atomasini/Development/mlx-manager/backend/mlx_manager/mlx_server/services/inference.py
```
Should show Queue and Thread usage.
</verification>

<success_criteria>
1. All 4 inference functions use queue-based threading (not run_in_executor with next())
2. asyncio.get_running_loop() used everywhere (no deprecated get_event_loop)
3. Unit tests pass for threading pattern
4. Manual testing confirms all 4 endpoints work:
   - POST /v1/chat/completions (stream=true) returns SSE events
   - POST /v1/chat/completions (stream=false) returns JSON
   - POST /v1/completions (stream=true) returns SSE events
   - POST /v1/completions (stream=false) returns JSON
5. Code passes ruff and mypy checks
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-07-SUMMARY.md`
</output>
