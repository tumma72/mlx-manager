---
phase: 07-foundation-server-skeleton
plan: 04
type: execute
wave: 2
depends_on: ["07-02"]
files_modified:
  - backend/mlx_manager/mlx_server/models/adapters/__init__.py
  - backend/mlx_manager/mlx_server/models/adapters/base.py
  - backend/mlx_manager/mlx_server/models/adapters/llama.py
  - backend/mlx_manager/mlx_server/models/adapters/registry.py
autonomous: true

must_haves:
  truths:
    - "ModelAdapter protocol defines interface for model-specific handling"
    - "LlamaAdapter handles Llama 3.x chat templates and stop tokens"
    - "Adapter registry maps model families to adapter instances"
    - "Llama adapter uses BOTH eos_token_id AND eot_id for stop detection"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/adapters/base.py"
      provides: "ModelAdapter protocol"
      contains: "class ModelAdapter"
    - path: "backend/mlx_manager/mlx_server/models/adapters/llama.py"
      provides: "Llama family adapter"
      contains: "class LlamaAdapter"
    - path: "backend/mlx_manager/mlx_server/models/adapters/registry.py"
      provides: "Adapter registry and detection"
      contains: "def get_adapter"
  key_links:
    - from: "backend/mlx_manager/mlx_server/models/adapters/llama.py"
      to: "backend/mlx_manager/mlx_server/models/adapters/base.py"
      via: "implements protocol"
      pattern: "ModelAdapter"
---

<objective>
Create the Model Adapter system with Llama 3.x support.

Purpose: Model adapters handle model-family-specific behaviors like chat templates, stop tokens, and tool parsing. The Llama adapter is critical for proper Llama 3.x generation (requires dual stop tokens). This modular design enables easy addition of Qwen, Mistral, Gemma adapters in Phase 8.

Output: ModelAdapter protocol + LlamaAdapter implementation + adapter registry.
</objective>

<execution_context>
@.claude/agents/gsd-executor.md
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-foundation-server-skeleton/07-RESEARCH.md

# Critical research findings:
# - Pattern 4: Dual Stop Token Detection (Llama 3.x)
# - Pitfall 3: Llama 3 Models Don't Stop Generating
# - Pitfall 4: Missing Chat Template Breaks Model Output
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ModelAdapter protocol and base implementation</name>
  <files>
    backend/mlx_manager/mlx_server/models/adapters/__init__.py
    backend/mlx_manager/mlx_server/models/adapters/base.py
  </files>
  <action>
Create the adapter protocol that defines the interface for model-specific handling.

1. Create `backend/mlx_manager/mlx_server/models/adapters/__init__.py`:
   - Export ModelAdapter, get_adapter

2. Create `backend/mlx_manager/mlx_server/models/adapters/base.py`:

   ```python
   """Model adapter base protocol and default implementation."""
   from typing import Protocol, runtime_checkable


   @runtime_checkable
   class ModelAdapter(Protocol):
       """Protocol for model-specific handling.

       Each model family (Llama, Qwen, Mistral, etc.) has specific:
       - Chat template formatting
       - Stop token configuration
       - Tool call parsing (future)
       - Thinking token handling (future)
       """

       @property
       def family(self) -> str:
           """Model family identifier (e.g., 'llama', 'qwen')."""
           ...

       def apply_chat_template(
           self,
           tokenizer,
           messages: list[dict],
           add_generation_prompt: bool = True,
       ) -> str:
           """Apply model-specific chat template.

           Args:
               tokenizer: HuggingFace tokenizer
               messages: List of {"role": str, "content": str} dicts
               add_generation_prompt: Whether to add assistant role marker

           Returns:
               Formatted prompt string
           """
           ...

       def get_stop_tokens(self, tokenizer) -> list[int]:
           """Get model-specific stop token IDs.

           Args:
               tokenizer: HuggingFace tokenizer

           Returns:
               List of token IDs that should stop generation
           """
           ...


   class DefaultAdapter:
       """Default adapter using tokenizer's built-in chat template.

       Used for unknown model families as a fallback.
       """

       @property
       def family(self) -> str:
           return "default"

       def apply_chat_template(
           self,
           tokenizer,
           messages: list[dict],
           add_generation_prompt: bool = True,
       ) -> str:
           """Use tokenizer's built-in chat template."""
           return tokenizer.apply_chat_template(
               messages,
               add_generation_prompt=add_generation_prompt,
               tokenize=False,
           )

       def get_stop_tokens(self, tokenizer) -> list[int]:
           """Return only the standard EOS token."""
           return [tokenizer.eos_token_id]
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.models.adapters.base import ModelAdapter, DefaultAdapter; print('OK')"`
    - `cd backend && python -c "from mlx_manager.mlx_server.models.adapters.base import DefaultAdapter; a = DefaultAdapter(); print(a.family)"`
  </verify>
  <done>ModelAdapter protocol and DefaultAdapter class import and work</done>
</task>

<task type="auto">
  <name>Task 2: Create LlamaAdapter for Llama 3.x models</name>
  <files>
    backend/mlx_manager/mlx_server/models/adapters/llama.py
  </files>
  <action>
Create the Llama adapter with proper dual stop token handling.

Create `backend/mlx_manager/mlx_server/models/adapters/llama.py`:

```python
"""Llama family model adapter.

Supports:
- Llama 3.x (Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, etc.)
- CodeLlama variants
- Meta Llama models

Critical: Llama 3 requires TWO stop tokens for proper chat completion:
1. eos_token_id (<|end_of_text|>)
2. <|eot_id|> (end of turn)

Without both, the model continues generating past the assistant's response.
"""
import logging

logger = logging.getLogger(__name__)


class LlamaAdapter:
    """Adapter for Llama family models."""

    @property
    def family(self) -> str:
        return "llama"

    def apply_chat_template(
        self,
        tokenizer,
        messages: list[dict],
        add_generation_prompt: bool = True,
    ) -> str:
        """Apply Llama chat template.

        Uses tokenizer's built-in template which handles:
        - <|begin_of_text|> prefix
        - <|start_header_id|>{role}<|end_header_id|> markers
        - <|eot_id|> after each message
        """
        return tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=add_generation_prompt,
            tokenize=False,
        )

    def get_stop_tokens(self, tokenizer) -> list[int]:
        """Get Llama 3 stop tokens.

        CRITICAL: Must include both:
        - eos_token_id: <|end_of_text|> (128009 for Llama 3)
        - <|eot_id|>: end of turn (128001 for Llama 3)

        The model signals end-of-message with <|eot_id|> but continues
        if only eos_token_id is checked.
        """
        stop_tokens = [tokenizer.eos_token_id]

        # Add <|eot_id|> for Llama 3.x
        try:
            eot_id = tokenizer.convert_tokens_to_ids("<|eot_id|>")
            if eot_id is not None and eot_id != tokenizer.unk_token_id:
                stop_tokens.append(eot_id)
                logger.debug(f"Llama adapter: added <|eot_id|> ({eot_id}) to stop tokens")
        except Exception as e:
            logger.warning(f"Could not get <|eot_id|> token: {e}")

        # Also check for end_of_turn if present (some variants)
        try:
            end_turn_id = tokenizer.convert_tokens_to_ids("<|end_of_turn|>")
            if end_turn_id is not None and end_turn_id != tokenizer.unk_token_id:
                if end_turn_id not in stop_tokens:
                    stop_tokens.append(end_turn_id)
        except Exception:
            pass  # Not all models have this token

        return stop_tokens

    def is_stop_token(self, token_id: int, tokenizer) -> bool:
        """Check if a token ID is a stop token.

        Convenience method for generation loop.
        """
        return token_id in self.get_stop_tokens(tokenizer)
```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.models.adapters.llama import LlamaAdapter; a = LlamaAdapter(); print(a.family)"`
  </verify>
  <done>LlamaAdapter imports and has correct family property</done>
</task>

<task type="auto">
  <name>Task 3: Create adapter registry with model family detection</name>
  <files>
    backend/mlx_manager/mlx_server/models/adapters/registry.py
    backend/mlx_manager/mlx_server/models/adapters/__init__.py
  </files>
  <action>
Create the adapter registry that maps model IDs to appropriate adapters.

1. Create `backend/mlx_manager/mlx_server/models/adapters/registry.py`:

   ```python
   """Model adapter registry and family detection."""
   import logging

   from mlx_manager.mlx_server.models.adapters.base import DefaultAdapter, ModelAdapter
   from mlx_manager.mlx_server.models.adapters.llama import LlamaAdapter

   logger = logging.getLogger(__name__)

   # Adapter instances (singletons)
   _ADAPTERS: dict[str, ModelAdapter] = {
       "llama": LlamaAdapter(),
       "default": DefaultAdapter(),
   }


   def detect_model_family(model_id: str) -> str:
       """Detect model family from HuggingFace model ID.

       Args:
           model_id: e.g., "mlx-community/Llama-3.2-3B-Instruct-4bit"

       Returns:
           Family name: "llama", "qwen", "mistral", "gemma", or "default"
       """
       model_id_lower = model_id.lower()

       # Llama family (including CodeLlama)
       if "llama" in model_id_lower or "codellama" in model_id_lower:
           return "llama"

       # Qwen family (Phase 8)
       if "qwen" in model_id_lower:
           return "qwen"

       # Mistral family (Phase 8)
       if "mistral" in model_id_lower or "mixtral" in model_id_lower:
           return "mistral"

       # Gemma family (Phase 8)
       if "gemma" in model_id_lower:
           return "gemma"

       # Phi family
       if "phi" in model_id_lower:
           return "phi"

       # Default fallback
       logger.info(f"Unknown model family for {model_id}, using default adapter")
       return "default"


   def get_adapter(model_id: str) -> ModelAdapter:
       """Get the appropriate adapter for a model.

       Args:
           model_id: HuggingFace model ID

       Returns:
           ModelAdapter instance for the model's family
       """
       family = detect_model_family(model_id)
       adapter = _ADAPTERS.get(family, _ADAPTERS["default"])
       logger.debug(f"Using {adapter.family} adapter for {model_id}")
       return adapter


   def register_adapter(family: str, adapter: ModelAdapter) -> None:
       """Register a custom adapter for a model family.

       Args:
           family: Family name (e.g., "qwen")
           adapter: ModelAdapter instance
       """
       _ADAPTERS[family] = adapter
       logger.info(f"Registered adapter for family: {family}")


   def get_supported_families() -> list[str]:
       """Get list of supported model families."""
       return list(_ADAPTERS.keys())
   ```

2. Update `backend/mlx_manager/mlx_server/models/adapters/__init__.py`:

   ```python
   """Model adapters for family-specific handling."""
   from mlx_manager.mlx_server.models.adapters.base import DefaultAdapter, ModelAdapter
   from mlx_manager.mlx_server.models.adapters.llama import LlamaAdapter
   from mlx_manager.mlx_server.models.adapters.registry import (
       detect_model_family,
       get_adapter,
       get_supported_families,
       register_adapter,
   )

   __all__ = [
       "ModelAdapter",
       "DefaultAdapter",
       "LlamaAdapter",
       "get_adapter",
       "detect_model_family",
       "register_adapter",
       "get_supported_families",
   ]
   ```
  </action>
  <verify>
    - `cd backend && python -c "from mlx_manager.mlx_server.models.adapters import get_adapter; a = get_adapter('mlx-community/Llama-3.2-3B-Instruct-4bit'); print(a.family)"`
    - `cd backend && python -c "from mlx_manager.mlx_server.models.adapters import detect_model_family; print(detect_model_family('mlx-community/Qwen2.5-7B-Instruct-4bit'))"`
  </verify>
  <done>
    - get_adapter("Llama...") returns LlamaAdapter
    - detect_model_family correctly identifies families
    - Default adapter returned for unknown models
  </done>
</task>

</tasks>

<verification>
1. Adapter protocol works:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.models.adapters import ModelAdapter, LlamaAdapter

   # Check LlamaAdapter implements protocol
   adapter = LlamaAdapter()
   assert isinstance(adapter, ModelAdapter), 'LlamaAdapter must implement ModelAdapter'
   print('Protocol check passed')
   "
   ```

2. Family detection works:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.models.adapters import detect_model_family, get_adapter

   # Test various model IDs
   tests = [
       ('mlx-community/Llama-3.2-3B-Instruct-4bit', 'llama'),
       ('mlx-community/CodeLlama-13B-Instruct-hf-4bit', 'llama'),
       ('mlx-community/Qwen2.5-7B-Instruct-4bit', 'qwen'),
       ('mlx-community/Mistral-7B-Instruct-v0.3-4bit', 'mistral'),
       ('mlx-community/gemma-2-9b-it-4bit', 'gemma'),
       ('mlx-community/Unknown-Model', 'default'),
   ]

   for model_id, expected in tests:
       actual = detect_model_family(model_id)
       assert actual == expected, f'{model_id}: expected {expected}, got {actual}'
       print(f'{model_id} -> {actual} OK')

   print('All family detection tests passed')
   "
   ```

3. Llama stop tokens:
   ```bash
   cd backend && python -c "
   from mlx_manager.mlx_server.models.adapters import LlamaAdapter

   adapter = LlamaAdapter()
   print(f'Family: {adapter.family}')
   # Note: Stop tokens require actual tokenizer, tested in integration
   print('Llama adapter initialized')
   "
   ```

4. Quality checks:
   ```bash
   cd backend && ruff check mlx_manager/mlx_server/models/adapters/ && ruff format --check mlx_manager/mlx_server/models/adapters/ && mypy mlx_manager/mlx_server/models/adapters/
   ```
</verification>

<success_criteria>
- [ ] ModelAdapter protocol defined with family, apply_chat_template, get_stop_tokens
- [ ] LlamaAdapter implements protocol with dual stop token support
- [ ] Registry maps model IDs to correct adapters
- [ ] detect_model_family works for llama, qwen, mistral, gemma
- [ ] Default adapter used for unknown models
- [ ] All quality checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-foundation-server-skeleton/07-04-SUMMARY.md`
</output>
