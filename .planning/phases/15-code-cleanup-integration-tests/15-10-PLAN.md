---
phase: 15-code-cleanup-integration-tests
plan: 10
type: execute
wave: 5
depends_on: ["15-03", "15-06"]
files_modified:
  - backend/pyproject.toml
  - backend/tests/conftest.py
  - backend/tests/fixtures/golden/vision/describe_image.txt
  - backend/tests/fixtures/golden/vision/compare_images.txt
  - backend/tests/fixtures/golden/vision/ocr_text.txt
  - backend/tests/fixtures/images/red_square.png
  - backend/tests/fixtures/images/blue_circle.png
  - backend/tests/fixtures/images/text_sample.png
  - backend/tests/e2e/conftest.py
  - backend/tests/e2e/test_vision_e2e.py
autonomous: false

must_haves:
  truths:
    - "E2E pytest marker infrastructure is configured"
    - "Vision golden prompt fixtures exist with paired test images"
    - "Vision E2E tests run against real Qwen2-VL-2B model (quick tier)"
    - "Vision E2E tests run against real Gemma-3-27b model (comprehensive tier)"
    - "Tests validate image description, multi-image comparison, and OCR"
    - "All E2E vision tests pass with downloaded models"
  artifacts:
    - path: "backend/tests/fixtures/golden/vision/"
      provides: "Golden prompt files for vision model testing"
    - path: "backend/tests/fixtures/images/"
      provides: "Test images for vision inference"
    - path: "backend/tests/e2e/test_vision_e2e.py"
      provides: "Parametrized E2E tests for vision models"
      exports: ["test_vision_describe_image", "test_vision_multi_image", "test_vision_ocr"]
  key_links:
    - from: "test_vision_e2e.py"
      to: "mlx_server/api/v1/chat.py"
      via: "HTTP POST /v1/chat/completions with image content"
      pattern: "httpx.AsyncClient"
---

<objective>
Create end-to-end integration tests for vision/multimodal models using golden prompt fixtures and real test images.

Purpose: Validate the complete vision inference pipeline — from HTTP request with image content through model detection, image preprocessing, mlx-vlm inference, and response generation — using actual downloaded models. This ensures the vision path works correctly in production, not just in mocked unit tests.

Output: A tiered E2E test suite with golden vision prompts and test images, runnable via `pytest -m e2e_vision` against pre-downloaded models.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/mlx_manager/mlx_server/api/v1/chat.py
@backend/mlx_manager/mlx_server/services/vision.py
@backend/mlx_manager/mlx_server/services/image_processor.py
@backend/mlx_manager/mlx_server/models/detection.py
@backend/tests/mlx_server/test_vision.py
</context>

<tasks>

<task type="interactive">
  <name>Task 1: Configure E2E pytest marker infrastructure</name>
  <files>
    backend/pyproject.toml
    backend/tests/e2e/conftest.py
  </files>
  <action>
    1. Add pytest markers to `backend/pyproject.toml` under `[tool.pytest.ini_options]`:
       ```toml
       markers = [
           "e2e: End-to-end tests requiring running server and downloaded models",
           "e2e_vision: Vision model E2E tests (requires vision models downloaded)",
           "e2e_vision_quick: Quick vision tests with small model (Qwen2-VL-2B)",
           "e2e_vision_full: Comprehensive vision tests with large model (Gemma-3-27b)",
           "e2e_anthropic: Anthropic API E2E tests",
           "e2e_embeddings: Embeddings E2E tests",
           "e2e_audio: Audio model E2E tests",
       ]
       ```

    2. Add default addopts to skip E2E by default (only run when explicitly requested):
       ```toml
       addopts = "-m 'not e2e'"
       ```

    3. Create `backend/tests/e2e/__init__.py` (empty).

    4. Create `backend/tests/e2e/conftest.py` with:
       ```python
       """E2E test configuration and fixtures.

       E2E tests require:
       - Models pre-downloaded to HuggingFace cache
       - MLX server running (or use the app_client fixture)

       Run with: pytest -m e2e_vision_quick
       Run all:  pytest -m e2e
       """

       import asyncio
       import pytest
       import httpx
       from pathlib import Path

       from mlx_manager.main import app
       from mlx_manager.mlx_server.models.pool import get_model_pool, reset_model_pool

       FIXTURES_DIR = Path(__file__).parent.parent / "fixtures"
       GOLDEN_DIR = FIXTURES_DIR / "golden"
       IMAGES_DIR = FIXTURES_DIR / "images"

       # Reference models for E2E testing
       VISION_MODEL_QUICK = "mlx-community/Qwen2-VL-2B-Instruct-4bit"
       VISION_MODEL_FULL = "mlx-community/gemma-3-27b-it-4bit-DWQ"


       def is_model_available(model_id: str) -> bool:
           """Check if a model is downloaded in HuggingFace cache."""
           try:
               from huggingface_hub import scan_cache_dir
               cache = scan_cache_dir()
               for repo in cache.repos:
                   if repo.repo_id == model_id:
                       return True
           except Exception:
               pass
           return False


       @pytest.fixture(scope="session")
       def vision_model_quick():
           """Return quick vision model ID, skip if not downloaded."""
           if not is_model_available(VISION_MODEL_QUICK):
               pytest.skip(f"Model {VISION_MODEL_QUICK} not downloaded")
           return VISION_MODEL_QUICK


       @pytest.fixture(scope="session")
       def vision_model_full():
           """Return full vision model ID, skip if not downloaded."""
           if not is_model_available(VISION_MODEL_FULL):
               pytest.skip(f"Model {VISION_MODEL_FULL} not downloaded")
           return VISION_MODEL_FULL


       @pytest.fixture(scope="session")
       def anyio_backend():
           return "asyncio"


       @pytest.fixture
       async def app_client():
           """Async HTTP client for the FastAPI app."""
           from httpx import ASGITransport
           async with httpx.AsyncClient(
               transport=ASGITransport(app=app),
               base_url="http://test",
               timeout=600.0,  # Vision inference can be slow
           ) as client:
               yield client


       @pytest.fixture(autouse=True)
       async def cleanup_pool():
           """Reset model pool after each test."""
           yield
           pool = get_model_pool()
           await pool.unload_all()
           reset_model_pool()
       ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    # Verify markers are registered
    pytest --markers | grep e2e
    # Verify default run skips e2e
    pytest --collect-only 2>&1 | grep -c "e2e"
    ```
  </verify>
  <done>E2E pytest marker infrastructure configured with model availability checks and app client fixture</done>
</task>

<task type="interactive">
  <name>Task 2: Create test images and vision golden prompt fixtures</name>
  <files>
    backend/tests/fixtures/images/red_square.png
    backend/tests/fixtures/images/blue_circle.png
    backend/tests/fixtures/images/text_sample.png
    backend/tests/fixtures/golden/vision/describe_image.txt
    backend/tests/fixtures/golden/vision/compare_images.txt
    backend/tests/fixtures/golden/vision/ocr_text.txt
  </files>
  <action>
    1. Create `backend/tests/fixtures/images/` directory.

    2. Generate simple test images programmatically (add a helper script or use PIL in conftest):
       - `red_square.png`: 256x256 red square on white background
       - `blue_circle.png`: 256x256 blue circle on white background
       - `text_sample.png`: 256x256 white image with "Hello MLX" text rendered

       Create `backend/tests/fixtures/images/generate_test_images.py`:
       ```python
       """Generate simple test images for E2E vision tests.

       Run once: python -m tests.fixtures.images.generate_test_images
       """
       from PIL import Image, ImageDraw, ImageFont
       from pathlib import Path

       OUT = Path(__file__).parent

       def generate():
           # Red square
           img = Image.new("RGB", (256, 256), "white")
           draw = ImageDraw.Draw(img)
           draw.rectangle([64, 64, 192, 192], fill="red")
           img.save(OUT / "red_square.png")

           # Blue circle
           img = Image.new("RGB", (256, 256), "white")
           draw = ImageDraw.Draw(img)
           draw.ellipse([64, 64, 192, 192], fill="blue")
           img.save(OUT / "blue_circle.png")

           # Text sample
           img = Image.new("RGB", (256, 256), "white")
           draw = ImageDraw.Draw(img)
           try:
               font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 36)
           except OSError:
               font = ImageFont.load_default()
           draw.text((40, 100), "Hello MLX", fill="black", font=font)
           img.save(OUT / "text_sample.png")

           print(f"Generated test images in {OUT}")

       if __name__ == "__main__":
           generate()
       ```

    3. Run the generator to create the images:
       ```bash
       cd backend && python -m tests.fixtures.images.generate_test_images
       ```

    4. Create `backend/tests/fixtures/golden/vision/` directory.

    5. Create golden prompt files (each file = one user prompt to send to the vision model):

       `describe_image.txt`:
       ```
       Describe this image in one sentence. What shape do you see and what color is it?
       ```

       `compare_images.txt`:
       ```
       Compare these two images. What are the differences in shape and color between them?
       ```

       `ocr_text.txt`:
       ```
       Read the text in this image. What does it say? Reply with only the text content.
       ```

    Golden prompt format notes:
    - Each file contains a single user prompt (no system prompt — that's configured in the test)
    - Prompts are designed to produce verifiable outputs (specific shapes, colors, text)
    - Short prompts encourage concise responses, keeping test execution fast
  </action>
  <verify>
    ```bash
    # Verify images exist and are valid PNGs
    file backend/tests/fixtures/images/*.png
    # Verify golden prompts
    cat backend/tests/fixtures/golden/vision/describe_image.txt
    cat backend/tests/fixtures/golden/vision/compare_images.txt
    cat backend/tests/fixtures/golden/vision/ocr_text.txt
    ```
  </verify>
  <done>Test images generated and vision golden prompt fixtures created with verifiable expected outputs</done>
</task>

<task type="interactive">
  <name>Task 3: Create tiered vision E2E test suite</name>
  <files>backend/tests/e2e/test_vision_e2e.py</files>
  <action>
    Create the E2E test file:

    ```python
    """End-to-end tests for vision/multimodal model inference.

    Tests validate the complete pipeline:
    1. HTTP request with base64 image content
    2. Model type detection (VISION)
    3. Image preprocessing (PIL conversion, resize)
    4. mlx-vlm inference (generate_vision_completion)
    5. Response generation (ChatCompletionResponse)

    Tiered approach:
    - Quick tier (@e2e_vision_quick): Qwen2-VL-2B (~1.5GB, fast)
    - Full tier (@e2e_vision_full): Gemma-3-27b (~15GB, thorough)

    Run:
      pytest -m e2e_vision_quick   # Quick tests only
      pytest -m e2e_vision_full    # Full tests only
      pytest -m e2e_vision         # All vision E2E
    """

    import base64
    import pytest
    from pathlib import Path

    FIXTURES_DIR = Path(__file__).parent.parent / "fixtures"
    GOLDEN_DIR = FIXTURES_DIR / "golden" / "vision"
    IMAGES_DIR = FIXTURES_DIR / "images"


    def load_image_base64(filename: str) -> str:
        """Load a test image as base64 data URI."""
        path = IMAGES_DIR / filename
        data = base64.b64encode(path.read_bytes()).decode()
        return f"data:image/png;base64,{data}"


    def load_golden_prompt(name: str) -> str:
        """Load a golden prompt fixture."""
        return (GOLDEN_DIR / f"{name}.txt").read_text().strip()


    def build_vision_request(
        model: str,
        prompt: str,
        image_urls: list[str],
        max_tokens: int = 256,
        stream: bool = False,
    ) -> dict:
        """Build an OpenAI-compatible chat completion request with images."""
        content = []
        for url in image_urls:
            content.append({"type": "image_url", "image_url": {"url": url}})
        content.append({"type": "text", "text": prompt})

        return {
            "model": model,
            "messages": [
                {"role": "system", "content": "You are a helpful vision assistant. Be concise."},
                {"role": "user", "content": content},
            ],
            "max_tokens": max_tokens,
            "stream": stream,
            "temperature": 0.1,  # Low temperature for deterministic outputs
        }


    # ──────────────────────────────────────────────
    # Quick tier: Qwen2-VL-2B-Instruct-4bit
    # ──────────────────────────────────────────────

    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_quick
    class TestVisionQuickDescribe:
        """Test single-image description with quick model."""

        @pytest.mark.anyio
        async def test_describe_red_square(self, app_client, vision_model_quick):
            """Vision model should describe a red square image."""
            prompt = load_golden_prompt("describe_image")
            image = load_image_base64("red_square.png")

            request = build_vision_request(vision_model_quick, prompt, [image])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            data = response.json()

            # Validate response structure
            assert data["object"] == "chat.completion"
            assert len(data["choices"]) == 1
            assert data["choices"][0]["message"]["role"] == "assistant"

            content = data["choices"][0]["message"]["content"].lower()
            # Model should mention red and/or square
            assert "red" in content or "square" in content, (
                f"Expected 'red' or 'square' in response, got: {content}"
            )

        @pytest.mark.anyio
        async def test_describe_blue_circle(self, app_client, vision_model_quick):
            """Vision model should describe a blue circle image."""
            prompt = load_golden_prompt("describe_image")
            image = load_image_base64("blue_circle.png")

            request = build_vision_request(vision_model_quick, prompt, [image])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            content = response.json()["choices"][0]["message"]["content"].lower()
            assert "blue" in content or "circle" in content, (
                f"Expected 'blue' or 'circle' in response, got: {content}"
            )


    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_quick
    class TestVisionQuickMultiImage:
        """Test multi-image comparison with quick model."""

        @pytest.mark.anyio
        async def test_compare_two_images(self, app_client, vision_model_quick):
            """Vision model should compare two different images."""
            prompt = load_golden_prompt("compare_images")
            img1 = load_image_base64("red_square.png")
            img2 = load_image_base64("blue_circle.png")

            request = build_vision_request(vision_model_quick, prompt, [img1, img2])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            content = response.json()["choices"][0]["message"]["content"].lower()
            # Should mention both shapes or both colors
            has_colors = "red" in content and "blue" in content
            has_shapes = "square" in content and "circle" in content
            assert has_colors or has_shapes, (
                f"Expected comparison mentioning both images, got: {content}"
            )


    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_quick
    class TestVisionQuickOCR:
        """Test OCR/text reading with quick model."""

        @pytest.mark.anyio
        async def test_read_text_from_image(self, app_client, vision_model_quick):
            """Vision model should read text from an image."""
            prompt = load_golden_prompt("ocr_text")
            image = load_image_base64("text_sample.png")

            request = build_vision_request(vision_model_quick, prompt, [image])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            content = response.json()["choices"][0]["message"]["content"]
            # Should contain "Hello" or "MLX" (case-insensitive)
            assert "hello" in content.lower() or "mlx" in content.lower(), (
                f"Expected 'Hello' or 'MLX' in OCR result, got: {content}"
            )


    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_quick
    class TestVisionQuickStreaming:
        """Test streaming vision responses with quick model."""

        @pytest.mark.anyio
        async def test_streaming_vision_response(self, app_client, vision_model_quick):
            """Vision model should stream response chunks via SSE."""
            prompt = load_golden_prompt("describe_image")
            image = load_image_base64("red_square.png")

            request = build_vision_request(
                vision_model_quick, prompt, [image], stream=True
            )
            async with app_client.stream(
                "POST", "/v1/chat/completions", json=request
            ) as response:
                assert response.status_code == 200

                chunks = []
                async for line in response.aiter_lines():
                    if line.startswith("data: ") and line != "data: [DONE]":
                        chunks.append(line[6:])

                # Should have received multiple chunks
                assert len(chunks) > 0, "Expected streaming chunks but got none"


    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_quick
    class TestVisionQuickErrorHandling:
        """Test error handling for vision requests."""

        @pytest.mark.anyio
        async def test_text_model_rejects_images(self, app_client):
            """Sending images to a text-only model should return 400."""
            image = load_image_base64("red_square.png")
            request = build_vision_request(
                "mlx-community/Qwen3-0.6B-4bit-DWQ",  # Text-only model
                "Describe this image",
                [image],
            )
            response = await app_client.post("/v1/chat/completions", json=request)
            assert response.status_code == 400


    # ──────────────────────────────────────────────
    # Full tier: Gemma-3-27b-it-4bit-DWQ
    # ──────────────────────────────────────────────

    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_full
    class TestVisionFullDescribe:
        """Test single-image description with full model (more accurate)."""

        @pytest.mark.anyio
        async def test_describe_red_square(self, app_client, vision_model_full):
            """Full model should accurately describe red square."""
            prompt = load_golden_prompt("describe_image")
            image = load_image_base64("red_square.png")

            request = build_vision_request(vision_model_full, prompt, [image])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            content = response.json()["choices"][0]["message"]["content"].lower()
            # Full model should identify BOTH color and shape
            assert "red" in content, f"Expected 'red' in response, got: {content}"
            assert "square" in content or "rectangle" in content, (
                f"Expected 'square' or 'rectangle' in response, got: {content}"
            )

        @pytest.mark.anyio
        async def test_describe_blue_circle(self, app_client, vision_model_full):
            """Full model should accurately describe blue circle."""
            prompt = load_golden_prompt("describe_image")
            image = load_image_base64("blue_circle.png")

            request = build_vision_request(vision_model_full, prompt, [image])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            content = response.json()["choices"][0]["message"]["content"].lower()
            assert "blue" in content, f"Expected 'blue' in response, got: {content}"
            assert "circle" in content or "round" in content, (
                f"Expected 'circle' or 'round' in response, got: {content}"
            )


    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_full
    class TestVisionFullOCR:
        """Test OCR with full model (higher accuracy expected)."""

        @pytest.mark.anyio
        async def test_read_text_accurately(self, app_client, vision_model_full):
            """Full model should read 'Hello MLX' text accurately."""
            prompt = load_golden_prompt("ocr_text")
            image = load_image_base64("text_sample.png")

            request = build_vision_request(vision_model_full, prompt, [image])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            content = response.json()["choices"][0]["message"]["content"]
            # Full model should get the complete text
            assert "Hello" in content and "MLX" in content, (
                f"Expected 'Hello MLX' in OCR result, got: {content}"
            )


    @pytest.mark.e2e
    @pytest.mark.e2e_vision
    @pytest.mark.e2e_vision_full
    class TestVisionFullMultiImage:
        """Test multi-image comparison with full model."""

        @pytest.mark.anyio
        async def test_detailed_comparison(self, app_client, vision_model_full):
            """Full model should provide detailed comparison of two images."""
            prompt = load_golden_prompt("compare_images")
            img1 = load_image_base64("red_square.png")
            img2 = load_image_base64("blue_circle.png")

            request = build_vision_request(vision_model_full, prompt, [img1, img2])
            response = await app_client.post("/v1/chat/completions", json=request)

            assert response.status_code == 200
            content = response.json()["choices"][0]["message"]["content"].lower()
            # Full model should mention all 4 attributes
            assert "red" in content, "Should mention red color"
            assert "blue" in content, "Should mention blue color"
            assert "square" in content or "rectangle" in content, "Should mention square shape"
            assert "circle" in content or "round" in content, "Should mention circle shape"
    ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    # Verify tests are collected with correct markers
    pytest tests/e2e/test_vision_e2e.py --collect-only -m e2e_vision_quick
    pytest tests/e2e/test_vision_e2e.py --collect-only -m e2e_vision_full

    # Run quick tier (requires Qwen2-VL-2B downloaded)
    pytest tests/e2e/test_vision_e2e.py -m e2e_vision_quick -v --timeout=600

    # Run full tier (requires Gemma-3-27b downloaded)
    pytest tests/e2e/test_vision_e2e.py -m e2e_vision_full -v --timeout=900
    ```
  </verify>
  <done>Tiered vision E2E test suite created with quick (Qwen2-VL-2B) and full (Gemma-3-27b) tiers</done>
</task>

</tasks>

<verification>
1. E2E markers registered:
   ```bash
   cd backend && pytest --markers | grep e2e_vision
   ```

2. Test images exist:
   ```bash
   file backend/tests/fixtures/images/*.png
   ```

3. Golden vision prompts exist:
   ```bash
   ls backend/tests/fixtures/golden/vision/*.txt
   ```

4. Tests collect correctly:
   ```bash
   pytest tests/e2e/test_vision_e2e.py --collect-only -m e2e_vision
   ```

5. Default test run excludes E2E:
   ```bash
   pytest --collect-only 2>&1 | grep -c "e2e"
   # Should be 0
   ```

6. Quick tier passes (with model):
   ```bash
   pytest -m e2e_vision_quick -v --timeout=600
   ```

7. Existing tests still pass:
   ```bash
   pytest -v
   ```
</verification>

<success_criteria>
- E2E pytest marker infrastructure configured (e2e, e2e_vision, e2e_vision_quick, e2e_vision_full)
- Default `pytest` run excludes E2E tests
- Test images generated (red_square, blue_circle, text_sample)
- Golden vision prompts created (describe_image, compare_images, ocr_text)
- Quick tier tests pass with Qwen2-VL-2B-Instruct-4bit
- Full tier tests pass with gemma-3-27b-it-4bit-DWQ
- Tests validate: single image description, multi-image comparison, OCR, streaming, error handling
- Model availability is checked at fixture level (graceful skip if not downloaded)
</success_criteria>

<output>
After completion, create `.planning/phases/15-code-cleanup-integration-tests/15-10-SUMMARY.md`
</output>
