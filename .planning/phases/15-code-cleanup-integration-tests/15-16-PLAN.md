---
phase: 15-code-cleanup-integration-tests
plan: 16
type: execute
wave: 6
depends_on: []
files_modified:
  - backend/mlx_manager/mlx_server/services/reasoning.py
  - backend/tests/mlx_server/test_reasoning.py
  - backend/mlx_manager/mlx_server/services/response_processor.py
  - backend/mlx_manager/mlx_server/schemas/openai.py
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/mlx_manager/mlx_server/models/adapters/base.py
  - backend/mlx_manager/mlx_server/models/adapters/qwen.py
  - backend/mlx_manager/mlx_server/models/adapters/llama.py
  - backend/mlx_manager/mlx_server/models/adapters/gemma.py
  - backend/mlx_manager/mlx_server/models/adapters/mistral.py
  - backend/mlx_manager/routers/chat.py
autonomous: false

must_haves:
  truths:
    - "Multi-turn tool use works end-to-end with Qwen3 models (no 'Can only get item pairs from a mapping' error)"
    - "Multi-turn tool use works end-to-end with Llama 3.x models"
    - "All message fields (role, content, tool_calls, tool_call_id) are preserved through API → service → adapter pipeline"
    - "Every adapter that returns supports_tool_calling()=True also overrides convert_messages()"
    - "There is exactly one ToolCall Pydantic model in the codebase (schemas/openai.py), no duplicates"
    - "ReasoningExtractor (services/reasoning.py) is deleted; its test coverage is migrated to ResponseProcessor tests"
    - "StreamingProcessor uses family-aware pattern configuration, not hardcoded class variables"
    - "All existing tests pass (pytest, mypy, ruff)"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/adapters/qwen.py"
      provides: "QwenAdapter.convert_messages() handling tool and assistant+tool_calls messages"
    - path: "backend/mlx_manager/mlx_server/models/adapters/llama.py"
      provides: "LlamaAdapter.convert_messages() handling tool and assistant+tool_calls messages"
    - path: "backend/mlx_manager/mlx_server/models/adapters/base.py"
      provides: "DefaultAdapter.convert_messages() with safe fallback for tool messages"
    - path: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      provides: "Full-fidelity message conversion preserving tool_calls, tool_call_id, role"
    - path: "backend/mlx_manager/routers/chat.py"
      provides: "Full-fidelity message conversion preserving tool_calls, tool_call_id, role"
    - path: "backend/mlx_manager/mlx_server/services/response_processor.py"
      provides: "Unified ToolCall using schemas/openai.py types; family-aware StreamingProcessor"
  key_links:
    - from: "api/v1/chat.py"
      to: "services/inference.py"
      via: "generate_chat_completion(messages=...)"
      pattern: "Messages include tool_calls and tool_call_id fields"
    - from: "services/inference.py"
      to: "models/adapters/*.py"
      via: "adapter.convert_messages(messages)"
      pattern: "Every tool-capable adapter transforms tool messages before template"
    - from: "services/response_processor.py"
      to: "schemas/openai.py"
      via: "from schemas.openai import ToolCall, FunctionCall"
      pattern: "Single canonical ToolCall type"

---

# 15-16: MLX Server Architecture Compliance

## Goal

Bring the MLX Server implementation into compliance with the architecture
blueprint (ARCHITECTURE.md). Fix bugs, remove dead code, eliminate
duplicates, and ensure consistent adapter contracts.

## Reference

All issues traced from `backend/mlx_manager/mlx_server/ARCHITECTURE.md`
principles:
- P1: Single inference pipeline
- P2: Adapter-driven model handling
- P3: No data loss through layers
- P4: One canonical type per concept
- P5: Shared infrastructure

---

## Issues (ordered by dependency)

### Issue 1: Dead Code — ReasoningExtractor
**Principle violated**: P5 (shared infrastructure — duplicated reasoning extraction)

`services/reasoning.py` contains `ReasoningExtractor` which duplicates the
thinking-tag extraction built into `ResponseProcessor`. Zero production
references — only its own test file imports it.

**Fix**:
- Delete `services/reasoning.py`
- Migrate test coverage from `tests/mlx_server/test_reasoning.py` to cover
  `ResponseProcessor.process()` thinking extraction
- Or delete tests if `ResponseProcessor` already has equivalent coverage

**Risk**: None. No production code references this module.

---

### Issue 2: Duplicate ToolCall Models
**Principle violated**: P4 (one canonical type per concept)

Two independent Pydantic hierarchies:
1. `response_processor.py`: `ToolCall` / `ToolCallFunction`
2. `schemas/openai.py`: `ToolCall` / `FunctionCall`

`_convert_tool_calls()` in `api/v1/chat.py` manually bridges between them.

**Fix**:
- Make `response_processor.py` import and use `ToolCall` / `FunctionCall`
  from `schemas/openai.py`
- Remove the duplicate `ToolCall` / `ToolCallFunction` from `response_processor.py`
- Remove `_convert_tool_calls()` bridging function from `api/v1/chat.py`
- Update `ParseResult` to use `schemas/openai.py::ToolCall`

**Risk**: Medium — touches response processor which is well-tested. Run full
test suite after change.

---

### Issue 3: Message Conversion Gap (BUG)
**Principle violated**: P3 (no data loss through layers)

`_handle_direct_request()` in `api/v1/chat.py` converts `ChatMessage` to
dict but drops `tool_calls`, `tool_call_id`, and `name` fields:

```python
# Current — loses tool context:
messages.append({"role": m.role, "content": text})
```

Same bug exists in `mlx_manager/routers/chat.py` (frontend chat endpoint).

**Fix**:
- Preserve all fields when converting `ChatMessage` → dict:
  `role`, `content`, `tool_calls` (as list of dicts), `tool_call_id`, `name`
- Apply the same fix to `mlx_manager/routers/chat.py`

**Risk**: Low — additive change, doesn't break existing text-only flow.

---

### Issue 4: Missing convert_messages() Overrides
**Principle violated**: P2 (adapter-driven model handling)

Only `GLM4Adapter` overrides `convert_messages()`. Qwen, Llama, Gemma, and
Mistral all inherit `DefaultAdapter.convert_messages()` which passes messages
through unchanged. When these messages contain `role: "tool"` or `tool_calls`,
the tokenizer's Jinja template crashes.

This is the **root cause** of the Qwen3-Coder tool use error.

**Fix**:
- Add `convert_messages()` to `QwenAdapter` using Hermes-style `<tool_call>` tags
- Add `convert_messages()` to `LlamaAdapter` using `<function=name>` tags
- Update `DefaultAdapter.convert_messages()` to safely handle tool messages
  as a fallback (convert `role: "tool"` → `role: "user"` with text formatting)
- Gemma and Mistral don't support tool calling, so they inherit the safe default

**Risk**: Medium — changes message format for all tool-capable models.
Must test with actual model inference.

---

### Issue 5: StreamingProcessor Hardcoded Patterns
**Principle violated**: P2 (adapter-driven), P5 (shared infrastructure)

`StreamingProcessor` has hardcoded class variables (`THINKING_STARTS`,
`TOOL_STARTS`, `PATTERN_ENDS`) that duplicate `MODEL_FAMILY_PATTERNS`.
The streaming phase uses the same markers for all families.

**Fix**:
- Make `StreamingProcessor` accept pattern configuration from
  `ModelFamilyPatterns` (which it already receives via `model_family` param)
- Replace hardcoded class variables with instance-level configuration
  derived from the family patterns
- Maintain backward compatibility for cases where no family is specified

**Risk**: Medium-High — streaming is latency-sensitive. Must verify no
regression in streaming behavior with thinking models.

---

### Issue 6: Thread Management Duplication (DEFERRED)
**Principle violated**: P5 (shared infrastructure)

The Queue+Thread+cleanup boilerplate is repeated ~8 times across
`inference.py`, `vision.py`, `embeddings.py`, `audio.py`.

**Fix**: Extract `utils/metal.py::run_on_metal_thread()` utility.

**Deferred**: This is a structural improvement with high blast radius.
Address after Issues 1-5 are resolved and validated.

---

## Execution Order

| Step | Issue | Estimated Impact | Dependencies |
|------|-------|-----------------|--------------|
| 1    | Issue 1: Delete ReasoningExtractor | Cleanup only | None |
| 2    | Issue 2: Unify ToolCall models | Internal refactor | None |
| 3    | Issue 3: Fix message conversion | Bug fix | None |
| 4    | Issue 4: Add convert_messages() | Bug fix | Issue 3 |
| 5    | Issue 5: Family-aware StreamingProcessor | Improvement | Issue 4 |
| 6    | Issue 6: Thread utility | Deferred | Issues 1-5 |

## Validation

After each step:
1. `ruff check . && ruff format --check .`
2. `mypy mlx_manager`
3. `pytest -v` (unit tests)
4. Manual test: Qwen3-Coder tool use multi-turn (after step 4)
