# Plan 15-05: Fix Memory Metrics & Stop Button

## Objective

Fix server tile metrics to show correct per-model memory usage and implement actual model unload on stop button click.

## Problem Statement

### Memory Metrics (Gap 3)
Current code divides total memory by model count:
```python
memory_mb=memory_used_gb * 1024 / max(1, len(loaded_models))  # WRONG
```

We have per-model tracking (`LoadedModel.size_gb`) but don't use it.

### Stop Button (Gap 4)
Returns success message but does nothing:
```python
return {"success": True, "message": "Embedded server cannot be stopped..."}
```

## Tasks

### Task 1: Fix per-model memory display

**File:** `backend/mlx_manager/routers/servers.py`

Replace line 125 calculation:

```python
# Current (wrong):
memory_mb=memory_used_gb * 1024 / max(1, len(loaded_models)),

# Fixed:
memory_mb=loaded_model.size_gb * 1024 if loaded_model else 0.0,
```

Update memory_percent to use model's size against total:
```python
memory_percent=(
    (loaded_model.size_gb / memory_total_gb * 100)
    if memory_total_gb > 0 and loaded_model
    else 0.0
),
```

### Task 2: Add memory limit percentage to response

**File:** `backend/mlx_manager/routers/servers.py`

Add field to track memory vs configured limit:

```python
# In RunningServer model or response
memory_limit_percent: float  # Percentage of max_memory_gb used by this model
```

Calculate:
```python
memory_limit_percent=(
    (loaded_model.size_gb / pool.max_memory_gb * 100)
    if pool.max_memory_gb > 0 and loaded_model
    else 0.0
),
```

### Task 3: Implement actual model unload on stop

**File:** `backend/mlx_manager/routers/servers.py`

Replace the no-op stop endpoint with actual unload:

```python
@router.post("/{profile_id}/stop")
async def stop_server(
    profile_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> dict:
    """Unload model associated with profile from memory."""
    # Get profile
    profile = await db.get(ServerProfile, profile_id)
    if not profile:
        raise HTTPException(status_code=404, detail="Profile not found")

    try:
        from mlx_manager.mlx_server.models.pool import get_model_pool

        pool = get_model_pool()
        model_id = profile.model_path

        # Check if model is loaded
        if model_id not in pool.get_loaded_models():
            return {
                "success": True,
                "message": f"Model {model_id} is not currently loaded"
            }

        # Check if model is preloaded (protected)
        loaded = pool._models.get(model_id)
        if loaded and loaded.preloaded:
            return {
                "success": False,
                "message": f"Model {model_id} is preloaded and protected from unload"
            }

        # Unload the model
        await pool.unload(model_id)

        return {
            "success": True,
            "message": f"Model {model_id} unloaded successfully"
        }
    except Exception as e:
        logger.error(f"Failed to unload model: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### Task 4: Add unload method to ModelPoolManager if missing

**File:** `backend/mlx_manager/mlx_server/models/pool.py`

Verify `unload()` method exists and works correctly. If not, add it:

```python
async def unload(self, model_id: str) -> bool:
    """Unload a specific model from memory.

    Args:
        model_id: HuggingFace model ID to unload

    Returns:
        True if model was unloaded, False if not loaded
    """
    async with self._lock:
        if model_id not in self._models:
            return False

        loaded = self._models[model_id]
        if loaded.preloaded:
            logger.warning(f"Cannot unload preloaded model: {model_id}")
            return False

        del self._models[model_id]
        logger.info(f"Unloaded model: {model_id}")

        # Clear MLX cache
        from mlx_manager.mlx_server.utils.memory import clear_cache
        clear_cache()

        return True
```

### Task 5: Update frontend ServerTile to show memory limit gauge

**File:** `frontend/src/lib/components/servers/ServerTile.svelte`

Add second memory gauge showing usage vs configured limit:
- First gauge: Memory % of system total (existing)
- Second gauge: Memory % of configured limit (new)

### Task 6: Update RunningServer model

**File:** `backend/mlx_manager/models.py`

Add `memory_limit_percent` field to `RunningServer`:

```python
class RunningServer(BaseModel):
    # ... existing fields
    memory_limit_percent: float = 0.0  # Memory as % of configured limit
```

## Success Criteria

1. [ ] Each server tile shows its actual model memory usage
2. [ ] Memory percentage is per-model, not divided total
3. [ ] Second gauge shows memory vs configured limit
4. [ ] Stop button actually unloads model from memory
5. [ ] Preloaded models show message that they can't be unloaded
6. [ ] Memory freed when model unloaded (verified via system metrics)

## Dependencies

- Pool must have working unload method

## Estimated Complexity

Medium - Bug fixes and straightforward feature additions

---

*Plan: 15-05*
*Phase: 15-code-cleanup-integration-tests*
*Priority: MEDIUM-HIGH*
