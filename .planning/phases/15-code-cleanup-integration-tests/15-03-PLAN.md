---
phase: 15-code-cleanup-integration-tests
plan: 03
type: execute
wave: 2
depends_on: ["15-01"]
files_modified:
  - backend/tests/fixtures/golden/qwen/tool_calls.txt
  - backend/tests/fixtures/golden/qwen/thinking.txt
  - backend/tests/fixtures/golden/qwen/stream/thinking_chunks.txt
  - backend/tests/fixtures/golden/llama/tool_calls.txt
  - backend/tests/fixtures/golden/llama/python_tag.txt
  - backend/tests/fixtures/golden/glm4/tool_calls.txt
  - backend/tests/fixtures/golden/glm4/duplicate_tools.txt
  - backend/tests/fixtures/golden/hermes/tool_calls.txt
  - backend/tests/fixtures/golden/minimax/tool_calls.txt
  - backend/tests/fixtures/golden/gemma/tool_calls.txt
  - backend/tests/mlx_server/test_response_processor_golden.py
autonomous: true

must_haves:
  truths:
    - "Golden files exist for all model families"
    - "ResponseProcessor correctly extracts tool calls from all formats"
    - "StreamingProcessor filters patterns during streaming"
    - "All golden file tests pass"
  artifacts:
    - path: "backend/tests/fixtures/golden/qwen/"
      provides: "Golden files for Qwen/Hermes-style outputs"
    - path: "backend/tests/fixtures/golden/llama/"
      provides: "Golden files for Llama XML-style outputs"
    - path: "backend/tests/fixtures/golden/glm4/"
      provides: "Golden files for GLM4 XML-style outputs"
    - path: "backend/tests/mlx_server/test_response_processor_golden.py"
      provides: "Parametrized integration tests"
      exports: ["test_response_processor_golden", "test_streaming_processor_golden"]
  key_links:
    - from: "test_response_processor_golden.py"
      to: "response_processor.py"
      via: "get_response_processor() and StreamingProcessor"
      pattern: "from.*response_processor import"
---

<objective>
Create golden file integration tests for ResponseProcessor and StreamingProcessor.

Purpose: Validate that the ResponseProcessor-based architecture correctly handles tool calls and thinking extraction for ALL model families. Golden files capture real model output patterns, ensuring the parsers work correctly without requiring live model inference at test time.

Output: Comprehensive golden file test suite with parametrized tests covering all model families and output formats.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-code-cleanup-integration-tests/15-RESEARCH.md

@backend/mlx_manager/mlx_server/services/response_processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create golden file directory structure and tool call golden files</name>
  <files>
    backend/tests/fixtures/golden/qwen/tool_calls.txt
    backend/tests/fixtures/golden/llama/tool_calls.txt
    backend/tests/fixtures/golden/llama/python_tag.txt
    backend/tests/fixtures/golden/glm4/tool_calls.txt
    backend/tests/fixtures/golden/glm4/duplicate_tools.txt
    backend/tests/fixtures/golden/hermes/tool_calls.txt
    backend/tests/fixtures/golden/minimax/tool_calls.txt
    backend/tests/fixtures/golden/gemma/tool_calls.txt
  </files>
  <action>
    1. Create directory structure:
       ```
       backend/tests/fixtures/golden/
       ├── qwen/
       ├── llama/
       ├── glm4/
       ├── hermes/
       ├── minimax/
       └── gemma/
       ```

    2. Create Hermes/Qwen-style tool call golden file (qwen/tool_calls.txt):
       ```
       I'll help you search for that information.
       <tool_call>{"name": "search", "arguments": {"query": "weather in London"}}</tool_call>
       Let me check that for you.
       ```

    3. Create Llama XML-style tool call golden file (llama/tool_calls.txt):
       ```
       I can help you with that calculation.
       <function=calculate>{"expression": "2 + 2"}</function>
       The answer is ready.
       ```

    4. Create Llama Python tag golden file (llama/python_tag.txt):
       ```
       Let me search for that.
       <|python_tag|>search.web(query="weather forecast")<|eom_id|>
       Found results.
       ```

    5. Create GLM4 XML-style tool call golden file (glm4/tool_calls.txt):
       ```
       I'll look that up.
       <tool_call><name>lookup</name><arguments>{"key": "value"}</arguments></tool_call>
       Here's what I found.
       ```

    6. Create GLM4 duplicate tool call golden file (glm4/duplicate_tools.txt):
       ```
       Running search.
       <tool_call><name>search</name><arguments>{"query": "test"}</arguments></tool_call>
       <tool_call><name>search</name><arguments>{"query": "test"}</arguments></tool_call>
       Done (should deduplicate to 1 tool call).
       ```

    7. Create Hermes style (same as Qwen, hermes/tool_calls.txt):
       ```
       Let me help.
       <tool_call>{"name": "get_weather", "arguments": {"city": "Paris"}}</tool_call>
       Done.
       ```

    8. Create MiniMax style (similar to Hermes, minimax/tool_calls.txt):
       ```
       Checking that now.
       <tool_call>{"name": "check_status", "arguments": {"id": 123}}</tool_call>
       Status retrieved.
       ```

    9. Create Gemma style (similar to Hermes, gemma/tool_calls.txt):
       ```
       I'll fetch that data.
       <tool_call>{"name": "fetch_data", "arguments": {"endpoint": "/api/users"}}</tool_call>
       Data fetched.
       ```

    Golden file format notes:
    - Each file contains a complete model response with tool calls embedded
    - Content before and after tool calls tests that surrounding text is preserved
    - Tool call markers should be completely removed from content
  </action>
  <verify>
    ```bash
    # Verify directory structure
    ls -la backend/tests/fixtures/golden/
    # Verify files exist
    ls backend/tests/fixtures/golden/*/tool_calls.txt
    # Verify content
    cat backend/tests/fixtures/golden/qwen/tool_calls.txt
    ```
  </verify>
  <done>Golden file directory structure created with tool call golden files for all 6 model families</done>
</task>

<task type="auto">
  <name>Task 2: Create thinking/streaming golden files</name>
  <files>
    backend/tests/fixtures/golden/qwen/thinking.txt
    backend/tests/fixtures/golden/qwen/stream/thinking_chunks.txt
    backend/tests/fixtures/golden/qwen/stream/tool_call_chunks.txt
  </files>
  <action>
    1. Create qwen/thinking.txt (complete response with thinking tags):
       ```
       <think>
       Let me analyze this step by step:
       1. First, I need to understand the question
       2. Then formulate a clear answer
       </think>
       Based on my analysis, the answer is 42.
       ```

    2. Create stream subdirectory:
       ```
       mkdir -p backend/tests/fixtures/golden/qwen/stream/
       ```

    3. Create qwen/stream/thinking_chunks.txt (streaming chunks, one per line):
       ```
       <think>Let me
        think about
        this problem</think> The
        answer is 42.
       ```
       Note: Each line represents a token/chunk from streaming. The streaming processor
       should filter out <think>...</think> content, yielding only "The answer is 42."

    4. Create qwen/stream/tool_call_chunks.txt (tool call split across chunks):
       ```
       I'll search for
        that. <tool_call>{"name":
        "search", "arguments": {"query":
        "test"}}</tool_call> Done
        searching.
       ```
       Note: Tool call markers should be filtered, yielding "I'll search for that. Done searching."

    Streaming golden file format:
    - Each line = one streaming chunk
    - Patterns may span multiple chunks (partial markers)
    - StreamingProcessor must buffer and filter correctly
  </action>
  <verify>
    ```bash
    # Verify stream directory and files
    ls -la backend/tests/fixtures/golden/qwen/stream/
    cat backend/tests/fixtures/golden/qwen/thinking.txt
    cat backend/tests/fixtures/golden/qwen/stream/thinking_chunks.txt
    ```
  </verify>
  <done>Thinking and streaming golden files created for testing extraction and pattern filtering</done>
</task>

<task type="auto">
  <name>Task 3: Create parametrized integration test file</name>
  <files>backend/tests/mlx_server/test_response_processor_golden.py</files>
  <action>
    Create test file with:

    ```python
    """Golden file integration tests for ResponseProcessor and StreamingProcessor.

    Tests validate that:
    1. Tool call markers are extracted and removed from content
    2. Thinking/reasoning tags are extracted and removed
    3. Streaming processor filters patterns across chunk boundaries
    4. All model family formats are correctly parsed
    """

    import pytest
    from pathlib import Path

    from mlx_manager.mlx_server.services.response_processor import (
        get_response_processor,
        StreamingProcessor,
        reset_response_processor,
    )

    GOLDEN_DIR = Path(__file__).parent.parent / "fixtures" / "golden"


    def collect_tool_call_files():
        """Collect all tool call golden files for parametrization."""
        test_cases = []
        for family_dir in GOLDEN_DIR.iterdir():
            if not family_dir.is_dir():
                continue
            tool_file = family_dir / "tool_calls.txt"
            if tool_file.exists():
                test_cases.append((family_dir.name, tool_file))
        return test_cases


    def collect_streaming_files():
        """Collect all streaming golden files for parametrization."""
        test_cases = []
        for family_dir in GOLDEN_DIR.iterdir():
            if not family_dir.is_dir():
                continue
            stream_dir = family_dir / "stream"
            if stream_dir.exists():
                for chunk_file in stream_dir.glob("*.txt"):
                    test_cases.append((family_dir.name, chunk_file.stem, chunk_file))
        return test_cases


    @pytest.fixture(autouse=True)
    def reset_processor():
        """Reset singleton between tests."""
        reset_response_processor()
        yield
        reset_response_processor()


    class TestResponseProcessorToolCalls:
        """Test tool call extraction from golden files."""

        @pytest.mark.parametrize("family,golden_file", collect_tool_call_files())
        def test_tool_calls_extracted(self, family: str, golden_file: Path):
            """Verify tool calls are extracted from {family} format."""
            text = golden_file.read_text()
            processor = get_response_processor()
            result = processor.process(text)

            # Tool calls should be extracted
            assert len(result.tool_calls) > 0, f"No tool calls found in {family}"

            # Each tool call should have name and arguments
            for tc in result.tool_calls:
                assert tc.function.name, f"Empty function name in {family}"
                assert tc.function.arguments, f"Empty arguments in {family}"

        @pytest.mark.parametrize("family,golden_file", collect_tool_call_files())
        def test_markers_removed_from_content(self, family: str, golden_file: Path):
            """Verify tool call markers removed from content for {family}."""
            text = golden_file.read_text()
            processor = get_response_processor()
            result = processor.process(text)

            # No raw markers in content
            assert "<tool_call>" not in result.content
            assert "</tool_call>" not in result.content
            assert "<function=" not in result.content
            assert "</function>" not in result.content
            assert "<|python_tag|>" not in result.content
            assert "<|eom_id|>" not in result.content

        def test_glm4_deduplication(self):
            """Verify GLM4 duplicate tool calls are deduplicated."""
            dup_file = GOLDEN_DIR / "glm4" / "duplicate_tools.txt"
            if not dup_file.exists():
                pytest.skip("GLM4 duplicate tools file not found")

            text = dup_file.read_text()
            processor = get_response_processor()
            result = processor.process(text)

            # Should have only 1 tool call (deduplicated)
            assert len(result.tool_calls) == 1

        def test_llama_python_tag(self):
            """Verify Llama Python tag format is parsed."""
            py_file = GOLDEN_DIR / "llama" / "python_tag.txt"
            if not py_file.exists():
                pytest.skip("Llama python tag file not found")

            text = py_file.read_text()
            processor = get_response_processor()
            result = processor.process(text)

            assert len(result.tool_calls) == 1
            # Python tag format: module.method -> function name
            assert "." in result.tool_calls[0].function.name


    class TestResponseProcessorThinking:
        """Test thinking/reasoning extraction from golden files."""

        def test_thinking_extracted(self):
            """Verify thinking content is extracted."""
            think_file = GOLDEN_DIR / "qwen" / "thinking.txt"
            if not think_file.exists():
                pytest.skip("Thinking file not found")

            text = think_file.read_text()
            processor = get_response_processor()
            result = processor.process(text)

            # Reasoning should be extracted
            assert result.reasoning is not None
            assert "analyze" in result.reasoning.lower()

        def test_thinking_tags_removed(self):
            """Verify thinking tags are removed from content."""
            think_file = GOLDEN_DIR / "qwen" / "thinking.txt"
            if not think_file.exists():
                pytest.skip("Thinking file not found")

            text = think_file.read_text()
            processor = get_response_processor()
            result = processor.process(text)

            # No thinking tags in content
            assert "<think>" not in result.content
            assert "</think>" not in result.content
            assert "<thinking>" not in result.content
            assert "</thinking>" not in result.content
            assert "<reasoning>" not in result.content
            assert "</reasoning>" not in result.content


    class TestStreamingProcessor:
        """Test streaming processor filters patterns correctly."""

        @pytest.mark.parametrize("family,format_name,chunk_file", collect_streaming_files())
        def test_streaming_filters_patterns(
            self, family: str, format_name: str, chunk_file: Path
        ):
            """Verify streaming filters {format_name} patterns for {family}."""
            chunks = chunk_file.read_text().splitlines()
            processor = StreamingProcessor()

            yielded = []
            for chunk in chunks:
                output, should_yield = processor.feed(chunk)
                if should_yield and output:
                    yielded.append(output)

            result = processor.finalize()
            full_streamed = "".join(yielded)

            # Markers should not appear in streamed output
            assert "<think>" not in full_streamed
            assert "</think>" not in full_streamed
            assert "<tool_call>" not in full_streamed
            assert "</tool_call>" not in full_streamed
            assert "<function=" not in full_streamed
            assert "<|python_tag|>" not in full_streamed

        def test_streaming_extracts_on_finalize(self):
            """Verify finalize() returns complete ParseResult."""
            chunk_file = GOLDEN_DIR / "qwen" / "stream" / "tool_call_chunks.txt"
            if not chunk_file.exists():
                pytest.skip("Streaming chunks file not found")

            chunks = chunk_file.read_text().splitlines()
            processor = StreamingProcessor()

            for chunk in chunks:
                processor.feed(chunk)

            result = processor.finalize()

            # ParseResult should have extracted tool calls
            assert len(result.tool_calls) > 0
            # Content should be clean
            assert "<tool_call>" not in result.content
    ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    # Run golden file tests
    pytest tests/mlx_server/test_response_processor_golden.py -v
    # Should pass all tests
    ```
  </verify>
  <done>Parametrized integration tests created covering all model families and streaming scenarios</done>
</task>

</tasks>

<verification>
1. Golden files exist for all families:
   ```bash
   ls backend/tests/fixtures/golden/*/tool_calls.txt
   ```
   Should show: qwen, llama, glm4, hermes, minimax, gemma

2. Streaming golden files exist:
   ```bash
   ls backend/tests/fixtures/golden/qwen/stream/*.txt
   ```

3. All golden tests pass:
   ```bash
   cd backend && pytest tests/mlx_server/test_response_processor_golden.py -v
   ```

4. Full test suite still passes:
   ```bash
   cd backend && pytest -v
   ```
</verification>

<success_criteria>
- Golden file directory structure created under backend/tests/fixtures/golden/
- Tool call golden files exist for all 6 model families (qwen, llama, glm4, hermes, minimax, gemma)
- Thinking/reasoning golden files exist with streaming variants
- test_response_processor_golden.py created with parametrized tests
- All golden file tests pass
- ResponseProcessor correctly extracts tool calls from all formats
- StreamingProcessor correctly filters patterns during streaming
</success_criteria>

<output>
After completion, create `.planning/phases/15-code-cleanup-integration-tests/15-03-SUMMARY.md`
</output>
