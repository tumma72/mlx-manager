---
phase: 15-code-cleanup-integration-tests
plan: 13
type: execute
wave: 6
depends_on: ["15-10", "15-12"]
files_modified:
  - backend/mlx_manager/mlx_server/models/types.py
  - backend/mlx_manager/mlx_server/models/detection.py
  - backend/mlx_manager/mlx_server/models/pool.py
  - backend/mlx_manager/mlx_server/services/audio.py
  - backend/mlx_manager/mlx_server/api/v1/speech.py
  - backend/mlx_manager/mlx_server/api/v1/transcriptions.py
  - backend/mlx_manager/mlx_server/api/v1/__init__.py
  - backend/mlx_manager/mlx_server/schemas/openai.py
  - backend/tests/mlx_server/test_audio.py
  - backend/tests/mlx_server/models/test_detection_audio.py
  - backend/tests/e2e/conftest.py
  - backend/tests/e2e/test_audio_e2e.py
  - frontend/src/lib/components/profiles/ProfileForm.svelte
autonomous: false

must_haves:
  truths:
    - "ModelType.AUDIO enum value exists"
    - "Audio models detected correctly (Kokoro, Whisper patterns)"
    - "Model pool loads audio models via mlx-audio"
    - "TTS endpoint generates audio from text"
    - "STT endpoint transcribes audio to text"
    - "E2E tests validate real audio inference"
    - "Profile UI supports audio model type"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/models/types.py"
      provides: "ModelType.AUDIO enum value"
    - path: "backend/mlx_manager/mlx_server/services/audio.py"
      provides: "Audio inference service (TTS + STT)"
    - path: "backend/mlx_manager/mlx_server/api/v1/speech.py"
      provides: "POST /v1/audio/speech endpoint"
    - path: "backend/mlx_manager/mlx_server/api/v1/transcriptions.py"
      provides: "POST /v1/audio/transcriptions endpoint"
    - path: "backend/tests/e2e/test_audio_e2e.py"
      provides: "E2E tests for audio inference"
      exports: ["test_tts_generate", "test_stt_transcribe"]
  key_links:
    - from: "api/v1/speech.py"
      to: "services/audio.py"
      via: "generate_speech()"
      pattern: "from.*audio import"
    - from: "api/v1/transcriptions.py"
      to: "services/audio.py"
      via: "transcribe_audio()"
      pattern: "from.*audio import"
    - from: "services/audio.py"
      to: "models/pool.py"
      via: "get_model_pool().get_model()"
      pattern: "from.*pool import"
---

<objective>
Implement full audio model support (TTS + STT) in the MLX unified server, including model type detection, loading, inference services, API endpoints, and E2E tests.

Purpose: mlx-audio (>=0.3.1) is already a dependency but has zero integration code. This plan bridges the gap by implementing the complete audio pipeline — from model detection through API endpoints — following the same patterns established for text, vision, and embeddings models.

Output: Working TTS (/v1/audio/speech) and STT (/v1/audio/transcriptions) endpoints with E2E tests, runnable via `pytest -m e2e_audio`.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/mlx_manager/mlx_server/models/types.py
@backend/mlx_manager/mlx_server/models/detection.py
@backend/mlx_manager/mlx_server/models/pool.py
@backend/mlx_manager/mlx_server/services/embeddings.py (pattern reference)
@backend/mlx_manager/mlx_server/api/v1/embeddings.py (pattern reference)
@backend/mlx_manager/mlx_server/api/v1/__init__.py
</context>

<prerequisite>
Before starting, investigate the mlx-audio library API:

```python
# Check what mlx-audio exposes
import mlx_audio
print(dir(mlx_audio))

# Check TTS API
from mlx_audio.tts import generate as tts_generate
help(tts_generate)

# Check STT API
from mlx_audio.stt import transcribe as stt_transcribe
help(stt_transcribe)

# Check model loading
from mlx_audio.utils import load_model
help(load_model)
```

Also review the mlx-audio server implementation for API patterns:
- Read `/path/to/mlx_audio/server.py` (installed package) for endpoint patterns
- Note request/response formats for TTS and STT

Adapt the implementation based on what the library actually exposes.
</prerequisite>

<tasks>

<task type="interactive">
  <name>Task 1: Add AUDIO model type and detection</name>
  <files>
    backend/mlx_manager/mlx_server/models/types.py
    backend/mlx_manager/mlx_server/models/detection.py
    backend/tests/mlx_server/models/test_detection_audio.py
  </files>
  <action>
    1. Add AUDIO to ModelType enum in `types.py`:
       ```python
       class ModelType(str, Enum):
           TEXT_GEN = "text-gen"
           VISION = "vision"
           EMBEDDINGS = "embeddings"
           AUDIO = "audio"
       ```

    2. Add audio detection patterns to `detection.py`:
       - Config-based: Check for `audio_config`, `tts_config`, `stt_config`, or
         architecture names containing "tts", "stt", "whisper", "kokoro", "speech"
       - Name-based fallback patterns: "kokoro", "whisper", "speech", "tts", "stt",
         "bark", "speecht5", "parler"

       Add audio detection AFTER embeddings detection, BEFORE the TEXT_GEN default:
       ```python
       # Audio detection (config-based)
       if config:
           has_audio_config = any(
               key in config
               for key in ("audio_config", "tts_config", "stt_config")
           )
           architectures = config.get("architectures", [])
           is_audio_arch = any(
               kw in arch.lower()
               for arch in architectures
               for kw in ("tts", "stt", "whisper", "kokoro", "speech", "bark")
           )
           if has_audio_config or is_audio_arch:
               return ModelType.AUDIO

       # Audio detection (name-based fallback)
       audio_patterns = (
           "kokoro", "whisper", "speech", "tts", "stt",
           "bark", "speecht5", "parler",
       )
       model_lower = model_id.lower()
       if any(p in model_lower for p in audio_patterns):
           return ModelType.AUDIO
       ```

    3. Create test file `backend/tests/mlx_server/models/test_detection_audio.py`:
       ```python
       """Tests for audio model type detection."""

       import pytest
       from mlx_manager.mlx_server.models.detection import detect_model_type
       from mlx_manager.mlx_server.models.types import ModelType


       class TestAudioDetectionByName:
           """Test audio model detection by model ID patterns."""

           @pytest.mark.parametrize("model_id", [
               "mlx-community/Kokoro-82M-4bit",
               "mlx-community/whisper-large-v3-mlx",
               "mlx-community/whisper-tiny-4bit",
               "openai/whisper-base",
               "some-org/tts-model-v1",
               "some-org/stt-model-v1",
               "suno/bark-small",
               "microsoft/speecht5_tts",
               "parler-tts/parler-tts-mini",
           ])
           def test_audio_detected_by_name(self, model_id):
               result = detect_model_type(model_id)
               assert result == ModelType.AUDIO, (
                   f"Expected AUDIO for {model_id}, got {result}"
               )


       class TestAudioDetectionByConfig:
           """Test audio model detection by config.json fields."""

           def test_audio_config_field(self):
               config = {"audio_config": {"sample_rate": 24000}}
               result = detect_model_type("some-model", config=config)
               assert result == ModelType.AUDIO

           def test_tts_config_field(self):
               config = {"tts_config": {"vocoder": "hifigan"}}
               result = detect_model_type("some-model", config=config)
               assert result == ModelType.AUDIO

           def test_stt_config_field(self):
               config = {"stt_config": {"decoder": "ctc"}}
               result = detect_model_type("some-model", config=config)
               assert result == ModelType.AUDIO

           def test_whisper_architecture(self):
               config = {"architectures": ["WhisperForConditionalGeneration"]}
               result = detect_model_type("some-model", config=config)
               assert result == ModelType.AUDIO

           def test_kokoro_architecture(self):
               config = {"architectures": ["KokoroTTSModel"]}
               result = detect_model_type("some-model", config=config)
               assert result == ModelType.AUDIO


       class TestAudioDoesNotFalsePositive:
           """Ensure non-audio models are not detected as audio."""

           @pytest.mark.parametrize("model_id", [
               "mlx-community/Qwen3-0.6B-4bit-DWQ",
               "mlx-community/Qwen2-VL-2B-Instruct-4bit",
               "mlx-community/all-MiniLM-L6-v2-4bit",
           ])
           def test_non_audio_not_detected(self, model_id):
               result = detect_model_type(model_id)
               assert result != ModelType.AUDIO
       ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    pytest tests/mlx_server/models/test_detection_audio.py -v
    # Existing detection tests still pass
    pytest tests/mlx_server/models/ -v
    ```
  </verify>
  <done>AUDIO model type added with config and name-based detection patterns</done>
</task>

<task type="interactive">
  <name>Task 2: Add audio model loading to pool</name>
  <files>backend/mlx_manager/mlx_server/models/pool.py</files>
  <action>
    Add audio loading branch to `_load_model()` method, following the same pattern
    as vision and embeddings:

    ```python
    elif model_type == ModelType.AUDIO:
        # mlx-audio loading - investigate the actual API first
        # The load function may differ between TTS and STT models
        # Possible approaches:
        #   from mlx_audio.tts import load as load_tts
        #   from mlx_audio.stt import load as load_stt
        # Or a unified loader:
        #   from mlx_audio.utils import load_model
        #
        # IMPORTANT: Before implementing, run the prerequisite investigation
        # to determine the actual mlx-audio API for loading models.
        # The implementation should match the library's actual interface.
        #
        # For TTS models (e.g., Kokoro):
        from mlx_audio.tts import load as load_tts
        result = await asyncio.to_thread(load_tts, model_id)
        model, tokenizer = result[0], result[1]
        #
        # For STT models (e.g., Whisper):
        # May need sub-type detection within audio
        # from mlx_audio.stt import load as load_stt
    ```

    **CRITICAL NOTE:** The exact loading API depends on what mlx-audio exposes.
    The executor MUST investigate the installed library first (see prerequisite section)
    and adapt this code accordingly. The pattern should follow the same structure as
    vision and embeddings loading.

    Also update the `LoadedModel` to store audio sub-type if needed:
    - The model may need to know if it's TTS or STT for routing purposes
    - Consider adding a `model_subtype` field or using the existing `model_type` field
      with values like "audio-tts" and "audio-stt"
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    # Verify pool can import and load audio
    python -c "
    from mlx_manager.mlx_server.models.types import ModelType
    print(ModelType.AUDIO)
    "
    # Run pool tests
    pytest tests/mlx_server/models/ -v
    ```
  </verify>
  <done>Model pool supports loading audio models via mlx-audio with TTS/STT sub-type handling</done>
</task>

<task type="interactive">
  <name>Task 3: Create audio inference service</name>
  <files>backend/mlx_manager/mlx_server/services/audio.py</files>
  <action>
    Create `backend/mlx_manager/mlx_server/services/audio.py` following the pattern from
    `embeddings.py` and `vision.py`:

    ```python
    """Audio inference service for TTS and STT.

    Provides text-to-speech and speech-to-text capabilities using mlx-audio.
    Uses the same queue-based threading pattern as other inference services
    to respect MLX Metal thread affinity.
    """

    import asyncio
    import io
    from loguru import logger

    from mlx_manager.mlx_server.models.pool import get_model_pool
    from mlx_manager.mlx_server.models.types import ModelType


    async def generate_speech(
        model_id: str,
        text: str,
        voice: str | None = None,
        speed: float = 1.0,
        response_format: str = "wav",
    ) -> bytes:
        """Generate speech audio from text using a TTS model.

        Args:
            model_id: HuggingFace model ID for TTS
            text: Text to convert to speech
            voice: Optional voice ID/name
            speed: Speech speed multiplier (default 1.0)
            response_format: Audio format (wav, mp3, opus, flac)

        Returns:
            Audio data as bytes
        """
        pool = get_model_pool()
        loaded_model = await pool.get_model(model_id)

        if loaded_model.model_type != ModelType.AUDIO:
            raise ValueError(f"Model {model_id} is not an audio model")

        logger.info(f"Generating speech with {model_id}: {len(text)} chars")

        # IMPORTANT: Adapt this based on mlx-audio's actual API
        # The executor must investigate and use the correct function calls
        def _generate():
            # This is a placeholder - adapt to actual mlx-audio TTS API
            # Possible patterns:
            #   from mlx_audio.tts import generate
            #   audio = generate(model, text, voice=voice, speed=speed)
            #
            #   Or using the model directly:
            #   audio = loaded_model.model.generate(text, voice=voice)
            #
            # Return audio bytes in the requested format
            raise NotImplementedError(
                "Adapt this to the actual mlx-audio TTS API. "
                "Check: python -c 'from mlx_audio.tts import generate; help(generate)'"
            )

        try:
            audio_bytes = await asyncio.wait_for(
                asyncio.to_thread(_generate),
                timeout=300.0,
            )
            logger.info(f"Generated {len(audio_bytes)} bytes of {response_format} audio")
            return audio_bytes
        except asyncio.TimeoutError:
            logger.error(f"TTS generation timed out for {model_id}")
            raise
        except Exception as e:
            logger.error(f"TTS generation failed: {e}")
            raise


    async def transcribe_audio(
        model_id: str,
        audio_data: bytes,
        language: str | None = None,
    ) -> dict:
        """Transcribe audio to text using an STT model.

        Args:
            model_id: HuggingFace model ID for STT
            audio_data: Raw audio bytes
            language: Optional language hint

        Returns:
            Dict with 'text' key containing transcription
        """
        pool = get_model_pool()
        loaded_model = await pool.get_model(model_id)

        if loaded_model.model_type != ModelType.AUDIO:
            raise ValueError(f"Model {model_id} is not an audio model")

        logger.info(f"Transcribing audio with {model_id}: {len(audio_data)} bytes")

        # IMPORTANT: Adapt this based on mlx-audio's actual STT API
        def _transcribe():
            # Placeholder - adapt to actual mlx-audio STT API
            # Possible patterns:
            #   from mlx_audio.stt import transcribe
            #   result = transcribe(model, audio_data, language=language)
            raise NotImplementedError(
                "Adapt this to the actual mlx-audio STT API. "
                "Check: python -c 'from mlx_audio.stt import transcribe; help(transcribe)'"
            )

        try:
            result = await asyncio.wait_for(
                asyncio.to_thread(_transcribe),
                timeout=300.0,
            )
            logger.info(f"Transcribed: {result.get('text', '')[:100]}...")
            return result
        except asyncio.TimeoutError:
            logger.error(f"STT transcription timed out for {model_id}")
            raise
        except Exception as e:
            logger.error(f"STT transcription failed: {e}")
            raise
    ```

    **NOTE:** The executor MUST replace the NotImplementedError placeholders with actual
    mlx-audio API calls after investigating the library. The no-placeholder policy requires
    these to be fully implemented before marking this task done.
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    python -c "from mlx_manager.mlx_server.services.audio import generate_speech, transcribe_audio; print('Service imports OK')"
    ```
  </verify>
  <done>Audio inference service created with TTS and STT functions using mlx-audio</done>
</task>

<task type="interactive">
  <name>Task 4: Create TTS and STT API endpoints</name>
  <files>
    backend/mlx_manager/mlx_server/schemas/openai.py
    backend/mlx_manager/mlx_server/api/v1/speech.py
    backend/mlx_manager/mlx_server/api/v1/transcriptions.py
    backend/mlx_manager/mlx_server/api/v1/__init__.py
  </files>
  <action>
    1. Add audio request/response schemas to `schemas/openai.py`:
       ```python
       class SpeechRequest(BaseModel):
           """OpenAI-compatible TTS request."""
           model: str
           input: str
           voice: str = "default"
           response_format: str = "wav"
           speed: float = 1.0

       class TranscriptionRequest(BaseModel):
           """OpenAI-compatible STT request (for JSON body)."""
           model: str
           language: str | None = None

       class TranscriptionResponse(BaseModel):
           """OpenAI-compatible STT response."""
           text: str
       ```

    2. Create `backend/mlx_manager/mlx_server/api/v1/speech.py`:
       ```python
       """POST /v1/audio/speech - Text-to-speech endpoint.

       OpenAI-compatible TTS API.
       """

       from fastapi import APIRouter, HTTPException
       from fastapi.responses import Response
       from loguru import logger

       from mlx_manager.mlx_server.models.detection import detect_model_type
       from mlx_manager.mlx_server.models.types import ModelType
       from mlx_manager.mlx_server.schemas.openai import SpeechRequest
       from mlx_manager.mlx_server.services.audio import generate_speech

       router = APIRouter(tags=["audio"])

       AUDIO_CONTENT_TYPES = {
           "wav": "audio/wav",
           "mp3": "audio/mpeg",
           "opus": "audio/opus",
           "flac": "audio/flac",
       }


       @router.post("/audio/speech")
       async def create_speech(request: SpeechRequest) -> Response:
           """Generate speech from text input."""
           # Validate model type
           model_type = detect_model_type(request.model)
           if model_type != ModelType.AUDIO:
               raise HTTPException(
                   status_code=400,
                   detail=f"Model {request.model} is not an audio model (detected: {model_type})",
               )

           # Validate response format
           if request.response_format not in AUDIO_CONTENT_TYPES:
               raise HTTPException(
                   status_code=400,
                   detail=f"Unsupported format: {request.response_format}. "
                          f"Supported: {list(AUDIO_CONTENT_TYPES.keys())}",
               )

           try:
               audio_bytes = await generate_speech(
                   model_id=request.model,
                   text=request.input,
                   voice=request.voice,
                   speed=request.speed,
                   response_format=request.response_format,
               )

               content_type = AUDIO_CONTENT_TYPES[request.response_format]
               return Response(
                   content=audio_bytes,
                   media_type=content_type,
                   headers={
                       "Content-Disposition": f"attachment; filename=speech.{request.response_format}",
                   },
               )
           except Exception as e:
               logger.error(f"TTS error: {e}")
               raise HTTPException(status_code=500, detail=str(e))
       ```

    3. Create `backend/mlx_manager/mlx_server/api/v1/transcriptions.py`:
       ```python
       """POST /v1/audio/transcriptions - Speech-to-text endpoint.

       OpenAI-compatible STT API.
       """

       from fastapi import APIRouter, File, Form, HTTPException, UploadFile
       from loguru import logger

       from mlx_manager.mlx_server.models.detection import detect_model_type
       from mlx_manager.mlx_server.models.types import ModelType
       from mlx_manager.mlx_server.schemas.openai import TranscriptionResponse
       from mlx_manager.mlx_server.services.audio import transcribe_audio

       router = APIRouter(tags=["audio"])


       @router.post("/audio/transcriptions", response_model=TranscriptionResponse)
       async def create_transcription(
           file: UploadFile = File(...),
           model: str = Form(...),
           language: str | None = Form(None),
       ) -> TranscriptionResponse:
           """Transcribe audio to text."""
           # Validate model type
           model_type = detect_model_type(model)
           if model_type != ModelType.AUDIO:
               raise HTTPException(
                   status_code=400,
                   detail=f"Model {model} is not an audio model (detected: {model_type})",
               )

           # Read audio file
           audio_data = await file.read()
           if len(audio_data) == 0:
               raise HTTPException(status_code=400, detail="Empty audio file")

           try:
               result = await transcribe_audio(
                   model_id=model,
                   audio_data=audio_data,
                   language=language,
               )
               return TranscriptionResponse(text=result["text"])
           except Exception as e:
               logger.error(f"STT error: {e}")
               raise HTTPException(status_code=500, detail=str(e))
       ```

    4. Register audio routers in `backend/mlx_manager/mlx_server/api/v1/__init__.py`:
       ```python
       from .speech import router as speech_router
       from .transcriptions import router as transcriptions_router

       # Add to v1_router includes
       v1_router.include_router(speech_router)
       v1_router.include_router(transcriptions_router)
       ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    # Verify routes are registered
    python -c "
    from mlx_manager.main import app
    routes = [r.path for r in app.routes]
    print([r for r in routes if 'audio' in r])
    "
    # Run type checks
    mypy mlx_manager/mlx_server/api/v1/speech.py
    mypy mlx_manager/mlx_server/api/v1/transcriptions.py
    ```
  </verify>
  <done>TTS and STT API endpoints created and registered in v1 router</done>
</task>

<task type="interactive">
  <name>Task 5: Create unit tests for audio</name>
  <files>backend/tests/mlx_server/test_audio.py</files>
  <action>
    Create unit tests following the pattern from `test_embeddings.py` and `test_vision.py`:

    ```python
    """Unit tests for audio model support.

    Tests cover:
    - Schema validation for TTS/STT requests
    - Endpoint routing and error handling
    - Service function signatures
    All tests use mocking (no actual model required).
    """

    import pytest
    from unittest.mock import AsyncMock, patch
    from httpx import AsyncClient, ASGITransport

    from mlx_manager.main import app
    from mlx_manager.mlx_server.models.types import ModelType
    from mlx_manager.mlx_server.schemas.openai import (
        SpeechRequest,
        TranscriptionResponse,
    )


    @pytest.fixture
    async def client():
        async with AsyncClient(
            transport=ASGITransport(app=app),
            base_url="http://test",
        ) as c:
            yield c


    class TestSpeechSchemas:
        """Test TTS request/response schemas."""

        def test_speech_request_defaults(self):
            req = SpeechRequest(model="test-model", input="Hello")
            assert req.voice == "default"
            assert req.response_format == "wav"
            assert req.speed == 1.0

        def test_speech_request_custom(self):
            req = SpeechRequest(
                model="test-model",
                input="Hello",
                voice="alloy",
                response_format="mp3",
                speed=1.5,
            )
            assert req.voice == "alloy"
            assert req.response_format == "mp3"
            assert req.speed == 1.5


    class TestTranscriptionSchemas:
        def test_transcription_response(self):
            resp = TranscriptionResponse(text="Hello world")
            assert resp.text == "Hello world"


    class TestSpeechEndpoint:
        """Test TTS endpoint."""

        @pytest.mark.anyio
        async def test_non_audio_model_returns_400(self, client):
            """Text model should be rejected for TTS."""
            with patch(
                "mlx_manager.mlx_server.api.v1.speech.detect_model_type",
                return_value=ModelType.TEXT_GEN,
            ):
                response = await client.post(
                    "/v1/audio/speech",
                    json={"model": "text-model", "input": "Hello"},
                )
                assert response.status_code == 400

        @pytest.mark.anyio
        async def test_unsupported_format_returns_400(self, client):
            """Unsupported audio format should be rejected."""
            with patch(
                "mlx_manager.mlx_server.api.v1.speech.detect_model_type",
                return_value=ModelType.AUDIO,
            ):
                response = await client.post(
                    "/v1/audio/speech",
                    json={
                        "model": "audio-model",
                        "input": "Hello",
                        "response_format": "aac",
                    },
                )
                assert response.status_code == 400

        @pytest.mark.anyio
        async def test_successful_tts(self, client):
            """Successful TTS should return audio bytes."""
            with (
                patch(
                    "mlx_manager.mlx_server.api.v1.speech.detect_model_type",
                    return_value=ModelType.AUDIO,
                ),
                patch(
                    "mlx_manager.mlx_server.api.v1.speech.generate_speech",
                    new_callable=AsyncMock,
                    return_value=b"fake-audio-data",
                ),
            ):
                response = await client.post(
                    "/v1/audio/speech",
                    json={"model": "audio-model", "input": "Hello"},
                )
                assert response.status_code == 200
                assert response.headers["content-type"] == "audio/wav"
                assert response.content == b"fake-audio-data"


    class TestTranscriptionEndpoint:
        """Test STT endpoint."""

        @pytest.mark.anyio
        async def test_non_audio_model_returns_400(self, client):
            """Text model should be rejected for STT."""
            with patch(
                "mlx_manager.mlx_server.api.v1.transcriptions.detect_model_type",
                return_value=ModelType.TEXT_GEN,
            ):
                response = await client.post(
                    "/v1/audio/transcriptions",
                    data={"model": "text-model"},
                    files={"file": ("test.wav", b"fake-audio", "audio/wav")},
                )
                assert response.status_code == 400

        @pytest.mark.anyio
        async def test_successful_stt(self, client):
            """Successful STT should return transcription."""
            with (
                patch(
                    "mlx_manager.mlx_server.api.v1.transcriptions.detect_model_type",
                    return_value=ModelType.AUDIO,
                ),
                patch(
                    "mlx_manager.mlx_server.api.v1.transcriptions.transcribe_audio",
                    new_callable=AsyncMock,
                    return_value={"text": "Hello world"},
                ),
            ):
                response = await client.post(
                    "/v1/audio/transcriptions",
                    data={"model": "audio-model"},
                    files={"file": ("test.wav", b"fake-audio", "audio/wav")},
                )
                assert response.status_code == 200
                assert response.json()["text"] == "Hello world"


    class TestAudioService:
        """Test audio service function signatures."""

        def test_generate_speech_is_async(self):
            from mlx_manager.mlx_server.services.audio import generate_speech
            import asyncio
            assert asyncio.iscoroutinefunction(generate_speech)

        def test_transcribe_audio_is_async(self):
            from mlx_manager.mlx_server.services.audio import transcribe_audio
            import asyncio
            assert asyncio.iscoroutinefunction(transcribe_audio)
    ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    pytest tests/mlx_server/test_audio.py -v
    ```
  </verify>
  <done>Unit tests for audio endpoints and service created (all mocked)</done>
</task>

<task type="interactive">
  <name>Task 6: Create audio E2E tests</name>
  <files>
    backend/tests/e2e/conftest.py
    backend/tests/e2e/test_audio_e2e.py
  </files>
  <action>
    1. Add audio model fixture to `backend/tests/e2e/conftest.py`:
       ```python
       # Reference models for audio testing
       AUDIO_TTS_MODEL = "mlx-community/Kokoro-82M-4bit"
       # STT model TBD - investigate which Whisper models work with mlx-audio
       # AUDIO_STT_MODEL = "mlx-community/whisper-large-v3-mlx"


       @pytest.fixture(scope="session")
       def audio_tts_model():
           """Return TTS model ID, skip if not downloaded."""
           if not is_model_available(AUDIO_TTS_MODEL):
               pytest.skip(f"Model {AUDIO_TTS_MODEL} not downloaded")
           return AUDIO_TTS_MODEL
       ```

    2. Create `backend/tests/e2e/test_audio_e2e.py`:
       ```python
       """End-to-end tests for audio model inference (TTS + STT).

       Tests validate the complete pipeline:
       - TTS: Text input → audio model → audio bytes output
       - STT: Audio input → audio model → text transcription

       Run:
         pytest -m e2e_audio -v
       """

       import io
       import struct
       import pytest


       # ──────────────────────────────────────────────
       # TTS Tests
       # ──────────────────────────────────────────────

       @pytest.mark.e2e
       @pytest.mark.e2e_audio
       class TestTTSGeneration:
           """Test text-to-speech generation."""

           @pytest.mark.anyio
           async def test_simple_tts(self, app_client, audio_tts_model):
               """TTS should generate non-empty audio from text."""
               response = await app_client.post(
                   "/v1/audio/speech",
                   json={
                       "model": audio_tts_model,
                       "input": "Hello, this is a test of text to speech.",
                   },
               )

               assert response.status_code == 200
               assert response.headers["content-type"] == "audio/wav"
               assert len(response.content) > 0, "Audio output should not be empty"

           @pytest.mark.anyio
           async def test_tts_produces_valid_wav(self, app_client, audio_tts_model):
               """TTS output should be a valid WAV file."""
               response = await app_client.post(
                   "/v1/audio/speech",
                   json={
                       "model": audio_tts_model,
                       "input": "Testing audio format.",
                       "response_format": "wav",
                   },
               )

               assert response.status_code == 200
               audio = response.content

               # WAV files start with "RIFF" header
               assert audio[:4] == b"RIFF", "Expected WAV RIFF header"
               assert audio[8:12] == b"WAVE", "Expected WAVE format"

           @pytest.mark.anyio
           async def test_tts_longer_text(self, app_client, audio_tts_model):
               """TTS should handle longer text input."""
               long_text = (
                   "The quick brown fox jumped over the lazy dog. "
                   "This sentence contains all letters of the English alphabet. "
                   "Text to speech models should handle multiple sentences gracefully."
               )
               response = await app_client.post(
                   "/v1/audio/speech",
                   json={"model": audio_tts_model, "input": long_text},
               )

               assert response.status_code == 200
               assert len(response.content) > 1000, (
                   "Longer text should produce more audio data"
               )

           @pytest.mark.anyio
           async def test_tts_speed_parameter(self, app_client, audio_tts_model):
               """Speed parameter should affect audio output."""
               text = "Testing speech speed."

               # Normal speed
               resp_normal = await app_client.post(
                   "/v1/audio/speech",
                   json={"model": audio_tts_model, "input": text, "speed": 1.0},
               )
               # Fast speed
               resp_fast = await app_client.post(
                   "/v1/audio/speech",
                   json={"model": audio_tts_model, "input": text, "speed": 2.0},
               )

               assert resp_normal.status_code == 200
               assert resp_fast.status_code == 200

               # Both should produce valid audio (size may differ)
               assert len(resp_normal.content) > 0
               assert len(resp_fast.content) > 0


       # ──────────────────────────────────────────────
       # STT Tests (round-trip: TTS → STT)
       # ──────────────────────────────────────────────

       @pytest.mark.e2e
       @pytest.mark.e2e_audio
       class TestSTTTranscription:
           """Test speech-to-text transcription.

           Uses round-trip approach: generate audio with TTS, then transcribe with STT.
           This tests both models in sequence.
           """

           @pytest.mark.anyio
           async def test_round_trip_tts_stt(self, app_client, audio_tts_model):
               """Generate audio from text, then transcribe back to text.

               NOTE: This test requires both TTS and STT models to be available.
               If STT model is not configured, this test will be skipped.
               """
               # First, generate audio
               tts_response = await app_client.post(
                   "/v1/audio/speech",
                   json={
                       "model": audio_tts_model,
                       "input": "Hello world",
                   },
               )
               assert tts_response.status_code == 200
               audio_data = tts_response.content

               # Then, transcribe it back
               # NOTE: STT model may be different from TTS model
               # For now, try with the same model or skip if STT not available
               try:
                   stt_response = await app_client.post(
                       "/v1/audio/transcriptions",
                       data={"model": audio_tts_model},
                       files={"file": ("speech.wav", audio_data, "audio/wav")},
                   )

                   if stt_response.status_code == 200:
                       text = stt_response.json()["text"]
                       assert len(text) > 0, "Transcription should not be empty"
                       # Loose check - just verify we got something back
                       assert isinstance(text, str)
                   elif stt_response.status_code == 400:
                       # Model may not support STT - this is acceptable
                       pytest.skip(
                           f"Model {audio_tts_model} may not support STT: "
                           f"{stt_response.json().get('detail', 'unknown error')}"
                       )
               except Exception as e:
                   pytest.skip(f"STT not available: {e}")


       # ──────────────────────────────────────────────
       # Error handling
       # ──────────────────────────────────────────────

       @pytest.mark.e2e
       @pytest.mark.e2e_audio
       class TestAudioErrorHandling:
           """Test error handling for audio endpoints."""

           @pytest.mark.anyio
           async def test_text_model_rejected_for_tts(self, app_client):
               """Text generation model should be rejected for TTS."""
               response = await app_client.post(
                   "/v1/audio/speech",
                   json={
                       "model": "mlx-community/Qwen3-0.6B-4bit-DWQ",
                       "input": "This should fail",
                   },
               )
               assert response.status_code == 400

           @pytest.mark.anyio
           async def test_empty_input_handled(self, app_client, audio_tts_model):
               """Empty text input should either fail gracefully or produce minimal audio."""
               response = await app_client.post(
                   "/v1/audio/speech",
                   json={"model": audio_tts_model, "input": ""},
               )
               # May return 400 (empty input) or 200 (silence) - both are acceptable
               assert response.status_code in (200, 400)
       ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    # Verify tests collect
    pytest tests/e2e/test_audio_e2e.py --collect-only -m e2e_audio

    # Run audio E2E (requires Kokoro model downloaded)
    pytest tests/e2e/test_audio_e2e.py -m e2e_audio -v --timeout=600
    ```
  </verify>
  <done>Audio E2E tests created for TTS generation, round-trip STT, and error handling</done>
</task>

<task type="interactive">
  <name>Task 7: Add audio model type to Profile UI</name>
  <files>frontend/src/lib/components/profiles/ProfileForm.svelte</files>
  <action>
    1. Update the model type mapping to include "audio":
       ```typescript
       // Before (after 15-12 adds embeddings):
       modelType = ['lm', 'multimodal', 'embeddings'].includes(profileModelType) ? profileModelType : 'lm';

       // After:
       modelType = ['lm', 'multimodal', 'embeddings', 'audio'].includes(profileModelType) ? profileModelType : 'lm';
       ```

    2. Add the audio option to the Select dropdown:
       ```svelte
       <Select id="modelType" bind:value={modelType}>
           <option value="lm">Language Model (lm)</option>
           <option value="multimodal">Multimodal (Vision)</option>
           <option value="embeddings">Embeddings</option>
           <option value="audio">Audio (TTS/STT)</option>
       </Select>
       ```

    3. Update the TypeScript type if needed (check `types.ts` - it may already include "audio"
       or similar; if not, add it).
  </action>
  <verify>
    ```bash
    cd frontend
    npm run check
    npm run lint
    ```
  </verify>
  <done>Profile UI updated with audio model type option</done>
</task>

</tasks>

<verification>
1. ModelType.AUDIO exists:
   ```python
   from mlx_manager.mlx_server.models.types import ModelType
   assert ModelType.AUDIO == "audio"
   ```

2. Audio detection works:
   ```bash
   pytest tests/mlx_server/models/test_detection_audio.py -v
   ```

3. Unit tests pass:
   ```bash
   pytest tests/mlx_server/test_audio.py -v
   ```

4. Audio endpoints registered:
   ```bash
   python -c "from mlx_manager.main import app; print([r.path for r in app.routes if 'audio' in r.path])"
   ```

5. E2E tests pass (with models):
   ```bash
   pytest -m e2e_audio -v --timeout=600
   ```

6. Profile UI includes audio option:
   ```bash
   cd frontend && npm run check
   ```

7. Full test suite passes:
   ```bash
   cd backend && pytest -v
   cd frontend && npm run test
   ```
</verification>

<success_criteria>
- ModelType.AUDIO enum value added
- Audio models detected by config (audio_config, architectures) and name patterns (kokoro, whisper, etc.)
- Model pool loads audio models via mlx-audio
- POST /v1/audio/speech generates audio bytes from text
- POST /v1/audio/transcriptions transcribes audio to text
- Unit tests pass with mocked audio inference
- E2E TTS test generates valid WAV audio from Kokoro model
- E2E round-trip test (TTS→STT) produces non-empty transcription
- Error handling: non-audio models rejected, empty input handled
- Profile UI supports audio model type selection
- No placeholders or NotImplementedError in final code
</success_criteria>

<output>
After completion, create `.planning/phases/15-code-cleanup-integration-tests/15-13-SUMMARY.md`
</output>
