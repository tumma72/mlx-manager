---
phase: 15-code-cleanup-integration-tests
plan: 12
type: execute
wave: 5
depends_on: ["15-10"]
files_modified:
  - backend/tests/e2e/test_embeddings_e2e.py
  - backend/tests/e2e/conftest.py
  - frontend/src/lib/components/profiles/ProfileForm.svelte
autonomous: false

must_haves:
  truths:
    - "E2E tests run actual embeddings inference with all-MiniLM-L6-v2"
    - "Embedding vectors have correct dimensionality and are normalized"
    - "Similar texts produce higher cosine similarity than dissimilar texts"
    - "Batch embedding requests work correctly"
    - "Profile UI allows selecting 'embeddings' model type"
  artifacts:
    - path: "backend/tests/e2e/test_embeddings_e2e.py"
      provides: "E2E tests for embeddings inference"
      exports: ["test_single_embedding", "test_batch_embeddings", "test_cosine_similarity"]
    - path: "frontend/src/lib/components/profiles/ProfileForm.svelte"
      provides: "Updated profile form with embeddings model type option"
  key_links:
    - from: "test_embeddings_e2e.py"
      to: "mlx_server/api/v1/embeddings.py"
      via: "HTTP POST /v1/embeddings"
      pattern: "httpx.AsyncClient"
---

<objective>
Create end-to-end tests for embeddings model inference and fix the Profile UI to support the embeddings model type.

Purpose: Validate the complete embeddings pipeline — from HTTP request through model detection, mlx-embeddings loading, vector generation, and response formatting — using the actual all-MiniLM-L6-v2-4bit model. Also close the UI gap where users cannot create embeddings profiles through the web interface.

Output: An E2E embeddings test suite runnable via `pytest -m e2e_embeddings`, plus a ProfileForm.svelte fix.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/mlx_manager/mlx_server/api/v1/embeddings.py
@backend/mlx_manager/mlx_server/services/embeddings.py
@backend/mlx_manager/mlx_server/models/pool.py
@backend/mlx_manager/mlx_server/models/detection.py
@backend/tests/mlx_server/test_embeddings.py
@frontend/src/lib/components/profiles/ProfileForm.svelte
</context>

<tasks>

<task type="interactive">
  <name>Task 1: Add embeddings model fixture to E2E conftest</name>
  <files>backend/tests/e2e/conftest.py</files>
  <action>
    Add to the existing `backend/tests/e2e/conftest.py`:

    ```python
    # Reference model for embeddings testing
    EMBEDDINGS_MODEL = "mlx-community/all-MiniLM-L6-v2-4bit"


    @pytest.fixture(scope="session")
    def embeddings_model():
        """Return embeddings model ID, skip if not downloaded."""
        if not is_model_available(EMBEDDINGS_MODEL):
            pytest.skip(f"Model {EMBEDDINGS_MODEL} not downloaded")
        return EMBEDDINGS_MODEL
    ```
  </action>
  <verify>
    ```bash
    cd backend && python -c "from tests.e2e.conftest import EMBEDDINGS_MODEL; print(EMBEDDINGS_MODEL)"
    ```
  </verify>
  <done>Embeddings model fixture added to E2E conftest</done>
</task>

<task type="interactive">
  <name>Task 2: Create embeddings E2E test suite</name>
  <files>backend/tests/e2e/test_embeddings_e2e.py</files>
  <action>
    Create the E2E test file:

    ```python
    """End-to-end tests for embeddings model inference.

    Tests validate the complete pipeline:
    1. HTTP request to /v1/embeddings
    2. Model type detection (EMBEDDINGS)
    3. Model loading via mlx-embeddings
    4. Vector generation and L2 normalization
    5. OpenAI-compatible response format

    Run:
      pytest -m e2e_embeddings -v
    """

    import math
    import pytest


    def cosine_similarity(a: list[float], b: list[float]) -> float:
        """Compute cosine similarity between two vectors."""
        dot = sum(x * y for x, y in zip(a, b))
        norm_a = math.sqrt(sum(x * x for x in a))
        norm_b = math.sqrt(sum(x * x for x in b))
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return dot / (norm_a * norm_b)


    def build_embedding_request(
        model: str,
        input_text: str | list[str],
    ) -> dict:
        """Build an OpenAI-compatible embedding request."""
        return {
            "model": model,
            "input": input_text,
        }


    # ──────────────────────────────────────────────
    # Single embedding tests
    # ──────────────────────────────────────────────

    @pytest.mark.e2e
    @pytest.mark.e2e_embeddings
    class TestSingleEmbedding:
        """Test single text embedding generation."""

        @pytest.mark.anyio
        async def test_single_embedding_response(self, app_client, embeddings_model):
            """Single text input should return a valid embedding vector."""
            request = build_embedding_request(embeddings_model, "Hello world")
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            data = response.json()

            # Validate response structure
            assert data["object"] == "list"
            assert data["model"] == embeddings_model
            assert len(data["data"]) == 1
            assert data["data"][0]["object"] == "embedding"
            assert data["data"][0]["index"] == 0

            # Validate embedding vector
            embedding = data["data"][0]["embedding"]
            assert isinstance(embedding, list)
            assert len(embedding) > 0, "Embedding vector should not be empty"
            assert all(isinstance(v, float) for v in embedding), "All values should be floats"

        @pytest.mark.anyio
        async def test_embedding_dimensionality(self, app_client, embeddings_model):
            """all-MiniLM-L6-v2 should produce 384-dimensional embeddings."""
            request = build_embedding_request(embeddings_model, "Test dimensionality")
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            embedding = response.json()["data"][0]["embedding"]
            assert len(embedding) == 384, (
                f"Expected 384 dimensions for MiniLM, got {len(embedding)}"
            )

        @pytest.mark.anyio
        async def test_embedding_is_normalized(self, app_client, embeddings_model):
            """Embeddings should be L2-normalized (unit vectors)."""
            request = build_embedding_request(embeddings_model, "Check normalization")
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            embedding = response.json()["data"][0]["embedding"]

            # L2 norm should be ~1.0
            l2_norm = math.sqrt(sum(v * v for v in embedding))
            assert abs(l2_norm - 1.0) < 0.01, (
                f"Expected L2 norm ~1.0, got {l2_norm}"
            )

        @pytest.mark.anyio
        async def test_usage_stats(self, app_client, embeddings_model):
            """Response should include usage statistics."""
            request = build_embedding_request(embeddings_model, "Usage test")
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            usage = response.json()["usage"]
            assert "prompt_tokens" in usage
            assert "total_tokens" in usage
            assert usage["prompt_tokens"] > 0
            assert usage["total_tokens"] > 0


    # ──────────────────────────────────────────────
    # Batch embedding tests
    # ──────────────────────────────────────────────

    @pytest.mark.e2e
    @pytest.mark.e2e_embeddings
    class TestBatchEmbeddings:
        """Test batch embedding generation."""

        @pytest.mark.anyio
        async def test_batch_returns_multiple(self, app_client, embeddings_model):
            """Batch input should return one embedding per text."""
            texts = [
                "The quick brown fox",
                "jumped over the lazy dog",
                "A completely different sentence about space",
            ]
            request = build_embedding_request(embeddings_model, texts)
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            data = response.json()
            assert len(data["data"]) == 3

            # Each should have correct index
            for i, item in enumerate(data["data"]):
                assert item["index"] == i
                assert len(item["embedding"]) == 384

        @pytest.mark.anyio
        async def test_batch_consistent_dimensionality(self, app_client, embeddings_model):
            """All embeddings in a batch should have the same dimensionality."""
            texts = ["Text one", "Text two", "Text three", "Text four"]
            request = build_embedding_request(embeddings_model, texts)
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            dimensions = [len(item["embedding"]) for item in response.json()["data"]]
            assert len(set(dimensions)) == 1, (
                f"Inconsistent dimensions: {dimensions}"
            )


    # ──────────────────────────────────────────────
    # Semantic similarity tests
    # ──────────────────────────────────────────────

    @pytest.mark.e2e
    @pytest.mark.e2e_embeddings
    class TestSemanticSimilarity:
        """Test that embeddings capture semantic similarity."""

        @pytest.mark.anyio
        async def test_similar_texts_higher_similarity(self, app_client, embeddings_model):
            """Similar texts should have higher cosine similarity than dissimilar ones."""
            texts = [
                "The cat sat on the mat",        # [0] - about a cat
                "A kitten was resting on a rug",  # [1] - similar to [0]
                "Python is a programming language",  # [2] - completely different
            ]
            request = build_embedding_request(embeddings_model, texts)
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            embeddings = [item["embedding"] for item in response.json()["data"]]

            sim_similar = cosine_similarity(embeddings[0], embeddings[1])
            sim_different = cosine_similarity(embeddings[0], embeddings[2])

            assert sim_similar > sim_different, (
                f"Similar texts similarity ({sim_similar:.4f}) should be > "
                f"dissimilar texts ({sim_different:.4f})"
            )

        @pytest.mark.anyio
        async def test_identical_texts_high_similarity(self, app_client, embeddings_model):
            """Identical texts should have cosine similarity ~1.0."""
            texts = [
                "This is a test sentence",
                "This is a test sentence",
            ]
            request = build_embedding_request(embeddings_model, texts)
            response = await app_client.post("/v1/embeddings", json=request)

            assert response.status_code == 200
            embeddings = [item["embedding"] for item in response.json()["data"]]

            sim = cosine_similarity(embeddings[0], embeddings[1])
            assert sim > 0.99, f"Identical texts should have similarity ~1.0, got {sim:.4f}"


    # ──────────────────────────────────────────────
    # Error handling tests
    # ──────────────────────────────────────────────

    @pytest.mark.e2e
    @pytest.mark.e2e_embeddings
    class TestEmbeddingsErrorHandling:
        """Test error handling for embeddings endpoint."""

        @pytest.mark.anyio
        async def test_text_model_rejected(self, app_client):
            """Text generation model should be rejected for embeddings."""
            request = build_embedding_request(
                "mlx-community/Qwen3-0.6B-4bit-DWQ",
                "This should fail",
            )
            response = await app_client.post("/v1/embeddings", json=request)
            assert response.status_code == 400

        @pytest.mark.anyio
        async def test_empty_input_rejected(self, app_client, embeddings_model):
            """Empty input should be rejected."""
            request = build_embedding_request(embeddings_model, [])
            response = await app_client.post("/v1/embeddings", json=request)
            assert response.status_code == 400
    ```
  </action>
  <verify>
    ```bash
    cd backend && source .venv/bin/activate
    # Verify tests collect
    pytest tests/e2e/test_embeddings_e2e.py --collect-only -m e2e_embeddings

    # Run embeddings E2E (requires all-MiniLM-L6-v2-4bit downloaded)
    pytest tests/e2e/test_embeddings_e2e.py -m e2e_embeddings -v --timeout=300
    ```
  </verify>
  <done>Embeddings E2E test suite with dimensionality, normalization, batch, and similarity tests</done>
</task>

<task type="interactive">
  <name>Task 3: Add embeddings model type to Profile UI</name>
  <files>frontend/src/lib/components/profiles/ProfileForm.svelte</files>
  <action>
    1. Update the model type mapping (around line 38-40) to include "embeddings":
       ```typescript
       // Before:
       modelType = ['lm', 'multimodal'].includes(profileModelType) ? profileModelType : 'lm';

       // After:
       modelType = ['lm', 'multimodal', 'embeddings'].includes(profileModelType) ? profileModelType : 'lm';
       ```

    2. Add the embeddings option to the Select dropdown (around line 136-139):
       ```svelte
       <Select id="modelType" bind:value={modelType}>
           <option value="lm">Language Model (lm)</option>
           <option value="multimodal">Multimodal (Vision)</option>
           <option value="embeddings">Embeddings</option>
       </Select>
       ```

    3. Verify the type definition in `frontend/src/lib/api/types.ts` already includes "embeddings"
       (it should — the research confirmed this).
  </action>
  <verify>
    ```bash
    cd frontend
    npm run check
    npm run lint
    ```
  </verify>
  <done>Profile UI updated to support embeddings model type selection</done>
</task>

</tasks>

<verification>
1. Tests collect correctly:
   ```bash
   pytest tests/e2e/test_embeddings_e2e.py --collect-only -m e2e_embeddings
   ```

2. E2E tests pass with actual model:
   ```bash
   pytest -m e2e_embeddings -v --timeout=300
   ```

3. Embeddings produce correct dimensions (384 for MiniLM):
   - Verified by test_embedding_dimensionality

4. Embeddings are L2-normalized:
   - Verified by test_embedding_is_normalized

5. Semantic similarity works:
   - Verified by test_similar_texts_higher_similarity

6. Profile UI shows embeddings option:
   ```bash
   cd frontend && npm run check
   ```

7. Existing tests still pass:
   ```bash
   cd backend && pytest -v
   cd frontend && npm run test
   ```
</verification>

<success_criteria>
- E2E embeddings tests pass with all-MiniLM-L6-v2-4bit model
- Single and batch embedding generation works correctly
- Embedding vectors are 384-dimensional and L2-normalized
- Semantic similarity: similar texts score higher than dissimilar texts
- Identical texts produce cosine similarity ~1.0
- Error handling: text models rejected, empty input rejected
- ProfileForm.svelte includes "embeddings" model type option
- Frontend type checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/15-code-cleanup-integration-tests/15-12-SUMMARY.md`
</output>
