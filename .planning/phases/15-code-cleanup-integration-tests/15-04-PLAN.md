# Plan 15-04: StreamingProcessor Redesign for OpenAI-Compatible Reasoning

## Objective

Redesign StreamingProcessor to properly stream `reasoning_content` during generation, following OpenAI's API spec for reasoning models (o1, o3). This fixes empty responses with thinking models and enables proper thinking bubble display.

## Problem Statement

Current architecture has conflicting approaches:
1. StreamingProcessor filters `<think>` tags → yields nothing → reasoning lost
2. chat.py has tag parsing → never sees tags (already filtered)
3. Schema has `reasoning_content` → never populated

**Result:** 95-205 tokens generated but response is empty.

## Design Principles

1. **Server sends OpenAI-compatible responses** - Chat UI should NOT manipulate data
2. **Thinking content goes in `delta.reasoning_content`** - Per OpenAI o1/o3 spec
3. **Regular content goes in `delta.content`** - Standard field
4. **Simplify pipeline** - Remove redundant processing, be fast and precise

## Architecture Change

### Before (Broken)
```
Model → StreamingProcessor.feed() → (None, False) for thinking → LOST
                                  → finalize() → reasoning extracted → IGNORED
```

### After (Correct)
```
Model → StreamingProcessor.feed() → StreamEvent with reasoning_content or content
      → inference.py yields delta with appropriate field
      → Chat UI displays what it receives
```

## Tasks

### Task 1: Add StreamEvent dataclass

**File:** `backend/mlx_manager/mlx_server/services/response_processor.py`

Add a structured event type for streaming output:

```python
@dataclass
class StreamEvent:
    """Event from streaming processor."""
    content: str | None = None
    reasoning_content: str | None = None
    is_complete: bool = False  # True when pattern ends
```

### Task 2: Redesign StreamingProcessor.feed()

**File:** `backend/mlx_manager/mlx_server/services/response_processor.py`

Change `feed()` to return `StreamEvent` instead of `tuple[str | None, bool]`:

```python
def feed(self, token: str) -> StreamEvent:
    """Feed a token, get streaming event.

    Returns:
        StreamEvent with either reasoning_content (inside <think>)
        or content (outside <think>)
    """
    self._accumulated += token

    if self._in_pattern:
        # Inside <think> - return as reasoning_content
        self._buffer += token
        end_marker = self.PATTERN_ENDS.get(self._pattern_start, "")
        if end_marker and end_marker in self._buffer:
            # Pattern complete
            self._in_pattern = False
            # Extract content before end marker
            end_idx = self._buffer.index(end_marker)
            reasoning = self._buffer[:end_idx]
            after = self._buffer[end_idx + len(end_marker):]
            self._buffer = ""
            self._pattern_start = ""

            if after:
                # Content after pattern - recurse
                subsequent = self.feed(after)
                # Combine: last reasoning chunk + subsequent
                return StreamEvent(
                    reasoning_content=reasoning if reasoning else None,
                    content=subsequent.content,
                )
            return StreamEvent(reasoning_content=reasoning, is_complete=True)

        # Still inside - yield reasoning incrementally
        # Buffer a small amount to avoid partial tokens
        if len(self._buffer) > 10:
            to_yield = self._buffer[:-10]
            self._buffer = self._buffer[-10:]
            return StreamEvent(reasoning_content=to_yield)
        return StreamEvent()  # Buffering

    # Not in pattern - check for pattern start
    # ... (similar logic but return StreamEvent)
```

**Key changes:**
- Return `StreamEvent` instead of tuple
- Inside `<think>`: yield as `reasoning_content`
- Outside `<think>`: yield as `content`
- Incremental yield instead of buffering until end

### Task 3: Update inference.py to use StreamEvent

**File:** `backend/mlx_manager/mlx_server/services/inference.py`

Update `_stream_chat_generate()` to yield proper deltas:

```python
# Line ~315, replace:
filtered_output, should_yield = stream_processor.feed(token_text)
if should_yield and filtered_output:
    yield {"choices": [{"delta": {"content": filtered_output}}]}

# With:
event = stream_processor.feed(token_text)
if event.reasoning_content or event.content:
    delta = {}
    if event.reasoning_content:
        delta["reasoning_content"] = event.reasoning_content
    if event.content:
        delta["content"] = event.content
    yield {
        "id": completion_id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": model_id,
        "choices": [{
            "index": 0,
            "delta": delta,
            "finish_reason": None,
        }],
    }
```

Also update finalize section to include reasoning in final message if present.

### Task 4: Remove duplicate tag parsing from chat.py

**File:** `backend/mlx_manager/routers/chat.py`

Remove the entire tag_buffer mechanism (lines ~180-225). The server now sends proper `reasoning_content` in deltas.

Simplify to:
```python
async for chunk in async_gen:
    delta = chunk.get("choices", [{}])[0].get("delta", {})
    content = delta.get("content", "")
    reasoning = delta.get("reasoning_content", "")

    if reasoning:
        yield f"data: {json.dumps({'type': 'thinking', 'content': reasoning})}\n\n"

    if content:
        yield f"data: {json.dumps({'type': 'response', 'content': content})}\n\n"

    # ... tool_calls handling unchanged
```

### Task 5: Update tests

**Files:**
- `backend/tests/mlx_server/test_response_processor.py`
- `backend/tests/mlx_server/test_response_processor_golden.py`

Update tests to use new `StreamEvent` return type.

## Success Criteria

1. [ ] Qwen3-0.6B with enable_thinking=True returns non-empty responses
2. [ ] Thinking content streams in `reasoning_content` field
3. [ ] Regular content streams in `content` field
4. [ ] Chat UI shows thinking bubble filling during generation
5. [ ] No tag parsing in chat.py - server sends proper format
6. [ ] All existing tests pass (updated for new API)
7. [ ] Golden file tests validate streaming behavior

## Dependencies

- None (self-contained refactor)

## Risks

- Breaking change to StreamingProcessor API
- Need to update all callers

## Estimated Complexity

Medium-High - Architectural change but localized to streaming pipeline

---

*Plan: 15-04*
*Phase: 15-code-cleanup-integration-tests*
*Priority: CRITICAL*
