---
phase: 10-dual-protocol-cloud-fallback
plan: 03
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - backend/mlx_manager/mlx_server/services/protocol.py
  - backend/tests/mlx_server/services/test_protocol.py
autonomous: true

must_haves:
  truths:
    - "Protocol translator converts Anthropic request to internal format"
    - "Anthropic system message extracted and placed first in messages array"
    - "Content blocks normalized to string text"
    - "Stop sequences mapped correctly between formats"
    - "Stop reason translation works bidirectionally"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/protocol.py"
      provides: "ProtocolTranslator service"
      exports: ["ProtocolTranslator", "InternalRequest"]
    - path: "backend/tests/mlx_server/services/test_protocol.py"
      provides: "Protocol translation tests"
      min_lines: 80
  key_links:
    - from: "services/protocol.py"
      to: "schemas/anthropic.py"
      via: "import"
      pattern: "from.*schemas.anthropic import"
    - from: "services/protocol.py"
      to: "schemas/openai.py"
      via: "import"
      pattern: "from.*schemas.openai import"
---

<objective>
Create bidirectional protocol translator service between OpenAI and Anthropic formats.

Purpose: Enable the /v1/messages endpoint to translate Anthropic requests to internal format for local inference, and translate responses back to Anthropic format.
Output: ProtocolTranslator with anthropic_to_internal and internal_to_anthropic_* methods.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-RESEARCH.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-01-SUMMARY.md
@backend/mlx_manager/mlx_server/schemas/openai.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create protocol translator service</name>
  <files>backend/mlx_manager/mlx_server/services/protocol.py</files>
  <action>
Create ProtocolTranslator service with bidirectional conversion:

```python
"""Protocol translation between OpenAI and Anthropic formats."""

from dataclasses import dataclass
from mlx_manager.mlx_server.schemas.anthropic import (
    AnthropicMessagesRequest,
    AnthropicMessagesResponse,
    TextBlockParam,
    MessageParam,
    Usage as AnthropicUsage,
    TextBlock,
)

@dataclass
class InternalRequest:
    """Internal request format used by inference service."""
    model: str
    messages: list[dict[str, str]]  # [{"role": str, "content": str}]
    max_tokens: int
    temperature: float
    top_p: float | None
    stream: bool
    stop: list[str] | None


class ProtocolTranslator:
    """Bidirectional translation between OpenAI and Anthropic formats."""

    # Stop reason mapping: Anthropic -> OpenAI
    STOP_REASON_TO_OPENAI = {
        "end_turn": "stop",
        "max_tokens": "length",
        "stop_sequence": "stop",
        "tool_use": "tool_calls",
    }

    # Stop reason mapping: OpenAI -> Anthropic
    STOP_REASON_TO_ANTHROPIC = {
        "stop": "end_turn",
        "length": "max_tokens",
        "content_filter": "end_turn",
        "tool_calls": "tool_use",
    }

    def anthropic_to_internal(self, request: AnthropicMessagesRequest) -> InternalRequest:
        """Convert Anthropic Messages request to internal format."""
        messages: list[dict[str, str]] = []

        # Handle system prompt (Anthropic has separate field)
        if request.system:
            if isinstance(request.system, str):
                system_text = request.system
            else:
                # List of TextBlockParam
                system_text = " ".join(b.text for b in request.system)
            messages.append({"role": "system", "content": system_text})

        # Convert content blocks to simple content
        for msg in request.messages:
            content = self._extract_text_content(msg.content)
            messages.append({"role": msg.role, "content": content})

        return InternalRequest(
            model=request.model,
            messages=messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            stream=request.stream,
            stop=request.stop_sequences,
        )

    def _extract_text_content(self, content: str | list) -> str:
        """Extract text from content (string or list of blocks)."""
        if isinstance(content, str):
            return content

        text_parts = []
        for block in content:
            if hasattr(block, "type"):
                if block.type == "text":
                    text_parts.append(block.text)
            elif isinstance(block, dict):
                if block.get("type") == "text":
                    text_parts.append(block.get("text", ""))
        return " ".join(text_parts)

    def internal_to_anthropic_response(
        self,
        response_text: str,
        request_id: str,
        model: str,
        stop_reason: str,
        input_tokens: int,
        output_tokens: int,
    ) -> AnthropicMessagesResponse:
        """Convert internal response to Anthropic format."""
        anthropic_stop_reason = self.STOP_REASON_TO_ANTHROPIC.get(stop_reason, "end_turn")

        return AnthropicMessagesResponse(
            id=request_id,
            model=model,
            content=[TextBlock(text=response_text)],
            stop_reason=anthropic_stop_reason,
            usage=AnthropicUsage(
                input_tokens=input_tokens,
                output_tokens=output_tokens,
            ),
        )

    def openai_stop_to_anthropic(self, stop_reason: str | None) -> str:
        """Convert OpenAI stop reason to Anthropic format."""
        if stop_reason is None:
            return "end_turn"
        return self.STOP_REASON_TO_ANTHROPIC.get(stop_reason, "end_turn")

    def anthropic_stop_to_openai(self, stop_reason: str | None) -> str:
        """Convert Anthropic stop reason to OpenAI format."""
        if stop_reason is None:
            return "stop"
        return self.STOP_REASON_TO_OPENAI.get(stop_reason, "stop")


# Module-level singleton
_translator: ProtocolTranslator | None = None


def get_translator() -> ProtocolTranslator:
    """Get the protocol translator singleton."""
    global _translator
    if _translator is None:
        _translator = ProtocolTranslator()
    return _translator
```
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.services.protocol import ProtocolTranslator, get_translator
from mlx_manager.mlx_server.schemas.anthropic import AnthropicMessagesRequest, MessageParam

translator = get_translator()

# Test conversion
request = AnthropicMessagesRequest(
    model='claude-3',
    max_tokens=1000,
    messages=[MessageParam(role='user', content='Hello')],
    system='You are helpful'
)
internal = translator.anthropic_to_internal(request)
print(f'Messages: {internal.messages}')
assert internal.messages[0]['role'] == 'system', 'System should be first'
assert internal.messages[1]['role'] == 'user', 'User message second'
print('OK: Anthropic to internal conversion works')
"
  </verify>
  <done>ProtocolTranslator service created with anthropic_to_internal conversion</done>
</task>

<task type="auto">
  <name>Task 2: Add protocol translator tests</name>
  <files>backend/tests/mlx_server/services/test_protocol.py</files>
  <action>
Create comprehensive tests for protocol translation:

1. Test anthropic_to_internal:
   - System message placed first
   - String content preserved
   - Content blocks extracted and concatenated
   - Multiple messages in order
   - Stop sequences mapped to stop parameter
   - Temperature/top_p passed through

2. Test _extract_text_content:
   - String returns unchanged
   - List of TextBlockParam returns joined text
   - Mixed content (with images) extracts only text

3. Test stop reason translation:
   - openai_stop_to_anthropic: stop->end_turn, length->max_tokens
   - anthropic_stop_to_openai: end_turn->stop, max_tokens->length
   - None handling

4. Test internal_to_anthropic_response:
   - Response text wrapped in TextBlock
   - Stop reason translated
   - Usage populated correctly

5. Test singleton:
   - get_translator() returns same instance
  </action>
  <verify>cd /Users/atomasini/Development/mlx-manager/backend && python -m pytest tests/mlx_server/services/test_protocol.py -v</verify>
  <done>Protocol translator tests passing</done>
</task>

</tasks>

<verification>
```bash
cd /Users/atomasini/Development/mlx-manager/backend
# Run protocol tests
python -m pytest tests/mlx_server/services/test_protocol.py -v

# Verify imports
python -c "from mlx_manager.mlx_server.services.protocol import get_translator; print('OK')"

# Type checking
mypy mlx_manager/mlx_server/services/protocol.py
```
</verification>

<success_criteria>
- anthropic_to_internal extracts system message and places first
- Content blocks normalized to string
- Stop reason bidirectional translation works
- internal_to_anthropic_response builds valid response
- All tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/10-dual-protocol-cloud-fallback/10-03-SUMMARY.md`
</output>
