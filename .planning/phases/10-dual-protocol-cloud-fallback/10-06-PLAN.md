---
phase: 10-dual-protocol-cloud-fallback
plan: 06
type: execute
wave: 3
depends_on: ["10-04"]
files_modified:
  - backend/mlx_manager/mlx_server/services/cloud/openai.py
  - backend/tests/mlx_server/services/cloud/test_openai.py
autonomous: true

must_haves:
  truths:
    - "OpenAI cloud backend sends requests to OpenAI API"
    - "Streaming returns async generator of SSE lines"
    - "Non-streaming returns parsed JSON response"
    - "Retry transport handles transient failures"
    - "Circuit breaker prevents cascade failures"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/cloud/openai.py"
      provides: "OpenAI cloud backend client"
      exports: ["OpenAICloudBackend"]
    - path: "backend/tests/mlx_server/services/cloud/test_openai.py"
      provides: "OpenAI cloud backend tests"
      min_lines: 60
  key_links:
    - from: "services/cloud/openai.py"
      to: "services/cloud/client.py"
      via: "inheritance"
      pattern: "class OpenAICloudBackend\\(CloudBackendClient\\)"
---

<objective>
Create OpenAI cloud backend client for fallback routing.

Purpose: Enable routing requests to OpenAI API when local inference unavailable.
Output: OpenAICloudBackend that extends CloudBackendClient with OpenAI-specific API calls.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-RESEARCH.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAI cloud backend</name>
  <files>backend/mlx_manager/mlx_server/services/cloud/openai.py</files>
  <action>
Create OpenAI-specific cloud backend client:

```python
"""OpenAI cloud backend client."""

import json
import logging
from collections.abc import AsyncGenerator
from typing import Any

from mlx_manager.mlx_server.services.cloud.client import CloudBackendClient

logger = logging.getLogger(__name__)

# Default OpenAI API URL
OPENAI_API_URL = "https://api.openai.com"


class OpenAICloudBackend(CloudBackendClient):
    """OpenAI cloud backend for fallback routing.

    Sends requests to OpenAI API in OpenAI format (no translation needed).
    Supports both streaming and non-streaming chat completions.
    """

    def __init__(
        self,
        api_key: str,
        base_url: str = OPENAI_API_URL,
        **kwargs: Any,
    ):
        """Initialize OpenAI cloud backend.

        Args:
            api_key: OpenAI API key
            base_url: API base URL (default: https://api.openai.com)
            **kwargs: Additional args passed to CloudBackendClient
        """
        super().__init__(base_url=base_url, api_key=api_key, **kwargs)

    def _build_headers(self) -> dict[str, str]:
        """Build OpenAI-specific headers."""
        return {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
        }

    async def chat_completion(
        self,
        messages: list[dict[str, Any]],
        model: str,
        max_tokens: int,
        temperature: float = 1.0,
        stream: bool = False,
        **kwargs: Any,
    ) -> AsyncGenerator[dict, None] | dict:
        """Send chat completion request to OpenAI API.

        Args:
            messages: List of message dicts with role and content
            model: Model ID (e.g., "gpt-4", "gpt-3.5-turbo")
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            stream: If True, return async generator of chunks
            **kwargs: Additional OpenAI-specific parameters

        Returns:
            Non-streaming: Complete response dict
            Streaming: Async generator yielding chunk dicts
        """
        request_data = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream,
            **kwargs,
        }

        logger.info(f"OpenAI request: model={model}, stream={stream}")

        if stream:
            return self._stream_chat_completion(request_data)
        else:
            return await self._complete_chat_completion(request_data)

    async def _complete_chat_completion(
        self,
        request_data: dict[str, Any],
    ) -> dict:
        """Non-streaming chat completion."""
        response = await self._post_with_circuit_breaker(
            "/v1/chat/completions",
            request_data,
        )
        return response.json()

    async def _stream_chat_completion(
        self,
        request_data: dict[str, Any],
    ) -> AsyncGenerator[dict, None]:
        """Streaming chat completion."""
        async for line in self._stream_with_circuit_breaker(
            "/v1/chat/completions",
            request_data,
        ):
            # Skip empty lines and [DONE] marker
            if not line or line.strip() == "":
                continue
            if line.startswith("data:"):
                data = line[5:].strip()
                if data == "[DONE]":
                    break
                try:
                    yield json.loads(data)
                except json.JSONDecodeError:
                    logger.warning(f"Failed to parse SSE data: {data}")
                    continue


# Factory function for easy creation
def create_openai_backend(
    api_key: str,
    base_url: str | None = None,
    **kwargs: Any,
) -> OpenAICloudBackend:
    """Create OpenAI cloud backend instance.

    Args:
        api_key: OpenAI API key
        base_url: Optional override for API URL (e.g., Azure OpenAI)
        **kwargs: Additional configuration

    Returns:
        Configured OpenAICloudBackend instance
    """
    return OpenAICloudBackend(
        api_key=api_key,
        base_url=base_url or OPENAI_API_URL,
        **kwargs,
    )
```
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.services.cloud.openai import OpenAICloudBackend, create_openai_backend

# Test initialization (won't make real API calls)
backend = create_openai_backend(api_key='test-key')
print(f'Base URL: {backend.base_url}')
print(f'Circuit open: {backend.is_circuit_open}')
print('OK: OpenAI backend created')
"
  </verify>
  <done>OpenAI cloud backend created</done>
</task>

<task type="auto">
  <name>Task 2: Add OpenAI backend tests</name>
  <files>backend/tests/mlx_server/services/cloud/test_openai.py</files>
  <action>
Create tests for OpenAI cloud backend:

1. Test initialization:
   - Default base_url is OPENAI_API_URL
   - Custom base_url accepted
   - Headers include Authorization with Bearer token

2. Test chat_completion (mock httpx):
   - Non-streaming: calls _post_with_circuit_breaker, returns dict
   - Streaming: calls _stream_with_circuit_breaker, returns generator

3. Test _complete_chat_completion:
   - Parses JSON response correctly
   - Propagates HTTP errors

4. Test _stream_chat_completion:
   - Parses SSE data lines
   - Skips empty lines
   - Handles [DONE] marker
   - Handles malformed JSON gracefully

5. Test factory function:
   - create_openai_backend with defaults
   - create_openai_backend with custom base_url

Use respx or unittest.mock.patch for HTTP mocking.
  </action>
  <verify>cd /Users/atomasini/Development/mlx-manager/backend && python -m pytest tests/mlx_server/services/cloud/test_openai.py -v</verify>
  <done>OpenAI backend tests passing</done>
</task>

</tasks>

<verification>
```bash
cd /Users/atomasini/Development/mlx-manager/backend

# Run tests
python -m pytest tests/mlx_server/services/cloud/test_openai.py -v

# Verify import
python -c "from mlx_manager.mlx_server.services.cloud.openai import OpenAICloudBackend; print('OK')"

# Type checking
mypy mlx_manager/mlx_server/services/cloud/openai.py
```
</verification>

<success_criteria>
- OpenAICloudBackend extends CloudBackendClient
- chat_completion handles both streaming and non-streaming
- SSE parsing handles data lines, empty lines, and [DONE]
- Factory function creates configured instances
- All tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/10-dual-protocol-cloud-fallback/10-06-SUMMARY.md`
</output>
