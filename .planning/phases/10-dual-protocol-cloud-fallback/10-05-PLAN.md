---
phase: 10-dual-protocol-cloud-fallback
plan: 05
type: execute
wave: 3
depends_on: ["10-01", "10-03"]
files_modified:
  - backend/mlx_manager/mlx_server/api/v1/messages.py
  - backend/mlx_manager/mlx_server/api/v1/__init__.py
  - backend/tests/mlx_server/api/v1/test_messages.py
autonomous: true

must_haves:
  truths:
    - "Client can POST to /v1/messages with Anthropic-format request body"
    - "Streaming response returns Anthropic SSE events (event: content_block_delta)"
    - "Non-streaming response returns Anthropic-format JSON with content array"
    - "Response includes stop_reason (end_turn, max_tokens, stop_sequence)"
    - "Response includes usage.input_tokens and usage.output_tokens"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/api/v1/messages.py"
      provides: "Anthropic Messages API endpoint"
      exports: ["router"]
    - path: "backend/tests/mlx_server/api/v1/test_messages.py"
      provides: "Messages endpoint tests"
      min_lines: 100
  key_links:
    - from: "api/v1/messages.py"
      to: "services/protocol.py"
      via: "import get_translator"
      pattern: "from.*services.protocol import"
    - from: "api/v1/messages.py"
      to: "services/inference.py"
      via: "import generate_chat_completion"
      pattern: "from.*services.inference import"
---

<objective>
Create Anthropic-compatible /v1/messages endpoint with protocol translation and Anthropic-format SSE streaming.

Purpose: Enable applications using Anthropic SDK to connect to local MLX inference.
Output: POST /v1/messages endpoint with full Anthropic Messages API compatibility.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-RESEARCH.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-01-SUMMARY.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-03-SUMMARY.md
@backend/mlx_manager/mlx_server/api/v1/chat.py

**Dependency note:** This plan uses `generate_chat_completion` from `services/inference.py` which was created in Phase 9 (continuous batching). This function accepts messages, model_id, max_tokens, temperature, top_p, stop, and stream parameters, returning either a dict (non-streaming) or async generator (streaming) in OpenAI-format.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create /v1/messages endpoint</name>
  <files>backend/mlx_manager/mlx_server/api/v1/messages.py</files>
  <action>
Create the Anthropic Messages API endpoint:

```python
"""Anthropic Messages API endpoint."""

import json
import logging
import time
import uuid
from typing import Any

from fastapi import APIRouter, HTTPException
from sse_starlette.sse import EventSourceResponse

from mlx_manager.mlx_server.schemas.anthropic import (
    AnthropicMessagesRequest,
    AnthropicMessagesResponse,
    TextBlock,
    Usage,
)
from mlx_manager.mlx_server.services.inference import generate_chat_completion
from mlx_manager.mlx_server.services.protocol import get_translator

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/v1", tags=["messages"])


@router.post("/messages", response_model=None)
async def create_message(
    request: AnthropicMessagesRequest,
) -> EventSourceResponse | AnthropicMessagesResponse:
    """Create a message using Anthropic Messages API format.

    Translates Anthropic-format request to internal format, runs inference,
    and returns Anthropic-format response. Supports both streaming and
    non-streaming modes.

    Reference: https://platform.claude.com/docs/en/api/messages
    """
    logger.info(f"Messages request: model={request.model}, stream={request.stream}")

    translator = get_translator()

    try:
        # Convert to internal format
        internal = translator.anthropic_to_internal(request)

        if request.stream:
            return await _handle_streaming(request, internal)
        else:
            return await _handle_non_streaming(request, internal)

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Messages API error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def _handle_non_streaming(
    request: AnthropicMessagesRequest,
    internal: Any,  # InternalRequest from protocol.py
) -> AnthropicMessagesResponse:
    """Handle non-streaming request."""
    translator = get_translator()

    # Generate completion
    result = await generate_chat_completion(
        model_id=internal.model,
        messages=internal.messages,
        max_tokens=internal.max_tokens,
        temperature=internal.temperature,
        top_p=internal.top_p or 1.0,
        stop=internal.stop,
        stream=False,
    )

    # Extract from result dict
    choice = result["choices"][0]
    usage = result["usage"]

    # Translate stop reason
    openai_stop = choice.get("finish_reason", "stop")
    anthropic_stop = translator.openai_stop_to_anthropic(openai_stop)

    return AnthropicMessagesResponse(
        id=result["id"].replace("chatcmpl-", "msg_"),
        model=result["model"],
        content=[TextBlock(text=choice["message"]["content"])],
        stop_reason=anthropic_stop,
        usage=Usage(
            input_tokens=usage["prompt_tokens"],
            output_tokens=usage["completion_tokens"],
        ),
    )


async def _handle_streaming(
    request: AnthropicMessagesRequest,
    internal: Any,  # InternalRequest from protocol.py
) -> EventSourceResponse:
    """Handle streaming request with Anthropic-format SSE events.

    Anthropic streaming uses named event types (not just data: lines):
    - event: message_start (initial message metadata)
    - event: content_block_start (begin content block)
    - event: content_block_delta (token chunks)
    - event: content_block_stop (end content block)
    - event: message_delta (final stop_reason and usage)
    - event: message_stop (stream complete)

    The internal generate_chat_completion returns OpenAI-format chunks which
    we translate to Anthropic events by wrapping each token in content_block_delta.
    """
    translator = get_translator()
    request_id = f"msg_{uuid.uuid4().hex[:24]}"
    created = int(time.time())

    async def generate_events() -> Any:
        """Generate Anthropic-format SSE events.

        Translation flow:
        1. Emit Anthropic message_start and content_block_start events
        2. For each OpenAI chunk from generate_chat_completion:
           - Extract delta.content (token text)
           - Wrap in Anthropic content_block_delta event format
        3. Emit Anthropic closing events with translated stop_reason
        """
        output_tokens = 0

        # 1. message_start event - Anthropic requires this as first event
        yield {
            "event": "message_start",
            "data": json.dumps({
                "type": "message_start",
                "message": {
                    "id": request_id,
                    "type": "message",
                    "role": "assistant",
                    "content": [],
                    "model": request.model,
                    "stop_reason": None,
                    "stop_sequence": None,
                    "usage": {"input_tokens": 0, "output_tokens": 0},
                },
            }),
        }

        # 2. content_block_start event - signals beginning of text block
        yield {
            "event": "content_block_start",
            "data": json.dumps({
                "type": "content_block_start",
                "index": 0,
                "content_block": {"type": "text", "text": ""},
            }),
        }

        # 3. Stream tokens as content_block_delta events
        # generate_chat_completion returns OpenAI-format chunks:
        # {"choices": [{"delta": {"content": "token"}, "finish_reason": null}]}
        finish_reason = "end_turn"
        gen = await generate_chat_completion(
            model_id=internal.model,
            messages=internal.messages,
            max_tokens=internal.max_tokens,
            temperature=internal.temperature,
            top_p=internal.top_p or 1.0,
            stop=internal.stop,
            stream=True,
        )

        async for chunk in gen:
            # Extract token from OpenAI-format chunk
            choices = chunk.get("choices", [])
            if choices:
                delta = choices[0].get("delta", {})
                token_text = delta.get("content", "")
                chunk_finish = choices[0].get("finish_reason")

                # Translate OpenAI chunk -> Anthropic content_block_delta
                if token_text:
                    output_tokens += 1
                    yield {
                        "event": "content_block_delta",
                        "data": json.dumps({
                            "type": "content_block_delta",
                            "index": 0,
                            "delta": {"type": "text_delta", "text": token_text},
                        }),
                    }

                # Capture finish_reason for translation at end
                if chunk_finish:
                    finish_reason = translator.openai_stop_to_anthropic(chunk_finish)

        # 4. content_block_stop event - signals end of text block
        yield {
            "event": "content_block_stop",
            "data": json.dumps({
                "type": "content_block_stop",
                "index": 0,
            }),
        }

        # 5. message_delta event - contains final stop_reason and usage
        yield {
            "event": "message_delta",
            "data": json.dumps({
                "type": "message_delta",
                "delta": {"stop_reason": finish_reason, "stop_sequence": None},
                "usage": {"output_tokens": output_tokens},
            }),
        }

        # 6. message_stop event - signals stream complete
        yield {
            "event": "message_stop",
            "data": json.dumps({"type": "message_stop"}),
        }

    return EventSourceResponse(generate_events())
```
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.api.v1.messages import router
print(f'Router prefix: {router.prefix}')
print(f'Routes: {[r.path for r in router.routes]}')
print('OK: Messages router created')
"
  </verify>
  <done>/v1/messages endpoint created with Anthropic-format streaming</done>
</task>

<task type="auto">
  <name>Task 2: Register messages router</name>
  <files>backend/mlx_manager/mlx_server/api/v1/__init__.py</files>
  <action>
Add the messages router to the v1 API:

1. Import the messages router:
```python
from mlx_manager.mlx_server.api.v1.messages import router as messages_router
```

2. Add to __all__ or include in the main router if using include_router pattern.

Check the current structure of api/v1/__init__.py and follow the existing pattern
for registering routers.
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.api.v1 import messages
print('OK: Messages module accessible from api.v1')
"
  </verify>
  <done>Messages router registered in api/v1</done>
</task>

<task type="auto">
  <name>Task 3: Add messages endpoint tests</name>
  <files>backend/tests/mlx_server/api/v1/test_messages.py</files>
  <action>
Create comprehensive tests for /v1/messages endpoint:

1. Test request validation:
   - Valid request with required max_tokens
   - Missing max_tokens returns 422
   - System message accepted

2. Test non-streaming response (mock generate_chat_completion):
   - Response has Anthropic format (id starts with msg_, content is list of TextBlock)
   - stop_reason translated correctly
   - Usage included

3. Test streaming response:
   - Returns EventSourceResponse
   - First event is message_start
   - Token events are content_block_delta with named event type
   - Final events include content_block_stop, message_delta, message_stop

4. Test protocol translation integration:
   - System message placed in internal messages array
   - Temperature/top_p passed through

Use FastAPI TestClient and mock the generate_chat_completion function.
  </action>
  <verify>cd /Users/atomasini/Development/mlx-manager/backend && python -m pytest tests/mlx_server/api/v1/test_messages.py -v</verify>
  <done>Messages endpoint tests passing</done>
</task>

</tasks>

<verification>
```bash
cd /Users/atomasini/Development/mlx-manager/backend

# Run tests
python -m pytest tests/mlx_server/api/v1/test_messages.py -v

# Verify endpoint is registered
python -c "
from mlx_manager.mlx_server.main import app
routes = [r.path for r in app.routes]
assert '/v1/messages' in str(routes), 'Messages endpoint not found'
print('OK: /v1/messages endpoint registered')
"

# Type checking
mypy mlx_manager/mlx_server/api/v1/messages.py
```
</verification>

<success_criteria>
- POST /v1/messages accepts Anthropic-format requests
- Streaming uses Anthropic SSE format with named event types (event: content_block_delta)
- Non-streaming returns AnthropicMessagesResponse
- stop_reason correctly translated between formats
- All tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/10-dual-protocol-cloud-fallback/10-05-SUMMARY.md`
</output>
