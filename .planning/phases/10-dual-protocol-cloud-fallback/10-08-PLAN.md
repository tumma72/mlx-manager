---
phase: 10-dual-protocol-cloud-fallback
plan: 08
type: execute
wave: 4
depends_on: ["10-02", "10-06", "10-07"]
files_modified:
  - backend/mlx_manager/mlx_server/services/cloud/router.py
  - backend/mlx_manager/mlx_server/services/cloud/__init__.py
  - backend/tests/mlx_server/services/cloud/test_router.py
autonomous: true

must_haves:
  truths:
    - "Backend router looks up mapping for model"
    - "Pattern matching works for both exact and wildcard patterns"
    - "Local backend tried first when configured"
    - "Fallback to cloud backend on local failure"
    - "Cloud credentials loaded from database"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/cloud/router.py"
      provides: "BackendRouter with failover logic"
      exports: ["BackendRouter", "get_router"]
    - path: "backend/tests/mlx_server/services/cloud/test_router.py"
      provides: "Router tests"
      min_lines: 100
  key_links:
    - from: "services/cloud/router.py"
      to: "models.py"
      via: "BackendMapping import"
      pattern: "from mlx_manager.models import BackendMapping"
    - from: "services/cloud/router.py"
      to: "services/cloud/openai.py"
      via: "import"
      pattern: "from.*services.cloud.openai import"
---

<objective>
Create backend router that routes requests based on model mapping with automatic failover.

Purpose: Enable intelligent routing between local MLX inference and cloud backends based on configuration, with automatic failover on local failure.
Output: BackendRouter service that manages backend selection and failover.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-RESEARCH.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-02-SUMMARY.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-06-SUMMARY.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-07-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create backend router service</name>
  <files>backend/mlx_manager/mlx_server/services/cloud/router.py</files>
  <action>
Create BackendRouter with pattern matching and failover.

**Note on scope:** This task creates a ~330 line file with tightly coupled concerns (pattern matching, mapping lookup, backend instantiation, routing logic). These are kept together intentionally because:
1. Pattern matching is only used by mapping lookup
2. Backend instantiation is only triggered by routing
3. Splitting would create artificial boundaries and cross-file coordination overhead

The complexity is inherent to the routing domain, not excessive coupling.

```python
"""Backend router with failover logic."""

import fnmatch
import logging
from collections.abc import AsyncGenerator
from typing import Any

from sqlalchemy import select
from sqlmodel.ext.asyncio.session import AsyncSession

from mlx_manager.database import get_db
from mlx_manager.models import BackendMapping, BackendType, CloudCredential
from mlx_manager.mlx_server.services.cloud.anthropic import AnthropicCloudBackend
from mlx_manager.mlx_server.services.cloud.openai import OpenAICloudBackend

logger = logging.getLogger(__name__)


class BackendRouter:
    """Routes requests to appropriate backend with failover support.

    Looks up backend mapping for model, routes to local or cloud,
    and handles automatic failover on local failure.
    """

    def __init__(self):
        """Initialize router."""
        self._cloud_backends: dict[BackendType, OpenAICloudBackend | AnthropicCloudBackend] = {}

    async def route_request(
        self,
        model: str,
        messages: list[dict[str, Any]],
        max_tokens: int,
        temperature: float = 1.0,
        stream: bool = False,
        db: AsyncSession | None = None,
        **kwargs: Any,
    ) -> AsyncGenerator[dict, None] | dict:
        """Route request to appropriate backend.

        Args:
            model: Model ID to route
            messages: Chat messages
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            stream: If True, return async generator
            db: Database session (optional, will create if needed)
            **kwargs: Additional parameters

        Returns:
            Response from selected backend
        """
        # Get or create database session
        if db is None:
            async for session in get_db():
                return await self._route_with_session(
                    session, model, messages, max_tokens, temperature, stream, **kwargs
                )
            raise RuntimeError("Failed to get database session")
        else:
            return await self._route_with_session(
                db, model, messages, max_tokens, temperature, stream, **kwargs
            )

    async def _route_with_session(
        self,
        db: AsyncSession,
        model: str,
        messages: list[dict[str, Any]],
        max_tokens: int,
        temperature: float,
        stream: bool,
        **kwargs: Any,
    ) -> AsyncGenerator[dict, None] | dict:
        """Route with database session."""
        # Find matching backend mapping
        mapping = await self._find_mapping(db, model)

        if mapping is None:
            # No mapping - default to local
            logger.info(f"No mapping for {model}, using local")
            return await self._route_local(
                model, messages, max_tokens, temperature, stream, **kwargs
            )

        logger.info(f"Routing {model} to {mapping.backend_type.value}")

        # Route based on backend type
        if mapping.backend_type == BackendType.LOCAL:
            try:
                return await self._route_local(
                    model, messages, max_tokens, temperature, stream, **kwargs
                )
            except Exception as e:
                if mapping.fallback_backend:
                    logger.warning(f"Local failed, falling back to {mapping.fallback_backend}: {e}")
                    return await self._route_cloud(
                        db,
                        mapping.fallback_backend,
                        mapping.backend_model or model,
                        messages,
                        max_tokens,
                        temperature,
                        stream,
                        **kwargs,
                    )
                raise
        else:
            # Cloud backend
            return await self._route_cloud(
                db,
                mapping.backend_type,
                mapping.backend_model or model,
                messages,
                max_tokens,
                temperature,
                stream,
                **kwargs,
            )

    async def _find_mapping(
        self,
        db: AsyncSession,
        model: str,
    ) -> BackendMapping | None:
        """Find backend mapping for model.

        Matches in priority order:
        1. Exact model name match
        2. Wildcard pattern match (e.g., "gpt-*")
        """
        # Get all enabled mappings ordered by priority (higher first)
        result = await db.execute(
            select(BackendMapping)
            .where(BackendMapping.enabled == True)  # noqa: E712
            .order_by(BackendMapping.priority.desc())
        )
        mappings = result.scalars().all()

        # Find first matching pattern
        for mapping in mappings:
            if self._pattern_matches(mapping.model_pattern, model):
                return mapping

        return None

    def _pattern_matches(self, pattern: str, model: str) -> bool:
        """Check if pattern matches model name.

        Supports:
        - Exact match: "gpt-4" matches "gpt-4"
        - Wildcard: "gpt-*" matches "gpt-4", "gpt-4-turbo"
        - Fnmatch patterns: "claude-3-*" matches "claude-3-opus"
        """
        # Exact match first
        if pattern == model:
            return True
        # Wildcard pattern
        return fnmatch.fnmatch(model, pattern)

    async def _route_local(
        self,
        model: str,
        messages: list[dict[str, Any]],
        max_tokens: int,
        temperature: float,
        stream: bool,
        **kwargs: Any,
    ) -> AsyncGenerator[dict, None] | dict:
        """Route to local MLX inference."""
        from mlx_manager.mlx_server.services.inference import generate_chat_completion

        return await generate_chat_completion(
            model_id=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            **kwargs,
        )

    async def _route_cloud(
        self,
        db: AsyncSession,
        backend_type: BackendType,
        model: str,
        messages: list[dict[str, Any]],
        max_tokens: int,
        temperature: float,
        stream: bool,
        **kwargs: Any,
    ) -> AsyncGenerator[dict, None] | dict:
        """Route to cloud backend."""
        # Get or create cloud backend client
        backend = await self._get_cloud_backend(db, backend_type)

        return await backend.chat_completion(
            messages=messages,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            **kwargs,
        )

    async def _get_cloud_backend(
        self,
        db: AsyncSession,
        backend_type: BackendType,
    ) -> OpenAICloudBackend | AnthropicCloudBackend:
        """Get or create cloud backend client."""
        if backend_type in self._cloud_backends:
            return self._cloud_backends[backend_type]

        # Load credentials from database
        result = await db.execute(
            select(CloudCredential).where(CloudCredential.backend_type == backend_type)
        )
        credential = result.scalar_one_or_none()

        if credential is None:
            raise ValueError(f"No credentials configured for {backend_type.value}")

        # Create backend client
        # Note: In Phase 11, encrypted_api_key will be decrypted here
        api_key = credential.encrypted_api_key  # TODO: Decrypt in Phase 11
        base_url = credential.base_url

        if backend_type == BackendType.OPENAI:
            backend = OpenAICloudBackend(
                api_key=api_key,
                base_url=base_url or "https://api.openai.com",
            )
        elif backend_type == BackendType.ANTHROPIC:
            backend = AnthropicCloudBackend(
                api_key=api_key,
                base_url=base_url or "https://api.anthropic.com",
            )
        else:
            raise ValueError(f"Unknown backend type: {backend_type}")

        self._cloud_backends[backend_type] = backend
        return backend

    async def close(self) -> None:
        """Close all cloud backend clients."""
        for backend in self._cloud_backends.values():
            await backend.close()
        self._cloud_backends.clear()


# Module-level singleton
_router: BackendRouter | None = None


def get_router() -> BackendRouter:
    """Get the backend router singleton."""
    global _router
    if _router is None:
        _router = BackendRouter()
    return _router


async def reset_router() -> None:
    """Reset the router (for testing)."""
    global _router
    if _router:
        await _router.close()
    _router = None
```
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.services.cloud.router import BackendRouter, get_router

router = get_router()
# Test pattern matching
assert router._pattern_matches('gpt-4', 'gpt-4') == True
assert router._pattern_matches('gpt-*', 'gpt-4') == True
assert router._pattern_matches('gpt-*', 'gpt-4-turbo') == True
assert router._pattern_matches('gpt-*', 'claude-3') == False
print('OK: BackendRouter created and pattern matching works')
"
  </verify>
  <done>BackendRouter created with pattern matching and failover</done>
</task>

<task type="auto">
  <name>Task 2: Update cloud package exports</name>
  <files>backend/mlx_manager/mlx_server/services/cloud/__init__.py</files>
  <action>
Add BackendRouter exports:

```python
"""Cloud backend services for fallback routing."""

from mlx_manager.mlx_server.services.cloud.client import CloudBackendClient
from mlx_manager.mlx_server.services.cloud.openai import OpenAICloudBackend, create_openai_backend
from mlx_manager.mlx_server.services.cloud.anthropic import AnthropicCloudBackend, create_anthropic_backend
from mlx_manager.mlx_server.services.cloud.router import BackendRouter, get_router, reset_router

__all__ = [
    "CloudBackendClient",
    "OpenAICloudBackend",
    "create_openai_backend",
    "AnthropicCloudBackend",
    "create_anthropic_backend",
    "BackendRouter",
    "get_router",
    "reset_router",
]
```
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.services.cloud import BackendRouter, get_router
print('OK: BackendRouter importable from package')
"
  </verify>
  <done>Cloud package exports updated</done>
</task>

<task type="auto">
  <name>Task 3: Add router tests</name>
  <files>backend/tests/mlx_server/services/cloud/test_router.py</files>
  <action>
Create comprehensive tests for BackendRouter:

1. Test pattern matching:
   - Exact match: "gpt-4" matches "gpt-4"
   - Wildcard: "gpt-*" matches "gpt-4", "gpt-4-turbo"
   - Non-match: "gpt-*" does not match "claude-3"

2. Test _find_mapping:
   - Returns None when no mappings exist
   - Returns exact match over wildcard
   - Respects priority order
   - Only returns enabled mappings

3. Test route_request:
   - No mapping defaults to local
   - LOCAL backend routes to local inference
   - OPENAI backend routes to OpenAI cloud
   - ANTHROPIC backend routes to Anthropic cloud

4. Test failover:
   - Local failure with fallback routes to cloud
   - Local failure without fallback raises exception

5. Test _get_cloud_backend:
   - Creates backend from credentials
   - Caches backend for reuse
   - Raises ValueError when no credentials

6. Test singleton:
   - get_router returns same instance
   - reset_router clears instance

Mock database, local inference, and cloud backends appropriately.
  </action>
  <verify>cd /Users/atomasini/Development/mlx-manager/backend && python -m pytest tests/mlx_server/services/cloud/test_router.py -v</verify>
  <done>Router tests passing</done>
</task>

</tasks>

<verification>
```bash
cd /Users/atomasini/Development/mlx-manager/backend

# Run tests
python -m pytest tests/mlx_server/services/cloud/test_router.py -v

# Verify imports
python -c "from mlx_manager.mlx_server.services.cloud import BackendRouter, get_router; print('OK')"

# Type checking
mypy mlx_manager/mlx_server/services/cloud/router.py
```
</verification>

<success_criteria>
- BackendRouter finds mapping for model patterns
- Local backend used when configured or no mapping
- Failover to cloud on local failure when fallback configured
- Cloud credentials loaded from database
- Pattern matching supports exact and wildcard
- All tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/10-dual-protocol-cloud-fallback/10-08-SUMMARY.md`
</output>
