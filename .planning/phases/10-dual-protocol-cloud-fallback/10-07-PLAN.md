---
phase: 10-dual-protocol-cloud-fallback
plan: 07
type: execute
wave: 3
depends_on: ["10-03", "10-04"]
files_modified:
  - backend/mlx_manager/mlx_server/services/cloud/anthropic.py
  - backend/mlx_manager/mlx_server/services/cloud/__init__.py
  - backend/tests/mlx_server/services/cloud/test_anthropic.py
autonomous: true

must_haves:
  truths:
    - "Anthropic cloud backend sends requests to Anthropic API"
    - "Requests translated from OpenAI format to Anthropic format"
    - "Responses translated back to OpenAI format"
    - "Streaming parses Anthropic SSE event format"
    - "Circuit breaker and retry work correctly"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/services/cloud/anthropic.py"
      provides: "Anthropic cloud backend with format translation"
      exports: ["AnthropicCloudBackend"]
    - path: "backend/tests/mlx_server/services/cloud/test_anthropic.py"
      provides: "Anthropic cloud backend tests"
      min_lines: 80
  key_links:
    - from: "services/cloud/anthropic.py"
      to: "services/protocol.py"
      via: "import get_translator"
      pattern: "from.*services.protocol import"
    - from: "services/cloud/anthropic.py"
      to: "services/cloud/client.py"
      via: "inheritance"
      pattern: "class AnthropicCloudBackend\\(CloudBackendClient\\)"
---

<objective>
Create Anthropic cloud backend client with automatic format translation.

Purpose: Enable routing requests to Anthropic API when local inference unavailable, with automatic OpenAI<->Anthropic format translation.
Output: AnthropicCloudBackend that handles format conversion transparently.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-RESEARCH.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-03-SUMMARY.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Anthropic cloud backend</name>
  <files>backend/mlx_manager/mlx_server/services/cloud/anthropic.py</files>
  <action>
Create Anthropic-specific cloud backend with format translation:

```python
"""Anthropic cloud backend client with format translation."""

import json
import logging
from collections.abc import AsyncGenerator
from typing import Any

from mlx_manager.mlx_server.services.cloud.client import CloudBackendClient
from mlx_manager.mlx_server.services.protocol import get_translator

logger = logging.getLogger(__name__)

# Default Anthropic API URL
ANTHROPIC_API_URL = "https://api.anthropic.com"
ANTHROPIC_VERSION = "2023-06-01"


class AnthropicCloudBackend(CloudBackendClient):
    """Anthropic cloud backend with automatic format translation.

    Accepts OpenAI-format requests, translates to Anthropic format,
    sends to Anthropic API, and translates responses back to OpenAI format.
    This enables transparent fallback from local to Anthropic cloud.
    """

    def __init__(
        self,
        api_key: str,
        base_url: str = ANTHROPIC_API_URL,
        anthropic_version: str = ANTHROPIC_VERSION,
        **kwargs: Any,
    ):
        """Initialize Anthropic cloud backend.

        Args:
            api_key: Anthropic API key
            base_url: API base URL
            anthropic_version: Anthropic API version header
            **kwargs: Additional args for CloudBackendClient
        """
        self._anthropic_version = anthropic_version
        super().__init__(base_url=base_url, api_key=api_key, **kwargs)

    def _build_headers(self) -> dict[str, str]:
        """Build Anthropic-specific headers."""
        return {
            "x-api-key": self._api_key,
            "anthropic-version": self._anthropic_version,
            "Content-Type": "application/json",
        }

    async def chat_completion(
        self,
        messages: list[dict[str, Any]],
        model: str,
        max_tokens: int,
        temperature: float = 1.0,
        stream: bool = False,
        **kwargs: Any,
    ) -> AsyncGenerator[dict, None] | dict:
        """Send chat completion with format translation.

        Accepts OpenAI-format input, translates to Anthropic Messages format,
        and translates response back to OpenAI format.

        Args:
            messages: OpenAI-format messages [{"role": str, "content": str}]
            model: Model ID (e.g., "claude-3-opus-20240229")
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            stream: If True, return async generator
            **kwargs: Additional parameters

        Returns:
            OpenAI-format response (dict or async generator)
        """
        # Translate OpenAI messages to Anthropic format
        anthropic_request = self._translate_request(
            messages, model, max_tokens, temperature, stream, **kwargs
        )

        logger.info(f"Anthropic request: model={model}, stream={stream}")

        if stream:
            return self._stream_with_translation(anthropic_request)
        else:
            return await self._complete_with_translation(anthropic_request)

    def _translate_request(
        self,
        messages: list[dict[str, Any]],
        model: str,
        max_tokens: int,
        temperature: float,
        stream: bool,
        **kwargs: Any,
    ) -> dict[str, Any]:
        """Translate OpenAI-format request to Anthropic format."""
        # Extract system message (Anthropic has separate field)
        system_content: str | None = None
        anthropic_messages: list[dict[str, str]] = []

        for msg in messages:
            if msg["role"] == "system":
                system_content = msg["content"]
            else:
                anthropic_messages.append({
                    "role": msg["role"],
                    "content": msg["content"],
                })

        request = {
            "model": model,
            "max_tokens": max_tokens,
            "messages": anthropic_messages,
            "temperature": temperature,
            "stream": stream,
        }

        if system_content:
            request["system"] = system_content

        # Map OpenAI stop to Anthropic stop_sequences
        if "stop" in kwargs and kwargs["stop"]:
            stop = kwargs["stop"]
            if isinstance(stop, str):
                request["stop_sequences"] = [stop]
            else:
                request["stop_sequences"] = stop

        return request

    async def _complete_with_translation(
        self,
        request_data: dict[str, Any],
    ) -> dict:
        """Non-streaming with response translation."""
        response = await self._post_with_circuit_breaker(
            "/v1/messages",
            request_data,
        )
        anthropic_response = response.json()
        return self._translate_response(anthropic_response)

    def _translate_response(self, anthropic_response: dict) -> dict:
        """Translate Anthropic response to OpenAI format."""
        translator = get_translator()

        # Extract content text
        content_blocks = anthropic_response.get("content", [])
        content_text = " ".join(
            block.get("text", "") for block in content_blocks if block.get("type") == "text"
        )

        # Translate stop reason
        anthropic_stop = anthropic_response.get("stop_reason")
        openai_stop = translator.anthropic_stop_to_openai(anthropic_stop)

        # Build OpenAI-format response
        usage = anthropic_response.get("usage", {})
        return {
            "id": anthropic_response.get("id", "").replace("msg_", "chatcmpl-"),
            "object": "chat.completion",
            "created": 0,  # Anthropic doesn't include timestamp
            "model": anthropic_response.get("model", ""),
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content_text,
                },
                "finish_reason": openai_stop,
            }],
            "usage": {
                "prompt_tokens": usage.get("input_tokens", 0),
                "completion_tokens": usage.get("output_tokens", 0),
                "total_tokens": usage.get("input_tokens", 0) + usage.get("output_tokens", 0),
            },
        }

    async def _stream_with_translation(
        self,
        request_data: dict[str, Any],
    ) -> AsyncGenerator[dict, None]:
        """Streaming with response translation to OpenAI format."""
        translator = get_translator()
        current_text = ""

        async for line in self._stream_with_circuit_breaker(
            "/v1/messages",
            request_data,
        ):
            if not line or line.strip() == "":
                continue

            # Parse Anthropic SSE format: "event: type\ndata: {...}"
            if line.startswith("event:"):
                event_type = line[6:].strip()
                continue

            if line.startswith("data:"):
                data_str = line[5:].strip()
                if not data_str:
                    continue

                try:
                    data = json.loads(data_str)
                except json.JSONDecodeError:
                    continue

                event_type = data.get("type", "")

                # Handle content_block_delta events
                if event_type == "content_block_delta":
                    delta = data.get("delta", {})
                    if delta.get("type") == "text_delta":
                        token_text = delta.get("text", "")
                        if token_text:
                            yield {
                                "id": "chatcmpl-streaming",
                                "object": "chat.completion.chunk",
                                "created": 0,
                                "model": request_data.get("model", ""),
                                "choices": [{
                                    "index": 0,
                                    "delta": {"content": token_text},
                                    "finish_reason": None,
                                }],
                            }

                # Handle message_delta for finish reason
                elif event_type == "message_delta":
                    stop_reason = data.get("delta", {}).get("stop_reason")
                    if stop_reason:
                        openai_stop = translator.anthropic_stop_to_openai(stop_reason)
                        yield {
                            "id": "chatcmpl-streaming",
                            "object": "chat.completion.chunk",
                            "created": 0,
                            "model": request_data.get("model", ""),
                            "choices": [{
                                "index": 0,
                                "delta": {},
                                "finish_reason": openai_stop,
                            }],
                        }

                # Handle message_stop
                elif event_type == "message_stop":
                    break


# Factory function
def create_anthropic_backend(
    api_key: str,
    base_url: str | None = None,
    **kwargs: Any,
) -> AnthropicCloudBackend:
    """Create Anthropic cloud backend instance."""
    return AnthropicCloudBackend(
        api_key=api_key,
        base_url=base_url or ANTHROPIC_API_URL,
        **kwargs,
    )
```
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.services.cloud.anthropic import AnthropicCloudBackend, create_anthropic_backend

backend = create_anthropic_backend(api_key='test-key')
print(f'Base URL: {backend.base_url}')
print('OK: Anthropic backend created')
"
  </verify>
  <done>Anthropic cloud backend created with format translation</done>
</task>

<task type="auto">
  <name>Task 2: Update cloud package exports</name>
  <files>backend/mlx_manager/mlx_server/services/cloud/__init__.py</files>
  <action>
Update __init__.py to export all cloud backends:

```python
"""Cloud backend services for fallback routing."""

from mlx_manager.mlx_server.services.cloud.client import CloudBackendClient
from mlx_manager.mlx_server.services.cloud.openai import OpenAICloudBackend, create_openai_backend
from mlx_manager.mlx_server.services.cloud.anthropic import AnthropicCloudBackend, create_anthropic_backend

__all__ = [
    "CloudBackendClient",
    "OpenAICloudBackend",
    "create_openai_backend",
    "AnthropicCloudBackend",
    "create_anthropic_backend",
]
```
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.services.cloud import (
    CloudBackendClient,
    OpenAICloudBackend,
    AnthropicCloudBackend,
    create_openai_backend,
    create_anthropic_backend,
)
print('OK: All cloud backends importable from package')
"
  </verify>
  <done>Cloud package exports updated</done>
</task>

<task type="auto">
  <name>Task 3: Add Anthropic backend tests</name>
  <files>backend/tests/mlx_server/services/cloud/test_anthropic.py</files>
  <action>
Create tests for Anthropic cloud backend:

1. Test initialization:
   - Headers include x-api-key and anthropic-version
   - Custom base_url accepted

2. Test _translate_request:
   - System message extracted to separate field
   - User/assistant messages preserved
   - Temperature and max_tokens mapped
   - Stop sequences translated from stop parameter

3. Test _translate_response:
   - Content blocks concatenated to single string
   - Stop reason translated to OpenAI format
   - Usage mapped correctly
   - ID prefix changed (msg_ -> chatcmpl-)

4. Test _stream_with_translation:
   - content_block_delta events yield OpenAI chunks
   - message_delta with stop_reason yields finish chunk
   - message_stop ends stream
   - Empty lines and malformed JSON handled

5. Test chat_completion integration:
   - Non-streaming returns translated dict
   - Streaming returns translated generator

Use respx or unittest.mock for HTTP mocking.
  </action>
  <verify>cd /Users/atomasini/Development/mlx-manager/backend && python -m pytest tests/mlx_server/services/cloud/test_anthropic.py -v</verify>
  <done>Anthropic backend tests passing</done>
</task>

</tasks>

<verification>
```bash
cd /Users/atomasini/Development/mlx-manager/backend

# Run tests
python -m pytest tests/mlx_server/services/cloud/test_anthropic.py -v

# Verify imports
python -c "from mlx_manager.mlx_server.services.cloud import AnthropicCloudBackend; print('OK')"

# Type checking
mypy mlx_manager/mlx_server/services/cloud/anthropic.py
```
</verification>

<success_criteria>
- AnthropicCloudBackend accepts OpenAI-format input
- _translate_request extracts system message to separate field
- _translate_response converts content blocks and stop reason
- Streaming parses Anthropic SSE format correctly
- All tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/10-dual-protocol-cloud-fallback/10-07-SUMMARY.md`
</output>
