---
phase: 10-dual-protocol-cloud-fallback
plan: 09
type: execute
wave: 4
depends_on: ["10-08"]
files_modified:
  - backend/mlx_manager/mlx_server/api/v1/chat.py
  - backend/mlx_manager/mlx_server/config.py
  - backend/tests/mlx_server/api/v1/test_chat_routing.py
autonomous: true

must_haves:
  truths:
    - "Chat endpoint routes through backend router when enabled"
    - "Fallback to local inference when router unavailable"
    - "Config has enable_cloud_routing setting"
    - "Router handles model-to-backend mapping lookup"
  artifacts:
    - path: "backend/mlx_manager/mlx_server/api/v1/chat.py"
      provides: "Chat endpoint with routing integration"
      contains: "get_router"
    - path: "backend/tests/mlx_server/api/v1/test_chat_routing.py"
      provides: "Routing integration tests"
      min_lines: 60
  key_links:
    - from: "api/v1/chat.py"
      to: "services/cloud/router.py"
      via: "import get_router"
      pattern: "from.*services.cloud.router import|from.*services.cloud import.*get_router"
---

<objective>
Wire backend router into the chat completions endpoint for automatic routing and failover.

Purpose: Enable the existing /v1/chat/completions endpoint to route through the backend router for cloud fallback.
Output: Chat endpoint integrated with BackendRouter when cloud routing is enabled.
</objective>

<execution_context>
@.claude/get-shit-done/workflows/execute-plan.md
@.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-dual-protocol-cloud-fallback/10-08-SUMMARY.md
@backend/mlx_manager/mlx_server/api/v1/chat.py
@backend/mlx_manager/mlx_server/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add routing config setting</name>
  <files>backend/mlx_manager/mlx_server/config.py</files>
  <action>
Add cloud routing configuration to MLX server settings:

```python
# Add to Settings class:
enable_cloud_routing: bool = False  # Enable backend router for cloud fallback
```

This setting controls whether the chat endpoint uses the backend router.
When False (default), all requests go directly to local inference.
When True, requests go through the router which checks mappings and handles failover.
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.config import get_settings
settings = get_settings()
print(f'enable_cloud_routing: {settings.enable_cloud_routing}')
assert settings.enable_cloud_routing == False, 'Should default to False'
print('OK: Config setting added')
"
  </verify>
  <done>enable_cloud_routing config setting added</done>
</task>

<task type="auto">
  <name>Task 2: Integrate router into chat endpoint</name>
  <files>backend/mlx_manager/mlx_server/api/v1/chat.py</files>
  <action>
Update the chat.py endpoint to route through BackendRouter when enabled:

1. Add import at top:
```python
from mlx_manager.mlx_server.services.cloud import get_router
```

2. Update _handle_text_request function to add routing path:

```python
async def _handle_text_request(
    request: ChatCompletionRequest,
) -> EventSourceResponse | ChatCompletionResponse:
    """Handle text-only request.

    Routes through:
    1. Backend router (if cloud routing enabled) - handles model mapping and failover
    2. Batching scheduler (if batching enabled) - handles concurrent request batching
    3. Direct inference - direct call to MLX
    """
    settings = get_settings()

    # Cloud routing path - handles model mapping and failover
    if settings.enable_cloud_routing:
        try:
            return await _handle_routed_request(request)
        except Exception as e:
            logger.warning(f"Cloud routing failed, falling back to local: {e}")
            # Fall through to direct/batched path

    # Existing batching and direct paths remain unchanged
    if settings.enable_batching:
        try:
            mgr = get_scheduler_manager()
            priority = mgr.get_priority_for_request(
                api_key=None,
                endpoint="/v1/chat/completions",
            )
            return await _handle_batched_request(request, priority)
        except RuntimeError:
            logger.warning("Batching enabled but scheduler not initialized, using direct")
        except Exception as e:
            logger.warning(f"Batching unavailable, falling back to direct: {e}")

    return await _handle_direct_request(request)


async def _handle_routed_request(
    request: ChatCompletionRequest,
) -> EventSourceResponse | ChatCompletionResponse:
    """Handle request through backend router."""
    router = get_router()

    # Convert messages to dict format
    messages: list[dict[str, Any]] = []
    for m in request.messages:
        if isinstance(m.content, str):
            text = m.content
        else:
            text, _ = extract_content_parts(m.content)
        messages.append({"role": m.role, "content": text})

    # Handle stop parameter
    stop: list[str] | None = (
        request.stop
        if isinstance(request.stop, list)
        else ([request.stop] if request.stop else None)
    )

    # Route through backend router
    result = await router.route_request(
        model=request.model,
        messages=messages,
        max_tokens=request.max_tokens or 4096,
        temperature=request.temperature,
        stream=request.stream,
        top_p=request.top_p,
        stop=stop,
    )

    if request.stream:
        # Result is async generator
        async def event_generator() -> Any:
            async for chunk in result:  # type: ignore[union-attr]
                yield {"data": json.dumps(chunk)}
            yield {"data": "[DONE]"}

        return EventSourceResponse(event_generator())
    else:
        # Result is dict, convert to response model
        result_dict = cast(dict[str, Any], result)
        choice = result_dict["choices"][0]
        return ChatCompletionResponse(
            id=result_dict["id"],
            created=result_dict.get("created", int(time.time())),
            model=result_dict["model"],
            choices=[
                ChatCompletionChoice(
                    index=choice["index"],
                    message=ChatMessage(
                        role=choice["message"]["role"],
                        content=choice["message"]["content"],
                    ),
                    finish_reason=choice.get("finish_reason"),
                )
            ],
            usage=Usage(
                prompt_tokens=result_dict["usage"]["prompt_tokens"],
                completion_tokens=result_dict["usage"]["completion_tokens"],
                total_tokens=result_dict["usage"]["total_tokens"],
            ),
        )
```

Add necessary imports: `time`, and ensure `Any` and `cast` are imported.
  </action>
  <verify>
cd /Users/atomasini/Development/mlx-manager/backend && python -c "
from mlx_manager.mlx_server.api.v1.chat import router, _handle_routed_request
print(f'Router routes: {[r.path for r in router.routes]}')
print('OK: Chat endpoint updated with routing')
"
  </verify>
  <done>Chat endpoint integrated with backend router</done>
</task>

<task type="auto">
  <name>Task 3: Add routing integration tests</name>
  <files>backend/tests/mlx_server/api/v1/test_chat_routing.py</files>
  <action>
Create tests for chat endpoint routing integration:

1. Test routing disabled (default):
   - Request goes directly to local inference
   - Router not called

2. Test routing enabled:
   - Request goes through router
   - Router's route_request called with correct parameters

3. Test routing fallback:
   - When router raises exception, falls back to local/batched path
   - Logs warning about fallback

4. Test streaming through router:
   - Streaming response wrapped in EventSourceResponse
   - Chunks yielded correctly

5. Test non-streaming through router:
   - Dict response converted to ChatCompletionResponse
   - Usage and choices populated correctly

Mock the get_router() function and settings appropriately.
  </action>
  <verify>cd /Users/atomasini/Development/mlx-manager/backend && python -m pytest tests/mlx_server/api/v1/test_chat_routing.py -v</verify>
  <done>Routing integration tests passing</done>
</task>

</tasks>

<verification>
```bash
cd /Users/atomasini/Development/mlx-manager/backend

# Run tests
python -m pytest tests/mlx_server/api/v1/test_chat_routing.py -v

# Run all chat tests to ensure no regressions
python -m pytest tests/mlx_server/api/v1/test_chat.py -v

# Type checking
mypy mlx_manager/mlx_server/api/v1/chat.py

# Verify config
python -c "from mlx_manager.mlx_server.config import get_settings; print(get_settings().enable_cloud_routing)"
```
</verification>

<success_criteria>
- enable_cloud_routing config setting defaults to False
- When enabled, chat endpoint routes through BackendRouter
- Routing failures fall back to local/batched path
- Streaming and non-streaming both work through router
- All existing chat tests still pass
- All new routing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-dual-protocol-cloud-fallback/10-09-SUMMARY.md`
</output>
